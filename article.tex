%%
%% Copyright 2007-2019 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% l
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

\documentclass[preprint, 1p]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}
\usepackage{easy-todo}
\usepackage{graphicx}
\usepackage{booktabs}
% À changer
\newcommand{\yobs}{\mathbf{y}^o}
\DeclareMathOperator*{\argmin}{arg\,min \,}
\DeclareMathOperator*{\argmax}{arg\,max \,}
\newcommand{\Var}{\mathbb{V}\textrm{ar}}
\newcommand{\Ex}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Cov}{\textsf{Cov}}
\newcommand{\tra}{\mathrm{tr}}
\newcommand{\kmean}{{\mathbf{k}}_{\Ex}}
\newcommand{\hatkmean}{\hat{\mathbf{k}}_{\Ex}}
\newcommand{\kvar}{{\mathbf{k}}_{\mathbb{V}}}
\newcommand{\hatkvar}{\hat{\mathbf{k}}_{\mathbb{V}}}
\newcommand{\kmpe}{{\mathbf{k}}_{\mathrm{MPE}}}
\newcommand{\hatkmpe}{\hat{\mathbf{k}}_{\mathrm{MPE}}}
\newcommand{\kest}{\hat{\mathbf{k}}}

\newcommand{\checkap}{{\alpha}_p}
\newcommand{\checka}{{\alpha}}
\newcommand{\checkk}{\mathbf{k}}
\newcommand{\checkkp}{{\mathbf{k}}_p}
\newcommand{\hatkp}{\hat{\mathbf{k}}_p}
\newcommand{\Kspace}{\mathbb{K}}
\newcommand{\Uspace}{\mathbb{U}}
\newcommand{\JBH}{J_{\mathrm{BH}}}
\newcommand{\JBHS}{J_{\mathrm{BHswap}}}

\newcommand{\arthur}[1]{{\itshape\color{cyan} ({#1})}}
\newcommand{\elise}[1]{{\itshape\color{red} ({#1})}}
\newcommand{\victor}[1]{{\itshape\color{green} ({#1})}}

\begin{document}

\title{Title-- \today{}}%TEXT}


% \Author[affil]{given_name}{surname}

\Author[]{Victor}{Trappler}
\Author[]{Élise}{Arnaud}
\Author[]{Laurent}{Debreu}
\Author[]{Arthur}{Vidard}

\affil[]{Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP*, LJK, 38000 Grenoble, France}
% \affil[]{ADDRESS}

%% The [] brackets identify the author with the corresponding affiliation. 1, 2, 3, etc. should be inserted.



\runningtitle{Title}

\runningauthor{Trappler V.}

\correspondence{Victor Trappler (victor.trappler@univ-grenoble-alpes.fr)}



\received{}
\pubdiscuss{} %% only important for two-stage journals
\revised{}
\accepted{}
\published{}

%% These dates will be inserted by Copernicus Publications during the typesetting process.


\firstpage{1}

\maketitle

% \listoftodos\

\begin{abstract}
Classical methods of parameter estimation usually imply the minimisation of an objective function, that measures the error between some observations and the results obtained by a numerical model. In the presence of random inputs, the objective function becomes a random variable, and notions of robustness have to be introduced.

In this paper, we are going to present how to take into account those uncertainties by defining a notion of robustness based on the distribution of the conditional minimisers and compare it with the minimum in the mean sense, and the minimum of variance.
\victor{Pour le nom: Il y a l'idée que l'on veut avoir un regret $J(\mathbf{k},\mathbf{u}) - \alpha J^*(\mathbf{u})$ controlé à l'aide de $\alpha$: Relative regret ?}
\end{abstract}

\copyrightstatement{TEXT}

%----------------------------------------------------------------------------
% \listoftodos{}

%\arthur{Revoir les notations ($\hat{\bf k}$, ${\bf k}^*$ c'est la meme chose ou pas ? les u, U je ne suis pas sûr que c'est bon non plus)}
%\victor{Pour moi, ce qui est en majuscule, c'est des v.a., et tous les inputs sont en gras (mais on peut laisser tomber le gras)}
%% \introduction[modified heading if necessary]
%\subsection{Motivation}

Numerical models are widely used to study or forecast natural phenomenon and improve industrial processes. However, by essence models only partially represents reality and sources of uncertainties are ubiquitous (discretisation errors, missing physical processes, poorly known boundary conditions).
%Many physical phenomena are modelled numerically in order to better understand and/or to predict their behaviour. Because of the complexity and or ignorance of the underlying physics, numerical models are imperfect representations of the reality. 
%
%Various sources of uncertainties can corrupt the model quality, such as the discretisation of the domain, or presence of  small scale phenomena that can not be fully represented. 
Moreover, such uncertainties may be of different nature. 
~\cite{walker_defining_2003} propose two to consider two categories of uncertainties. On the one hand aleatoric uncertainties,  coming from the inherent variability of a phenomenon, e.g.\ intrinsic randomness of some environmental variables. On the other hand, epistemic uncertainties coming from a lack of knowledge about the properties and conditions of the phenomena underlying the behaviour of the system under study.
The latter can be accounted for through the introduction of ad-hoc correcting terms in the numerical model, that need to be properly estimated. Thus, reducing the epistemic uncertainty can be done through parameters estimation approaches. 
%when one is having observations of the reality. 
This is usually done using optimal control techniques, leading to an optimisation of a well chosen cost function which is typically built as a comparison with reference observations.
%
An application of such an approach, in the context of ocean circulation modeling, is the estimation of ocean bottom friction parameters in~\cite{das_estimation_1991} and~\cite{boutet_estimation_2015}. 


If parameters to be estimated are not the only source of uncertainties, their optimal control 
%However, in the presence of other sources of uncertainties, parameter estimation 
is doomed to overfit the data, \emph{e.g} to artificially introduce errors in the controlled parameter to compensate for other sources. If such uncertainties are of aleatoric nature, then the parameter estimation is only optimal for the observed situation, and may be very poor in other configurations, phenomenon coined as \textit{localized optimization} in~\cite{huyse_free-form_2001}.
%A methodology based on the Hessian matrix has been described in~\cite{ribaud_krigeage_2018-1}.
%\arthur{L\`A AUSSI UNE REF SERAIT BIENVENUE}. 
Taking into account aleatoric uncertainties in optimisation problems takes several names, including \textit{robust optimisation}, \textit{robust design} in~\cite{lelievre_consideration_2016}, or \textit{optimization under uncertainties} \citep{petrone_robustness_2011, seshadri_density-matching_2014,cook_horsetail_2018}. 

%In this paper we adopt the \textit{robust} terminology.

%Taking into account aleatoric uncertainties while calibrating the model (reducing epistemic uncertainties) is often referred as {\it robust control problem} in the literature (see for instance, \arthur{REFs}), or optimization under uncertainties \citep{petrone_robustness_2011, seshadri_density-matching_2014,cook_horsetail_2018}.

Let's denote $\mathbf{k} \in \Kspace$ the parameter set to be estimated, to reduce epistemic errors. The aleatoric uncertainties are modelled as a random vector $\mathbf{U}$ whose sample space is $\Uspace$. The probability measure of $\mathbf{U}$ is $\Prob_{\mathbf{U}}$, and its density, if it exists, is $p_{\mathbf{U}}$. %Furthermore, $\mathbf{u}^t$ (used to generate $\mathbf{y}^{\mathrm{obs}}$) is sampled from $\mathbf{U}$.
%
The cost function $J(\mathbf{k}, \mathbf{U})$ is a random variable in this context. %, quantity of interest summarising what we aim to represent with our numerical system $J(\mathbf{k}, \mathbf{U})$ is a random variable that will be further designed as the cost function. 
It is most often defined as the squared norm of a given function $\mathcal{G}(\mathbf{k}, \mathbf{U})$% involving the numerical model. 
\begin{align}
  \label{eq:def_cost_fun}
  J(\mathbf{k},\mathbf{U}) & = \frac12\|\mathcal{G}(\mathbf{k}, \mathbf{U})\|^2
  \end{align}
%$J$ is possibly complemented by a regularisation function that is omitted here for the sake of clarity. 
For instance, in data assimilation, $J$ describes a distance between the output of the numerical model and given observed data, plus generally some regularization terms.
%Calibrating the numerical models consists in the computation of $\kest$ verifying
%\begin{equation}
%  \label{eq:khat_def}
%  \kest = \argmin_{\mathbf{k} \in \Kspace} J(\mathbf{k}, \mathbf{u})
%\end{equation}
%\elise{la il y a un soucis car J(k,U) est une variable aleatoire donc on sait pas ce que ca veut dire de prendre le min} \victor{ici ça serait mieux de mettre un $\mathbf{u}^b$}
%
%where $J$ is then an objective function to minimise,
 %
%For a given $\mathbf{k}$, $J(\mathbf{k},\mathbf{U})$ is a random variable. The main point is to be able to compute a value $\mathbf{k}$ that ``minimises'' in a certain sense the random variable $J(\mathbf{k},\mathbf{U})$. \elise{a revoir. un v.a. ne se minimise pas} 
%
For a practical purpose, we assume that $\forall~ \mathbf{u}$ a realisation of $\mathbf{U}$, finding ${\mathbf{k}}_{\mathbf{u}}$ that minimises the cost function $J(\mathbf{k}, \mathbf{U}=\mathbf{u})$ is a well-posed problem, and that the optimum is unique.
Additionally, the following assumption are made: the cost function is strictly positive, and $\forall~  \mathbf{k} \in \Kspace$, the random variable $J(\mathbf{k},\mathbf{U})$ has finite first- and second-order moments. 
 
In this paper, we aim at finding $\kest$ a {\it robust\/} estimator of $\mathbf{k}$.
The definition of robustness differs depending on the context in which it is used. Indeed, one definition of the robustness of an estimate is a measure of the sensibility of said estimate to outliers~\citep{huber_robust_2011}. This lead to the introduction of robust norms in data assimilation \citep{rao_robust_2015}. %The point of this norm is to assume Gaussian distribution for small errors (the usual $L_2$ norm), while the large errors are assumed to be Laplace distributed (so the $L_1$ norm).
In a Bayesian framework, robustness may refer to the sensitivity to a wrong specification of the priors~\citep{berger_overview_1994}.
%
Throughout this paper, 
%we define robustness as the ability of $\kest$ to perform reasonably well under different operating conditions (i.e.\ different $\mathbf{u}$).
robust has to be understood as satisfactory for a broad range of $\mathbf{u}$, and/or as insensitive as possible to uncertainties encompassed in $\mathbf{U}$.

The usual practice consists in neglecting the variability of $\mathbf{U}$ by setting it to an {\it a priori} value $\mathbf{u}^b$.
 In this case, $\kest$ is set to the optimum ${\mathbf{k}}_{\mathbf{u}^{b}} $ of $J(\mathbf{k}, \mathbf{U}=\mathbf{u}^b)$.  There is no guarantee on the performance of $\kest$ if the calibrated model is used for predictions, as the estimated value will compensate the error made by a possibly wrong specification of $\mathbf{u}^b$.
In a data assimilation context, this situation appears if $\mathbf{u}^b$ does not properly represent the conditions on which the observations have been obtained.
Another strategy, that consists in minimising $J$ over the joint space $\Kspace\times\Uspace$, is not always possible or relevant. The complexity of the optimisation is increased, and the computed estimation of $\kest$ has no reason to be robust in the end: this kind of method does not take into consideration the variability of the uncertain variable. 
%
The worst-case approach \citep{marzat_worst-case_2013} is another popular method, and is based on the minimisation with respect to $\mathbf{k}$ of the maximum of the cost function for $\mathbf{u}\in\Uspace$: $\min_{\mathbf{k}} \max_{\mathbf{u}} J(\mathbf{k}, \mathbf{U} = \mathbf{u})$. This approach may yield over-conservative solutions, and does not take into account the random nature of $\mathbf{U}$.% when the support of $\mathbf{U}$ is unbounded.

Accounting for the probabilistic nature of  $\mathbf{U}$ leads to study the distribution of the random variable $J(\mathbf{k},\mathbf{U})$, or the distribution of its minimisers ${\mathbf{k}}_{\mathbf{U}}$. The latter is referred as 
the distribution of the conditional minimisers, notion that appeared notably in~\cite{villemonteix_informational_2006} and in~\cite{hennig_entropy_2011} for a global optimisation purpose. Both approaches and related robust estimates are described in Section~\ref{robust formulations}. Section~\ref{sec:relax_constraint} introduces a new class of estimators, by relaxing the constraint of optimality and defining regions of acceptability, similarly as~\cite{buhmann_robust_2013} in discrete combinatorial problems.  The intention is that a robust estimate provides values of the cost function close enough to the attainable minimum for each configuration induced by $\mathbf{u}\in\Uspace$. Illustration of the various discribed methods are given on a numerical exemple in Section~\ref{sec:SWE_application}.

% ----------------------------------------------------------------------------


\section{Classical robust estimators}
\label{robust formulations}

As mentioned before, robustness can be understood as satisfactory for a broad range of $\mathbf{u}$, and/or as insensitive as possible to uncertainties encompassed in $\mathbf{U}$. Under this definition, one may design classical robust estimators, either by optimising the moments of the cost function; or based on the distribution of its minimisers.


%
%When robustness is defined as the ability of $\kest$ to perform reasonably well under different operating conditions (i.e.\ different $\mathbf{u}$), two classical  approaches can be used to define robust estimators:
%\begin{itemize} 
%\item Optimisation of the statistical moments of the random variable $J(\mathbf{k}, \mathbf{U})$: usually minimisation of the expected value, of the variance or a combination of them
%\item Estimate based on the distribution of the minimisers $\mathbf{k}_{\mathbf{u}}$, assuming the inverse problem is well-posed to ensure the existence and the unicity of the minimiser for each $\mathbf{u}$. 
%\end{itemize}




%\subsection{simple/classical estimators \arthur{TITRE \`A REVOIR}}

%Broadly speaking, two different aspects can be used to define robust estimators:
%\begin{itemize} 
%\item Optimisation of the statistical moments of the random variable: usually minimisation of the expected value, of the variance or a combination of them
%\item Estimate based on the distribution of the minimisers, assuming the inverse problem is well-posed to ensure the existence and the unicity of the minimiser for each $\mathbf{u}$. 
%\end{itemize}




% ----------------------------------------------------------------------------
\subsection{Optimisation of the moments}

%Statistical moments, especially  expectation and variance, are common tools used to describe and summarise a random variable. %A case in point are the Gaussian random variables, that are uniquely defined by their expected value and their variance. 
%It then makes sense that the statistical moments have a central role in robust optimisation. In practice, such ideas relies on the expected value as seen in \cite{shapiro_lectures_2009}, while looking at the same time mean and variance is the basis of Markowitz's portfolio selection theory in \cite{markowitz_portfolio_1952}. 
%

{\color{red} kmean ou kexpectation ?}

Let us define $\mu(\mathbf{k})$ and $\sigma^2(\mathbf{k})$, the expected value and the variance of the cost variable  for a given $\mathbf{k}$ as 
\begin{align}
  \label{eq:def_mu}
  \mu(\mathbf{k}) &= \Ex_{\mathbf{U}}\left[J(\mathbf{k},\mathbf{U})\right] = %\int_{\Uspace} J(\mathbf{k},\mathbf{u}) \, \mathrm{d}\Prob_{\mathbf{U}}(\mathbf{u}) 
   \int_{\Uspace} J(\mathbf{k},\mathbf{u})p_{\mathbf{U}}(\mathbf{u}) \, \mathrm{d}\mathbf{u} \\
  \label{eq:def_sigma2}
  \sigma^2(\mathbf{k}) &= \Var_{\mathbf{U}}\left[J(\mathbf{k},\mathbf{U})\right] 
  % \int_{\Uspace} \left(J(\mathbf{k},\mathbf{u}) - \mu(\mathbf{k})\right)^2 \, \mathrm{d}\Prob_{\mathbf{U}}(\mathbf{u})
   =\int_{\Uspace} \left(J(\mathbf{k},\mathbf{u}) - \mu(\mathbf{k})\right)^2 p_{\mathbf{U}}(\mathbf{u})\, \mathrm{d}\mathbf{u}
\end{align} 
%
Minimising the expectation leads to the estimate $\kmean$ defined by:
\begin{equation}
  \label{eq:stoch_opt_def_mean}
  \kmean = \argmin_{\mathbf{k}\in\Kspace} \mu(\mathbf{k})
\end{equation}
In order to take into account the spread around the mean value, one can choose to minimise the variance, leading to $\kvar$:
\begin{equation}
  \label{eq:stoch_opt_def_var}
  \kvar = \argmin_{\mathbf{k} \in \Kspace} \sigma^2(\mathbf{k})
\end{equation}

A lot of different methods are readily available to solve these
minimisation problems. For instance, stochastic Sample Approximation
\citep{juditsky_stochastic_2009,kim_guide_2015} is based on a finite
and fixed set of samples $\{ \mathbf{u}^{i}\}_{i=1 \dots N}$ of
$\mathbf{U}$.
%the sampling of a certain amount of $\mathbf{U}$ 
The estimations at a given $\mathbf{k}$ are computed using standard Monte Carlo, resulting in the following optimisation problems:
\begin{equation}
\hatkmean = \argmin_{\mathbf{k} \in \Kspace} \sum_{i=1}^N  J(\mathbf{k},\mathbf{u}^i)
\end{equation}
and
\begin{equation}
\hatkvar = \argmin_{\mathbf{k} \in \Kspace} \sum_{i=1}^N  \left( J(\mathbf{k},\mathbf{u}^i) -  \sum_{j=1}^N  J(\mathbf{k},\mathbf{u}^j) \right )^2
\end{equation}
%In the following, the hat notation will indicate an estimator. 
If computationally affordable, one can perform these estimations on a regular grid on $\Kspace \times \Uspace$. In case of expensive computer code, one can build a meta model to ease the minimisation, such as Gaussian processes \citep{janusevskis_simultaneous_2010}.
Even though $\kmean$ is a reasonable choice, there is no guarantee that $J(\kmean, \mathbf{U}=\mathbf{u})$ will not reach catastrophic level for some  $\mathbf{u}$. On the other hand, using $\kvar$ will ensure stability of the cost function, but whithout any control of its level of quality.
%
%For a single $\mathbf{k}$, the value of the cost function is controlled in a probabilistic sense by the mean and variance.
%Chebyshev's inequality gives an upper bound on the probability of being more than $\lambda>0$ units from the mean:
%\begin{equation}
%  \label{eq:cheby_ineq}
%  \Prob_{\mathbf{U}}\left[|J(\mathbf{k}% _{\Ex}
%    ,\mathbf{U}) - \mu(\mathbf{k}% _{\Ex}
%    )|> \lambda\right] \leq \frac{\sigma^2(\mathbf{k}% _{\Ex}
%    )}{\lambda^2}
%\end{equation}
%
Ideally, one would want to have a small mean value, and a small variance as well. Multi-objective optimisation is a proper tool to deal with these simultaneous and sometimes concurrent objectives, for exemple by computing the Pareto front of $\left(\mu(\mathbf{k}),\sigma^2(\mathbf{k})\right)$ as done in~\cite{baudoui_optimisation_2012}.

As the computation of this Pareto front is usually hard and expensive, alternative strategies based on the minimisation of a scalarized version of the vector of objectives are often considered. Some are based on a weighted sum of the objectives, as presented in~\cite{grodzevich_normalization_2006} and in~\cite{marler_weighted_2010}, while some others are based on the minimisation of one of the objectives under constraints on the others, as performed in~\cite{lehman_designing_2004}. Both of these methods are based on an \textit{delicate} choice of weights or of constraints before any computation. This choice relies heavily on a knowledge of the properties of the cost function.

To summarise, even though the notions of mean and variance are quite easily understood, getting a satisfactory estimator is not that straightforward. One could instead consider how often a particular value $\mathbf{k}$ is a minimiser of the cost function, leading to the notion of most probable estimate, as explained in the next subsection.

{\color{red} mean ou expectation ?}

% ----------------------------------------------------------------------------
\subsection{Most probable estimate}
Let us consider the minimal attainable cost in each configuration brought by $\mathbf{u}$.
The resulting conditional minimum is denoted as $J^*$:
\begin{equation}
\label{eq:def_Jstar}
J^*: \mathbf{u} \in\Uspace \longmapsto J^*(\mathbf{u}) = \min_{\mathbf{k}\in \Kspace} J(\mathbf{k},\mathbf{U}=\mathbf{u})
\end{equation}
Similarly, the function of conditional minimisers can then defined by:
  \begin{equation}
  \label{eq:conditional_minimiser}
   \mathbf{k}^*: \mathbf{u}\in\Uspace \longmapsto \mathbf{k}^*(\mathbf{u}) = \mathbf{k}_{\mathbf{u}}=  \argmin_{{\mathbf{k}}\in\Kspace} J({\mathbf{k}},\mathbf{U}=\mathbf{u})
  \end{equation}
Using this function, we can define the corresponding random variable $\mathbf{K}^*$ as
  \begin{equation}
    \label{eq:def_study_minimisers}
    \mathbf{K}^*= \mathbf{k}^*(\mathbf{U}),
  \end{equation}
and its associated density function $p_{\mathbf{K}^*}(\mathbf{k})$, that will be further referred as the density of minimisers.
The mode of this density is called the Most Probable Estimate (MPE) and is noted  $\kmpe$:
\begin{equation}
  \label{eq:MPE}
  \kmpe = \argmax_{\mathbf{k} \in\Kspace} p_{\mathbf{K}^*}(\mathbf{k}) 
\end{equation}
To give some intuition on this estimate, let us imagine that the distribution of minimisers is a dirac centered on $\kmpe$. Then it would mean that this estimate is the minimiser of the cost function whatever the realisation of the uncertain variable, therefore optimal in all conditions. 
If the distribution $p_{\mathbf{K}^*}$ is heavily dominated by a single value, the MPE may be a good candidate for robust control. This is not so obvious in case of a multimodal distribution. 
In general, an analytical form of $p_{\mathbf{K}^*}$ is usually impossible to obtain, so an estimation $\hat{p}_{\mathbf{K}^*}$ must be used, and its maximum computed to get the MPE.

Once again, a set of samples  $\{\mathbf{u}^i\}_{i=1\dots N}$ can be used to compute the set $\{\mathbf{k}_{\mathbf{u}^i}\}_{i=1\dots N}$, from which one can 
approximate  $p_{\mathbf{K}^*}$. The resulting approximation and therefore its mode, is sensitive to the density estimation method. Main methods are KDE  (Kernel Density Estimation)~\cite{elise}, and  EM (Expectation-Maximisation)~\citep{dempster_maximum_1977}.

%Two widely used methods to estimate the density are the KDE (Kernel Density Estimation), and the EM~\citep{dempster_maximum_1977} algorithm to fit a mixture of Gaussian random variable to the observations.

KDE is a non-parametric estimation technique based on the use of a kernel function $f$. Assuming an isotropic kernel, the estimation has the following form:
\begin{equation}
\hat{p}_{\mathbf{K}^*}(\mathbf{k}) = \frac{1}{Nh^{\dim \Kspace}} \sum_{i=1}^N f\left(\frac{\mathbf{k} - \mathbf{k}_{\mathbf{u}^i}}{h}\right)
\end{equation}
where $h$ is the bandwidth. In a multidimensional setting, one usually consider a kernel based on the product of 1D kernels, applied independently to all components: $f(\mathbf{k}) = \prod_{j=1}^{\dim \Kspace} f_{\mathrm{1D}} (k^{(j)})$
where $k^{(j)}$ is the $j$-th component of $\mathbf{k}$.  There is wide choice of available $f_{\mathrm{1D}}$, and a popular choice is  %such as a rectangle function, $f_{\mathrm{1D}}(x) = 1 \text{ if } |x|< \frac{1}{2}$ and  $0$ elsewhere,
the Gaussian kernel $f_{\mathrm{1D}}(x)= \frac{1}{\sqrt{2\pi}}\exp(-x^2 / 2)$. % One can notice that the rectangle function as a kernel yields non-smooth densities, very similar to the histogram of the minimisers.

The EM algorithm can also be used to estimate the density, by minimising the statistical distance between the empirical distribution and a mixture of $\nu$ Gaussian densities. The estimation has then the following form:
\begin{equation}
\hat{p}_{\mathbf{K}^*}(\mathbf{k}) = \sum_{i=1}^{\nu} \pi_i \phi(\mathbf{k}; \mathbf{m}_i, \mathbf{\Sigma}_i)
\end{equation}
where $\phi( \cdot; \mathbf{m}, \mathbf{\Sigma})$ is the probability density function of the normal distribution of mean $\mathbf{m}$ and covariance matrix $\mathbf{\Sigma}$, and $\{\pi_i \}_{i=1 \dots \nu}$ are the mixing coefficients.

%\elise{Ici parle de l'estimation de kMPE. se referer au cas precedent ou on utilise une ensemble fini de ui. et dire que ca nous génere des kui avec lesquels on fait une approximation de la densite de pK*. Le choix de la methode d'approximation aura une grosse influence sur l'estimation de kMPE. 
%Soit on fait une approximation par noyau (eventuellement noyau créneau d'ou histogramme), ou on peut faire approcher le truc par un melange de gaussienne. Eventuellement reutiliser des phrases du paragraphe suivant (en commentaire) ? }

%The computation of the MPE is more problematic. Indeed, an analytical form of the distribution of the minimisers $p_{\mathbf{K}^*}$ is usually impossible to obtain, so an approximation must be used, and its maximum computed to get the MPE.
%Computationally speaking, to compute a realisation of $\mathbf{K}^*$, a value $\mathbf{u}$ is sampled from $\mathbf{U}$, and a minimisation problem is solved to get $\mathbf{k}^*(\mathbf{u})$. Using the grid mentioned before, 1000 realisations of $\mathbf{K}^*$ have been obtained by direct search on grid.


Despite the fact that those methods are well established, using them in a plug-in approach has some flaws. One of the basic assumption of density estimation is to assume that $\mathbf{K}^*$ is a continuous random variable, hypothesis that may be violated. Worse, the notion of mode is not well defined when the distribution of the minimisers is a discrete-continuous mixture. This may result in inconsistent 
estimations of $\hatkmpe$ when using different methods as illustrated in next subsection.

%The quantities introduced here try to include the random nature of the environmental variables, but the control variable is considered still very locally: some of the values of $\mathbf{k}$, when changed slightly, produce still a very acceptable cost

%Beside the fact that one would like a value that encompasses the robustness capacity of the estimate $\kmpe$, its estimation is sensitive to the required density approximation procedure, 
%\victor{j'ai enlevé la "robustness capacity"}
%However, if the value of this maximum is not high, it may not be considered robust: this formulation does not provide information on the performance of the estimator for the configuration where it is not optimal, situation that happens with probability $1-p_{\mathbf{K}^*}(\kmpe)$. 

%In the case where $\mathbf{K}^*$ is continuous, $p_{\mathbf{K}^*}$ is now its probability density function, and $\kmpe$ is defined in the same fashion. 
%However, the MPE is sensitive to the method used to estimate the density as illustrated in Section~\ref{ssec:num_illu}. 
%The introduction of a relaxation of the equality constraint will also allow us to get rid of these issues altogether in Section~\ref{ssec:relax_constraint}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Numerical illustration}
\label{ssec:num_illu}
Before going further in the explanation of our approach, let us illustrate the nature of previously detailed estimators, $\hatkmean, \hatkvar$ and $\hatkmpe$ on two analytical cost functions. These functions are based on the Branin-Hoo's function, slightly modified to ensure strict positivity:
\begin{align}
  \label{eq:branin_hoo}
 \mathrm{BH}(x_1,x_2) &= \frac{1}{51.95}\left[\left(\bar{x}_2 - \frac{5.1 \bar{x}_1^2}{4\pi^2} + \frac{5\bar{x}_1}{\pi} -6 \right)^2 + \left(10 - \frac{10}{8\pi}\right)\cos(\bar{x}_1)-44.81\right]+2\\
&\text{with } \bar{x}_1 = 3x_1-5,\quad \bar{x}_2 = 3x_2
\end{align}
%
Using Eq.~\eqref{eq:branin_hoo}, we define the two cost functions on $\Kspace \times \Uspace = [0,5] \times [0,5]$ as:
\begin{align*} J_{\mathrm{BH}} : (\mathbf{k},\mathbf{u}) \mapsto \mathrm{BH}(\mathbf{k},\mathbf{u}) \\
J_{\mathrm{BHswap}} : (\mathbf{k},\mathbf{u}) \mapsto \mathrm{BH}(\mathbf{u},\mathbf{k})
\end{align*}
%We will continue to use $J$ as a generic notation for the cost function, when its use is not specific to the Branin-Hoo function.
Even though the functions are quite similar, the asymmetric roles of $\mathbf{k}$ and $\mathbf{u}$ cause different behaviour.


\begin{figure*}[!ht]
  \centering
  \includegraphics[width=12cm]{Figures/branin_side_moments_noscale.pdf}
  \caption{The left column concerns the $J_{\mathrm{BH}}$ function, while the right one deals with $J_{\mathrm{BHswap}}$. Contours of both functions are plotted on the top, and curves of  $\mu(\mathbf{k})$ and $\sigma(\mathbf{k})$ are shown on the bottom (respective scales are note represented). Estimates $\hatkmean$ and $\hatkvar$ are plotted with the dashed line. } \label{fig:branin_moments}
\end{figure*}
%\clearpage % Pour lisibilité dans le preview


The random variable $\mathbf{U}$ is assumed to be uniformly distributed over $\Uspace$.
%The definition and the estimation of $\kmean$ and $\kvar$ is pretty standard.  
The estimations are %$\mathbf{k}\mapsto \mu(\mathbf{k})$ and $\mathbf{k}\mapsto\sigma(\mathbf{k})$ 
based on a $1000 \times 1000$ regular grid over $\Kspace\times\Uspace$. Both cost functions are shown on~Figure~\ref{fig:branin_moments}, top row.


 The left, respectively right, column stands for $J_{\mathrm{BH}}$, respectively $J_{\mathrm{BHswap}}$ function. Functions $\mu(\mathbf{k})$ and $\sigma(\mathbf{k})$ are drawn on the bottom row, respectively in purple and green. The corresponding minimizers $\hatkmean$ and $\hatkvar$ are also plotted.
On this figure, we can observe that $\hatkmean$ and $\hatkvar$ are close for the $J_{\mathrm{BH}}$ function, while being significantly different for the $J_{\mathrm{BHswap}}$ function.
%\clearpage


Similarly $\hatkmpe$ estimations are depicted on Figure~\ref{fig:contours}.  Top row shows the contour plot of both functions as well as the set of conditional minimisers $\{\mathbf{k}_{\mathbf{u}^i}\}_{1\leq i \leq N}$ in red, as defined in Eq.~\eqref{eq:conditional_minimiser}.
The bottom rows presents three approximations of the density of minimisers: the histogram in grey (bin size selected using Freedman-Diaconis from~\cite{freedman_histogram_1981}), the result of a kernel density estimation (KDE) with Gaussian kernels in red (using Scott's rule from~\cite{scott_optimal_1979} for bandwidth selection), and the estimation by a Gaussian mixture. The latter has been calculated with the EM algorithm. The number of Gaussians has been fixed to 3, a guess based on the general shape of the histogram. Respective estimations of $\hatkmpe$ are also depicted using dashed lines. 

%The computation of the MPE is more problematic. Indeed, an analytical form of the distribution of the minimisers $p_{\mathbf{K}^*}$ is usually impossible to obtain, so an approximation must be used, and its maximum computed to get the MPE.
%Computationally speaking, to compute a realisation of $\mathbf{K}^*$, a value $\mathbf{u}$ is sampled from $\mathbf{U}$, and a minimisation problem is solved to get $\mathbf{k}^*(\mathbf{u})$. Using the grid mentioned before, 1000 realisations of $\mathbf{K}^*$ have been obtained by direct search on grid.


%In this paper, we used two different methods to estimate the distribution of the minimisers: KDE (kernel density estimation)~\citep{parzen_estimation_1962,bishop_pattern_2006} and the EM algorithm~\citep{dempster_maximum_1977,borman_expectation_2004} to fit a mixture of Gaussian distributions. This algorithm needing the number of densities to fit, we chose to fit three Gaussian, a guess based on the general shape of the histograms.
%Figure~\ref{fig:contours} shows the contour plot, conditional mean and conditional minimisers of the $J_{\mathrm{BH}}$ function (left) and the $J_{\mathrm{BHswap}}$ function (right).

\begin{figure*}
  \centering
  \includegraphics[width=12cm]{Figures/branin_side_66.pdf}
  \caption{Top Left:  $J_{\mathrm{BH}}$ function along with conditional minimisers in red. Bottom left: Estimated density using KDE and EM algorithm and conditional mean. The dashed lines indicate the MPE found using those methods. Right: Same quantities with $J_{\mathrm{BHswap}}$.}
\label{fig:contours}
\end{figure*}

For the $J_{\mathrm{BHswap}}$ function, we can observe that those three methods give coherent results, as $\hatkmpe {}_{,\mathrm{KDE}} =\hatkmpe {}_{,\mathrm{EM}}=\hatkmpe {}_{,\mathrm{histogram}}\approx 0.8$. This is not the case for the  $J_{\mathrm{BH}}$ function however. Using Kernel density estimation (Gaussian), the estimation of $\kmpe$ is $\hatkmpe {}_{,\mathrm{KDE}} \approx 1.5$, while using the histogram and Gaussian mixture, $\hatkmpe {}_{,\mathrm{histogram}}=\hatkmpe {}_{,\mathrm{EM}}=3.8$.
This difference is explained by the accumulation of minimisers at this point: this challenges the assumption that $\mathbf{K}^*$ is continuous. As the density estimation techniques traditionally assume this continuity, the EM algorithm fits this using a normal distribution with a very small variance, while the KDE considers a sum of Gaussian kernels of constant bandwidth, located at the same point. This particular problem highlights the issue with $\kmpe$, as its estimation is highly sensitive to the density approximation procedure.

Alternatively, we can consider the mixing coefficients of the EM
density estimation. The Gaussian centered at $1.5$ has been attributed
a weight of $0.608$, while the one centered at $3.8$ has a mixing
weight of $0.226$. Ultimately, an optimiser would be more likely to be
close to $1.5$, but we do not have information on its performance in
this region around this value. \arthur{je ne comprends pas ce passage}

% as we can argue that both values may be a sensible choice for the MPE. This comes from the fact that the hypothesis that $\mathbf{K}^*$ is a continuous r.v.\ is violated. 
A way to avoid this issue is to introduce a bit of leeway for a value to be optimal, or at least ``acceptably not optimal''. This slackness takes the form of a relaxation coefficient, whose choice leads to a new family of robust estimators, along with a measure of their robustness via the coefficient.
%This leads us to the definition of a new family of estimators, for which one can attribute a {\it level of robustness} \elise{je me rend bien compte que cette phrase est un peu bizarre ...mais bon c'est l'idee}


%, by transforming the equality of Eq.~\eqref{eq:def_R1} into a inequality.

% ----------------------------------------------------------------------------


\section{Relative regret-based family of estimators}%A new family of robust estimators \elise{titre a revoir}}
\label{sec:relax_constraint}
\subsection{Relaxing the optimality constraint}
The density of minimisers has been estimated by optimising $\mathbf{k}\mapsto J(\mathbf{k},\mathbf{u})$ over $\Kspace$ for different realisations of $\mathbf{u}$. Instead of only considering  optimal values, we propose to consider their {\it acceptable} neighbourhood in terms of performance of the cost function as well, as illustrated in Figure~\ref{fig:relax_tuto}. 
To do so, for each $\mathbf{u}$, we define as acceptable values of the cost function, the values that are below $\alpha J^*(\mathbf{u})$, with $\alpha > 1$. 

For each $\mathbf{u}$, we construct a region around the optimiser that produces acceptable values of the cost function, as seen on the bottom left plot. Looking at it the other way around, for a given $\mathbf{k}$, the set $R_{\alpha}(\mathbf{k}) \subset \Uspace$ is defined as the set of $\mathbf{u}$ verifying the condition of acceptability:
\begin{equation}
\label{eq:def_Ralpha}
R_{\alpha}(\mathbf{k}) = \left\{ \mathbf{u} \in \Uspace \mid J(\mathbf{k},\mathbf{u}) \leq \alpha J^*(\mathbf{u}) \right\}
\end{equation}
% This notion of acceptability will be introduced later.
Finally, for each $\mathbf{k}$, we will measure (with respect to $\Prob_{\mathbf{U}}$) the set of $\mathbf{u}$ within this acceptable neighbourhood, as illustrated in the bottom right plot. 

\begin{figure*}[!t]
\centering
\includegraphics[width=12cm]{Figures/relaxation_tuto.pdf}
\caption{Principle of the relaxation of the constraint on $J_{\mathrm{BHswap}}$, and illustration of $R_\alpha(\mathbf{k})$}
\label{fig:relax_tuto}
\end{figure*}

%In order to construct this region, we introduce a factor $\alpha \geq 1$, that will be multiplied to $J^*$:
%for a given $\mathbf{u}$, $\mathbf{k}$ is deemed acceptable if $J(\mathbf{k},\mathbf{u}) \leq \alpha J^*( \mathbf{u})$.
Introducing the random nature of $\mathbf{U}$, $\Gamma_\alpha(\mathbf{k})$ is defined as the probability that $\mathbf{k}$ is acceptable given $\alpha$:
\begin{equation}
  \label{eq:def_Gamma}
  \Gamma_\alpha(\mathbf{k}) = \Prob_{\mathbf{U}}\left[\mathbf{U} \in R_\alpha(\mathbf{k})\right] = \Prob_{\mathbf{U}}\left[J(\mathbf{k},\mathbf{U}) \leq 
    \alpha J^*(\mathbf{U}) \right]
\end{equation}
In other words, $\Gamma_{\alpha}(\mathbf{k})$ is then the probability that $J(\mathbf{k},\mathbf{U})$ is between 
$J^*(\mathbf{U})$ and 
$\alpha J^*(\mathbf{U})$. Without relaxation, i.e.\ when $\alpha$ is set to $1$, $\Gamma_1$ is non-zero if the set $\{\mathbf{u}\in\Uspace \mid J(\mathbf{k},\mathbf{u}) = J^*(\mathbf{u})\}$ has non-zero measure with respect to $\Prob_{\mathbf{U}}$, that is when the distribution of $\mathbf{K}^*$ presents atoms. This can be linked to the definition of the MPE from Eq.~\eqref{eq:MPE}. In the more general case, the MPE can be seen as the limiting case when $\alpha$ tends to $1$.

The motivation behind this relaxation is to take into account the local behaviour of the function around the conditional minimisers.
For a given set of environmental conditions $\mathbf{u}$, if the function $\mathbf{k} \mapsto J(\mathbf{k},\mathbf{u})$ is flat around its minimum $\mathbf{k}^*(\mathbf{u})$, then choosing  $\mathbf{k}^*(\mathbf{u}) + \epsilon$ (for a small $\epsilon$) will produce a value closer to the minimum than when the function has a high curvature.
In addition to that, relaxing the constraint using a multiplicative constant puts more weight (i.e.\ a slower increase of the acceptable region) on the values of $\mathbf{k}^*(\mathbf{u})$ when $J^*(\mathbf{u})$ is close to zero.

Given that $J>0$, $\Gamma_{\alpha}(\mathbf{k})$ is increasing with respect to $\alpha$ for any $\mathbf{k}\in\Kspace$. We can then focus on the smallest value of $\alpha$ such that $\Gamma_\alpha$ reaches a certain level of confidence $p\in[0,1]$. This leads to the definition of $\checkap$
\begin{align}
  \checkap &= \inf\left\{ \alpha\geq 1 \mid \exists \checkkp \in \Kspace,\, \Gamma_{\alpha}(\checkkp) \geq p \right\} \nonumber \\
   &= \inf \left\{ \alpha \geq 1 \mid \max_{\mathbf{k}\in\Kspace} \Gamma_{\alpha}(\mathbf{k}) \geq p \right\}   \label{eq:def_alpha_check}
\end{align}
that is the smallest $\alpha$, such that there exists a particular $\checkkp \in \Kspace$ for which $J(\checkkp,\mathbf{U}) \leq \checkap J^*(\mathbf{U})$ with probability $p$. This formulation can be linked to the quantiles and Value-at-Risk of the random variable $\max_{\mathbf{k}} \{ \frac{J(\mathbf{k},\mathbf{U})}{J^*(\mathbf{U})}\}$, that is a measure of risk usually applied in the financial sector (see~\cite{rockafellar_deviation_2002}). For different levels $p$, and thus different $\alpha_p$, Figure~\ref{fig:illu_alpha_p} shows examples of $\Gamma_{\alpha_p}$. By definition, the associated $\mathbf{k}_p$ is then the first value for which $\Gamma_{\alpha_p}$ reaches $p$. We can see that changing the level $p$ shifts the maximiser of $\Gamma_{\alpha_p}$, and that for small $\alpha$, $\Gamma_{\alpha}(\mathbf{k}_1)$ is also very small. This indicates that $\mathbf{k}_1$ is located quite far from the conditional minimisers, and arise as a compromise when the relaxation is large enough.

\begin{figure}[!ht]
\centering
\includegraphics[width = 8.3cm]{Figures/illu_alpha_p.pdf}
\caption{Illustration of the influence of different levels $p$ on $\Gamma_{\alpha_p}$ and on $\mathbf{k}_p$ \victor{Pertinence ?}}
\label{fig:illu_alpha_p}
\end{figure}



% For computational purpose, we can propose a loose upper bound on relevant values of $\alpha$ from Eq.~\eqref{eq:def_Ralpha}, provided that the maximum and minimum of $J$ is attained on $\Kspace\times\Uspace$. We have that for all $\mathbf{k}$, $\Prob_{\mathbf{U}}\left[J(\mathbf{k},\mathbf{U}) \leq \max_{\mathbf{k},\mathbf{u}} J\right]=1$, so
% \begin{equation}
%   \label{eq:upper_bound_alpha}
%   \alpha \geq \frac{\max_{(\mathbf{k},\mathbf{u})} J(\mathbf{k},\mathbf{u})}{\min_{(\mathbf{k},\mathbf{u})} J(\mathbf{k},\mathbf{u}) } \iff R_\alpha(\mathbf{k}) = 1 \quad \forall \mathbf{k}\in\Kspace
% \end{equation}

% In other words, this definition can be seen as the construction of the smallest confidence interval of level $p$:
% \begin{equation}
%  \label{eq:def_CI}
%  \Prob_{\mathbf{U}}\left[ \frac{J(\checkkp,\mathbf{U})}{J^*(\mathbf{U})}\in [1, \checka_p]\right] = p
%\end{equation}

By considering the particular case where $p=1$, a relation between $\checkk_1$ and $\checka_1$ can be derived, by using Eq.~\eqref{eq:def_alpha_check} and the strict positivity of the cost function:
\begin{equation}
  \label{eq:def_alpha1}
  \Prob_{\mathbf{U}}\left[\frac{J(\mathbf{k}_1,\mathbf{U})}{ J^*(\mathbf{U}) } \leq \checka_1\right]=1
\end{equation}
it follows then that 
\begin{equation}
  \label{eq:upper_bound_alpha_check}
  \checka_1 = \sup_{\mathbf{u}\in\Uspace} \frac{J(\mathbf{k}_1,\mathbf{u})}{J^*(\mathbf{u})} =  \inf_{\mathbf{k}\in\Kspace} \left\{ \sup_{\mathbf{u}\in\Uspace} \frac{J(\mathbf{k},\mathbf{u})}{J^*(\mathbf{u})} \right\} 
\end{equation}

This alternative definition of $\checka_1$ is useful to estimate quickly $\hat{\checka}_1$ whenever a grid over $\mathbb{K}\times\mathbb{U}$ has already been evaluated, thus avoiding multiple computations of $\Gamma_{\alpha}(\mathbf{k})$ until its maximum reaches $1$. By Eq.\eqref{eq:upper_bound_alpha_check}, choosing a level of confidence equal to $1$ is then equivalent to looking for the worst-case scenario of the ratio $\frac{J(\mathbf{k},\mathbf{u})}{J^*(\mathbf{u})}$, and therefore suffers from the same pitfall of the worst-case approach. As mentioned in the introduction, this returns over-conservative solution, and is not suited for random variable with unbounded support.


A high value of $\checka_1$ corresponds to a high ratio $\frac{J(\mathbf{k},\mathbf{u})}{J^*(\mathbf{u})}$ for at least one particular $\mathbf{u}$. In that sense, $J(\checkk_1,\mathbf{u})$ may be possibly very high.
More generally, for a level of confidence $p$ fixed, $\checkap$ is the slackness needed to be able to reach the probability $p$. So, in addition to the value of the estimate $\checkkp$, we have an indicator of the robustness associated. We can then see that it is sensible to study jointly $p$, $\checkap$ and $\checkkp$ in order to make a final decision.


\subsection{Concurrence between $p$ and $\alpha$}%Finding a balance between $p$ and $\checkap$}%Selection of a level of confidence}
\label{ssec:balance}
For a practical purpose, we suppose that we have sampled $N$ values of $\mathbf{U}$, namely $\{\mathbf{u}^i\}_{1\leq i \leq N}$.
In order to estimate the quantities introduced above, there are two starting points: either fixing $p$ or fixing the maximal threshold $\alpha$. 
The most trivial way is to set $\alpha$, to find the couples of points $(\mathbf{k},\mathbf{u})$ verifying $J(\mathbf{k},\mathbf{u}) \leq \alpha J^*(\mathbf{u})$, and then to estimate the probability $\Gamma_{\alpha}(\mathbf{k})$ defined in Eq.~\eqref{eq:def_Ralpha} as 
\begin{equation}
\hat{\Gamma}_{\alpha}(\mathbf{k}) = \frac{\# \left\{\mathbf{u}^i \mid J(\mathbf{k},\mathbf{u}^i) \leq \alpha J^*(\mathbf{u}^i)\right\}}{N}
\end{equation}
If $\alpha$ is chosen too small, the resulting $\hat{p}=\max_{\mathbf{k}} \hat{\Gamma}_{\alpha}(\mathbf{k})$ will be also too small, meaning that the cost function will overshoot the value $\alpha J^*(\mathbf{U})$ with high probability.

Similarly, if $p$ is fixed, the corresponding $\hat{\checka}_p$ is computed by searching for the smallest $\alpha$ such that $\max_{\mathbf{k}} \hat{\Gamma}_{\alpha}(\mathbf{k})$ is equal to the level $p$ fixed beforehand. In the end, if the value $\hat{\checka}_p$ is too large, the relaxation needed to get acceptable values with probability $p$ is very high, so the resulting estimation $\hat{\checkk}_p$ may not be relevant for the future application.
\victor{chapeau car ici je parle de l'estimation en pratique}


Without posing any constraints on $p$ nor on $\alpha$, a compromise between $p$ and $\alpha$ would be preferable, and a rough idea of this balance can be obtained by studying $p \mapsto \checkap$, and particularly its slope.
If this curve presents a steep increase, the multiplicative constant $\checkap$ must be increased by a large amount in order to increase the probability $p$ by a small amount. Interesting couples $(p,\checkap)$ would then be the ones located before an abrupt increase of the slope of $p \mapsto \checkap$.

Another possibility is to combine directly $p$ and $\checkap$ in order to find a compromise. We opted to model this compromise by the ratio $(p/\checkap)$, as it increases with respect to $p$ and decreases with respect to $\checka_p$. The level of confidence $p_{\mathrm{maxratio}}$ is then defined as the maximiser of $p\mapsto p / \checka_p$.



The different estimators introduced in this paper are summarised in Table~\ref{tab:RO_recap}.

\begin{table}[t]
  \centering
\begin{tabular}{lrr}
  \toprule
  Definition & Related quantities & Interpretation \\ \midrule
   $\argmin_{\mathbf{k}\in\Kspace} \Ex_{\mathbf{U}}\left[J(\mathbf{k},\mathbf{U})\right]$& $\kmean$ & Long run performances\\
   $\argmin_{\mathbf{k}\in\Kspace} \Var_{\mathbf{U}}\left[J(\mathbf{k},\mathbf{U})\right]$& $\kvar$  & Steady performances\\
   $\argmax_{\mathbf{k}\in\Kspace} p_{\mathbf{K}^*}(\mathbf{k})$ & $\kmpe$ & Most probable minimiser\\
    $\inf\left\{ \alpha \mid \exists \checkkp \in \Kspace,\, \Gamma_{\alpha}(\checkkp) \geq p \right\}$ & $(p, \checkkp,\checkap)$ & Acceptable values with fixed probability $p$ \\
    $p_{\mathrm{maxratio}} = \argmax p/\checkap$ & $(p_{\mathrm{maxratio}}, \checkk_{\mathrm{maxratio}}, \checka_{\mathrm{maxratio}})$ & Maximal ratio of $p$ and $\checkap$ \\
  \bottomrule
\end{tabular}
\label{tab:RO_recap}
  \caption{Robust estimators, based on a cost function $J$}
\end{table}

% We are now going to compute and compare those estimators on the $J_{\mathrm{BH}}$and $J_{\mathrm{BHswap}}$ function.



% ----------------------------------------------------------------------------
\subsection{Estimation of $\checkkp$ and $\checkap$ for the $J_{\mathrm{BH}}$ and $J_{\mathrm{BHswap}}$ functions}
%Numerical application}
As stated before, we chose to model the uncertainties as a random variable uniformly distributed on $\Uspace$. The bounded nature of $\Uspace$ allow us to consider the estimate introduced before up to a level of confidence $p=1$.
From now on, $\hatkmpe$ is estimated using KDE with Gaussian kernels.
The smallest relaxation $\hat{\checka}_1$ and the corresponding $\hat{\checkk}_1$ has been computed for the $J_{\mathrm{BH}}$ and $J_{\mathrm{BHswap}}$ functions, using a regular grid of $1000 \times 1000$ points on $\Kspace\times\Uspace$. The contour plots of those functions can be seen in the top plots of Figure~\ref{fig:contour_alpha}. The frontier corresponding to the couples of points $(\mathbf{k},\mathbf{u})$ verifying $\{J(\mathbf{k},\mathbf{u}) = \alpha J^*(\mathbf{u})\}$ has been drawn on top of these contour plots, for $\alpha=\hat{\checka}_1$ and an arbitrary $\alpha=1.5<\hat{\checka}_1$, to illustrate the effect of the acceptable region when the relaxation $\alpha$ increases. 
On the bottom plots, the curves $\mathbf{k}\mapsto \hat{\Gamma}_{\alpha}(\mathbf{k})$ for $\alpha=\hat{\checka}_1$ and $\alpha=1.5$ along with the histogram of the minimisers are represented.
One can notice that the relaxation does not present an issue with the accumulation of the minimisers of $\JBH$ at $3.8$, as opposed to the MPE and its dependence on the estimation procedure of the distribution.

\begin{figure*}[!ht]
  \centering
\includegraphics[width=10cm]{Figures/branin_side_66_relax_gamma_both.pdf}
\caption{Top: $J_{\mathrm{BH}}$ and $J_{\mathrm{BHswap}}$ contours. The thick yellow lines are the boundaries of the acceptable region defined for $\hat{\checka}_1$, the thick orange are for $\alpha=1.5$. The red dashed line is the estimation $\hat{\checkk}_1$. Bottom: $\hat{\Gamma}_{\alpha}$ for $\alpha = 1.5$ and $\alpha=\hat{\checka}_1$, and estimated density of the minimisers.}
\label{fig:contour_alpha}
\end{figure*}

% For the $J_{\mathrm{BH}}$ function (left), the $\hat{\checka}_1$, $\hatkmean$ and $\hatkvar$ are quite close, so naturally we would expect a robust estimate to be significantly close to those values. $\hat{\checkk}_1$ supports this intuition, and appears to be very close to $\hatkmean$. 
%Concerning the $J_{\mathrm{BHswap}}$ function (right), $\hat{\checkk}_1$ appears to be closer to $\hatkvar$ this time.

In order to choose a satisfying level of confidence $p$, we are going to study $p\mapsto \hat{\checka}_p$ and $p\mapsto p/\hat{\checka}_p$, as described in~\ref{ssec:balance}.

 The plot of $p\mapsto \hat{\checka}_p$ for the $J_{\mathrm{BH}}$ function on Figure~\ref{fig:ratio_BH} shows what seems to be a piecewise linear behaviour. The last change of slope, i.e.\ for $p\approx 0.9$ corresponds to a local maximum of the ratio, while the first change of slope corresponds to the global maximum of the ratio.
 
 \begin{figure}[!ht]
   \centering
\includegraphics[width=10cm]{Figures/alpha_p_BH.pdf}
\caption{Evolution of the couples $(p,\checkap)$ and corresponding ratio $p/\checkap$ for the $J_{\mathrm{BH}}$ function. The dashed line indicates the level $p$ associated with the highest ratio}
\label{fig:ratio_BH}
\end{figure}
 
 For the $J_{\mathrm{BH}}$ function, the numerical values can be found in Table~\ref{tab:recap_estimates_branin}. One can notice that the different estimates are close to each other for this problem.
 
\begin{table*}[!ht]
\centering
\caption{Estimation performed for $J_{\mathrm{BH}}$, sorted by value}
\label{tab:recap_estimates_branin}
\begin{tabular}{lr}
\toprule
Estimate & Value \\ \midrule
$\hatkvar$ & 1.371 \\ 
$\kest_p,~p=1$ & 1.557 \\ 
$\hatkmean$ & 1.587 \\ 
  $\hatkmpe$ & 1.628 \\
  $\kest_{\mathrm{maxratio}},~\hat{p}_{\mathrm{maxratio}}=0.654$ & 1.637 \\
$\kest_p,~p=0.95$ & 1.672 \\ \bottomrule
\end{tabular}
\end{table*}

Practically speaking, in order to compare the effective values taken by the objective function given an estimate $\kest$ we are going to consider the functions of the form $\mathbf{u} \mapsto J(\kest,\mathbf{u})$, that we will call ``profile of $\kest$''. Those profiles are well suited for the representation of the cost function for an estimate $\kest$ fixed as the uncertain variable is modelled with a 1D uniform random variable.

For the $J_{\mathrm{BH}}$ function, the curves are plotted in Figure~\ref{fig:profiles_branin}. Based on its definition, the profile of $\hat{\checkk}_1$ is always within the shaded region, corresponding to $[J^*(\mathbf{u}), \hat{\checka}_1 J^*(\mathbf{u})]$. The profile of $\hatkmean$ in contrast, exceeds $\hat{\checka}_1 J^*(\mathbf{u})$ for $\mathbf{u}$ close to $5$, while the profile of $\hatkvar$ does for $\mathbf{u}$ close to $0$.
As the different $\kest$ are very close for every criteria introduced in this paper, the profiles are almost undistinguishable.

\begin{figure*}[t]
  \centering
\includegraphics[width=10cm]{Figures/profile_BH_all_estimates_noku.pdf}
\caption{Profiles of the different estimates for the $J_{\mathrm{BH}}$ function, corresponding to the vertical cross sections of the contour. The shaded region corresponds to the interval $[J^*(\mathbf{u}), \hat{\checka}_1 J^*(\mathbf{u})]$. The red dashed line, corresponding to the values taken by the cost function when the control variable is set to $\mathbf{k}_1$, is always between $[J^*(\mathbf{u}), \hat{\checka}_1 J^*(\mathbf{u})]$ per design.}
\label{fig:profiles_branin}
\end{figure*}

We are now going to shift our focus on the $J_{\mathrm{BHswap}}$ function, as Figure~\ref{fig:ratio_BHs} provides the plots of $p\mapsto \hat{\checka}_p$ and $p\mapsto p/\hat{\checka}_p$.
\begin{figure}[ht]
  \centering
\includegraphics[width=10cm]{Figures/alpha_p_BHs.pdf}
\caption{Evolution of the couples $(p,\checkap)$ and corresponding ratio $p/\checkap$ for the $J_{\mathrm{BHswap}}$ function. The dashed line indicates the level $p$ associated with the highest ratio}
\label{fig:ratio_BHs}
\end{figure}

Compared to the plots for $J_{\mathrm{BH}}$ in Figure~\ref{fig:ratio_BH}, $\hat{\checka}_p$ exhibits a smoother behaviour for the $J_{\mathrm{BHswap}}$ function as no change of slope is easily discernable. The computation of the ratio advocates for a level of confidence $p_{\mathrm{maxratio}}$ close to $0.75$. The numerical values of the estimations $\hat{k}$ presented in Table~\ref{tab:recap_estimates_BHs} show that, contrary to the $J_{\mathrm{BH}}$ function, the objective are concurrent.

\begin{table*}[!h]
\centering
\caption{Estimations performed for $J_{\mathrm{BHswap}}$, sorted by value}
\label{tab:recap_estimates_BHs}
\begin{tabular}{lr} \toprule
Estimate & Value \\ \midrule
$\kest_{\mathrm{MPE}}$ & 0.606 \\ 
$\kest_{\mathrm{maxratio}},~ \hat{p}_{\mathrm{maxratio}}=0.766$ & 1.537 \\ 
$\hatkmean$ & 1.752 \\ 
$\kest_p,~p=0.90$ & 2.112 \\ 
$\kest_p,~p=0.95$ & 2.457 \\ 
$\hatkvar$ & 2.638 \\ 
$\kest_1,~p=1$ & 2.798   \\ \bottomrule
\end{tabular}
\end{table*}


In a similar fashion, the profiles of the different estimates for the $J_{\mathrm{BHswap}}$ function are shown in Figure~\ref{fig:profiles_branin_switch}.

In this case, $\hatkmpe$, $\hatkmean$ and $\hat{\checkk}_{\mathrm{maxratio}}$ present a similar behaviour. They perform very well for $\mathbf{u}>2$, especially for $\hatkmpe$ which is very close to the minimal value; However for $\mathbf{u}<2$, they produce high values of the function.

The performances of $\hat{\checkk}_1$ are closer to the performances of $\hatkvar$ for this function, but it performs worse than $\hatkmean$ and $\hatkvar$ for $\mathbf{u}>2$, even though its range is designed to stay within the interval $[J^*(\mathbf{u}); \hat{\checka}_1 J^*(\mathbf{u})]$.


\begin{figure*}[t]
  \centering
\includegraphics[width=10cm]{Figures/profile_BHs_all_estimates_noku.pdf}
\caption{Profiles of the different estimates for the $J_{\mathrm{BHswap}}$ function. Those profiles are the vertical cross sections of the contours above. The shaded region corresponds to the interval $[J^*(\mathbf{u}), \hat{\checka}_1 J^*(\mathbf{u})]$. The red dashed line, corresponding to the values taken by the cost function when the control variable is set to $\mathbf{k}_1$, is always between $[J^*(\mathbf{u}), \hat{\checka}_1 J^*(\mathbf{u})]$ per design.}
\label{fig:profiles_branin_switch}
\end{figure*}





% ----------------------------------------------------------------------------
% ----------------------------------------------------------------------------
\clearpage

% ----------------------------------------------------------------------------
\section{Robust calibration of a numerical model}
\label{sec:SWE_application}
% ----------------------------------------------------------------------------

\subsection{Calibration of a toy numerical model}
 We will follow \citeauthor{kennedy_bayesian_2001}'s approach in order to establish the function $\mathcal{G}$ described in the first section in Eq.~\eqref{eq:def_cost_fun}, and the resulting cost function $J$.
 
 Calibration of a numerical model is usually based on the comparison between the numerical model and some observations in a fixed time interval $[0, T]$ called assimilation window.
The physical system at stake is based on an input $\mathbf{u} \in \mathcal{U}$, that represents some operating conditions. This system can then be seen as a map from $\mathcal{U}$ to $\mathcal{Y}$, denoted as $\mathcal{M}^o : \mathbf{u}\mapsto \mathcal{M}^o(\mathbf{u})$. 

In addition to the operating conditions $\mathbf{u}$, the numerical model is also dependent on some other input $\mathbf{k}\in\mathcal{K}$. This additional parametrization comes usually from the successive simplifications needed to implement a numerical model of the physical system observed, and from some unobserved physical parameters. These parameters need to be calibrated accordingly, so that the numerical model can be used to predict the behaviour of the physical system under different operating conditions.

The true value taken by the physical system during this window is denoted by $\mathcal{M}^o(\mathbf{u}^{\mathrm{true}}) \in \mathcal{Y}$, where $\mathbf{u}^{\mathrm{true}} \in \mathcal{U}$ is unknown. The misfit $\mathcal{G}$ and the cost function $J$ defined Eq.\eqref{eq:def_cost_fun} can then be defined as
\begin{equation}
J(\mathbf{k},\mathbf{u}) = \frac12 \| \mathcal{G}(\mathbf{k},\mathbf{u}) \|^2 = \frac12 \|\mathcal{M}(\mathbf{k},\mathbf{u}) - \mathcal{M}^{o}(\mathbf{u}^{\mathrm{true}}) \|^2
\end{equation}
\subsection{The Shallow Water equations}

The model we wish to calibrate is based on the Shallow Water equations, described Eq.~\eqref{eq:SWE_equations}. $h$ is the height of the water column, $q$ is the discharge, and $z$ is the bathymetry, while $g$ is the usual gravitation constant. The parameter to calibrate, $\mathbf{k}$, is the quadratic friction term, proportional to the square of the inverse of Manning-Strickler coefficient. The environmental parameter $\mathbf{u}$ is the amplitude of a sine wave of period $1$. The domain of those two parameters are $\mathcal{K} = [0.0, 1.3]$ and $\mathcal{U} = [0.5, 0.7]$.


\begin{align}
  \label{eq:SWE_equations}
  \left\{
    \begin{array}{rl}
      \partial_t h + \partial_x q &= 0 \\
      \partial_t q + \partial_x \left(\frac{q^2}{h} + \frac{g}{2}h^2\right) &= - gh\partial_x z - \mathbf{k}q |q|h^{-7/3} \\
      h(0, t) &= 20.0 + 3\cdot \sin \left(\frac{2\pi t}{2}\right) + 1.5\cdot \mathbf{u} \cdot \sin\left(2 \pi t\right)\\
      \partial_x q(0, t) &= 0
      \end{array}
  \right.
\end{align}
These equations are integrated using a finite-volume scheme on a discretized domain $[0, L]$, up to a time $T$. The output of the computer code is the sea surface height $h$, on the center of all the volumes and at all the time-steps, that will be denoted $\mathcal{M}(\mathbf{k},\mathbf{u})$.
In this setting, the variable input is uniformly distributed on $\mathcal{U}$.
To generate the observations, we choose $\mathbf{u}^{\mathrm{true}}=2/3$, and define $\mathcal{M}^o$ based on the computer model $\mathcal{M}$, such that $\mathcal{M}^o(\mathbf{u}^{\mathrm{true}}) = \mathcal{M}(\mathbf{k}^{\mathrm{true}}, \mathbf{u}^{\mathrm{true}})$. 
This true value $\mathbf{k}^{\mathrm{true}} = (k_1^{\mathrm{true}}, k_2^{\mathrm{true}},\dots, k_{N_\mathrm{vol}}^{\mathrm{true}})$ is not constant over the whole domain, and is defined as
\begin{equation*}
{k}^{\mathrm{true}}_i = 0.2\cdot \left(1 + \sin\left(\frac{2 \pi x_i}{L}\right)\right)
\end{equation*}
where $x_i$ is the center of the $i$-th volume.
\subsection{Estimation}
Given that the numerical model is expensive to evaluate, it has first been evaluated on a relatively small regular grid. A metamodel based on Gaussian process is constructed and the PEI criterion~\citep{ginsbourger_bayesian_2014,bossek_learning_2015} allow us to add point to evaluate and to better capture the locus of the conditional minimisers $\{(\mathbf{k}^*(\mathbf{u}), \mathbf{u}) \mid \mathbf{u} \in \Uspace \}$. A bigger regular grid is evaluated by the metamodel once the design space has been sufficiently explored. The different steps of the estimation are illustrated Figure~\ref{fig:estimation_swe}. We can see that the estimation for this problem seems less problematic than the analytical examples shown above, as all derived quantites are unimodal.
The different estimates however take a wide range of values, as seen on Table~\ref{tab:recap_estimates_metaSWE}. As a basis for comparison, the global minimiser of $J$ over $\Kspace \times \Uspace$ has been computed, and $\kest_{\mathrm{global}}$ is then obtained by discarding the $\mathbf{u}$ value.

As discussed earlier, the estimation of the MPE is dependent on the method used, and then its estimation using KDE in the middle plot of Figure~\ref{fig:estimation_swe} is here only for illustrative purposes.

\begin{figure}[!h]
\centering
\includegraphics[width=.7\textwidth]{Figures/RobustSWEmis_plots.pdf}
\caption{Procedure of robust calibration for the shallow water problem. Top: contours of $J$, conditional minimisers and $\{(\mathbf{k},\mathbf{u}) | J(\mathbf{k},\mathbf{u}) = \hat{\alpha}_1 J^*(\mathbf{u})\}$. Middle: Conditional moments and histogram and KDE of the conditional minimisers. Bottom: $\hat{\Gamma}_{\alpha_p}$ for different levels $p$}
\label{fig:my_label}
\end{figure}
\begin{table*}[!h]
\centering
\caption{Calibrated values of $\mathbf{k}$ according to different criteria}
\begin{tabular}{lr} \toprule
Estimator & Value \\ \midrule
$\hatkmpe$ & 0.249 \\
$\kest_{\mathrm{global}}$ & 0.290 \\
$\hatkp,~p=1$ & 0.423 \\ 
$\hatkp,~p=0.90$ & 0.458 \\ 
$\hatkmean$ & 0.501 \\ 
$\hatkp,~p=0.80$ & 0.505 \\ 
$\hatkp,~p=0.70$ & 0.560 \\ 
$\hatkvar$ & 0.990\\ \bottomrule
\end{tabular}
\caption{Estimation of $\mathbf{k}$ according to different criterion}
\label{tab:recap_estimates_metaSWE}
\end{table*}
Similarly as for $\JBH$ and $\JBHS$, the profiles are depicted Figure~\ref{fig:profiles_swe}.
\begin{figure}[!h]
  \centering
  \includegraphics[width=.8\textwidth]{Figures/profile_swe}
  \caption{Profiles for the calibration problem}
\label{fig:profiles_swe}
\end{figure}
We can see that the performances of $\kest_1$ and $\hatkmean$ are very similar, but $\kest_1$ has better performances when $\mathbf{u} > 0.6$. When comparing with the global optimiser $\kest_{\mathrm{global}}$, $\kest_1$ performs better when $\mathbf{u}<0.625$. As $\mathbf{U}$ is uniformly distributed on $\Uspace=[0.5,0.7]$, we can graphically estimate that the probability that $\kest_1$ outperforms $\kest_{\mathrm{global}}$ is greater than $0.5$, that is that $\Prob_\mathbf{U}\left[J(\kest_1,\mathbf{U})<J(\kest_{\mathrm{global}},\mathbf{U})\right] > 0.5$.


The calibration of this numerical model is done in order to get a calibrated value $\kest$, that will be used for forecasts, using the same numerical model. We are now going to compare how well three of those values compare in a forecast context: $\kest_1$, $\hatkmean$ and $\kest_{\mathrm{global}}$, as the first two refer quite naturally to the notion of robustness, and the latter is an example of an estimate that does not take into account the uncertainties.
\clearpage
\subsection{Assessing the quality of the forecast of the calibrated model}
The calibration of the model has been performed on the time-period $[0, T]$, called assimilation window. 
We now want to compare the quality of the different forecasts depending on the criterion used to calibrate the bottom friction.
To define those forecasts, the models will be integrated from $T$ to $T_{\mathrm{pred}}$.

Given the probabilistic nature $\mathbf{U}$, the forecasts will also be probabilistic. We will then compare $\mathcal{M}_{\mathrm{pred}}^o(\mathbf{U})$ and $\mathcal{M}_{\mathrm{pred}}(\mathbf{k},\mathbf{U})$, for a calibrated $\mathbf{k}$.

\subsubsection{Squared forecast error}
Let us assume that the observations result from the parameter $\mathbf{u}'$, while the computer simulation is run using $\mathbf{u}$.
The squared forecast error given $\mathbf{u}$ and $\mathbf{u}'$ is
\begin{equation}
\label{eq:squared_forecast_error}
S_{\mathrm{pred}}(\mathbf{k},\mathbf{u},\mathbf{u}') = \left(\mathcal{M}_{\mathrm{pred}}(\mathbf{k},\mathbf{u}) - \mathcal{M}^{o}_{\mathrm{pred}}(\mathbf{u}')\right)^2
\end{equation}
Averaging over $\mathbf{u}$, $\mathbf{u}'$ defines the mean squared forecast error, defined on every point of the spatial domain, and at every time-steps. This can be done using a Monte-Carlo approximation: as $\mathbf{u}$ and $\mathbf{u}'$ are iid, let us assume that we have a set of samples $\{\mathbf{u}^i\}_{1\leq i \leq N_{\mathbf{u}}}$. 
\begin{equation}
  \label{eq:squared_forecast_error_averaged}
  S(\mathbf{k}) = \frac{1}{N_{\mathbf{u}}^2 }\sum_{i=1}^{N_{\mathbf{u}}}\sum_{j=1}^{N_{\mathbf{u}}} S_{\mathrm{pred}}(\mathbf{k},\mathbf{u}^i,\mathbf{u}^j) = \frac{1}{N_{\mathbf{u}}^2 }\sum_{i=1}^{N_{\mathbf{u}}}\sum_{j=1}^{N_{\mathbf{u}}}  \left(\mathcal{M}_{\mathrm{pred}}(\mathbf{k},\mathbf{u}^i) - \mathcal{M}^{o}_{\mathrm{pred}}(\mathbf{u}^j)\right)^2
\end{equation}
The mean squared forecast error averaged over the whole space and over all the time-steps gives an indication on the overall prediction quality of the prediction given this metric, and are represented on the right plot of Figure~\ref{fig:forecast_squared_error}. We can see that $\kest_{\mathrm{global}}$ performs slightly better than $\kest_1$, itself performing slightly better than $\hatkmean$.
Averaging $S(\mathbf{\kest}$ over only the time steps between $T$ and $T_{\mathrm{pred}}$, we have an indication on the quality of the forecast in the squared sense depending on the spatial position, as seen on the left plot of Figure~\ref{fig:forecast_squared_error}.
\begin{figure*}[!h]
\centering
\includegraphics[width=.9\textwidth]{Figures/L2_full.pdf}
\caption{Quality of forecast, depending on the calibrated parameter. The left figures show the metric averaged over time, while the right figure show the mean forecast error, averaged over time and space}
\label{fig:forecast_squared_error}
\end{figure*}
We can see that the mean forecast error squared, on the right side, averaged over time and space is the smallest for $\kest_{\mathrm{global}}$, while $\kest_{1.0}$ performs slightly better $\hatkmean$.
\begin{figure}[!h]
  \centering
  \includegraphics[width=.9\textwidth]{Figures/CRPS_full}
  \caption{CRPS computed for different calibrated parameters}
\label{fig:forecast_crps}
\end{figure}

% \begin{figure*}[!h]
% \centering
% \includegraphics[width=10cm]{Figures/distrib_L2norm.pdf}
% \caption{Distribution of the forecast squared error, averaged over the different values of the uncertain parameter in the numerical simulations \victor{doublon avec fig.\ref{fig:forecast_squared_error} ?}}
% \label{fig:forecast_squared_error_distrib}
% \end{figure*}

\subsubsection{Continuous Ranked Probability Score}
Given the random variables $\mathcal{M}_{\mathrm{pred}}(\kest, \mathbf{U})$ and $\mathcal{M}_{\mathrm{pred}}^o(\mathbf{U})$, representing the probabilistic forecast and the probabilistic observations, we can define the cumulative distribution functions (CDF) $F_{\mathrm{pred}}(\cdot, \kest)$ and $F^o_{\mathrm{pred}}(\cdot)$
The Continuous ranked probability score (CRPS) measures the squared difference between the predicted CDF $F_{\mathrm{pred}}$ using a calibrated value, and the CDF of the observations $F_{\mathrm{pred}}^o$. 
\begin{equation}
\label{eq:checkloss}
\rho_{\tau}(x) = \begin{cases}
    \tau |x| \quad \text{ if } x \geq 0 \\
    (1- \tau) |x| \quad \text{ if } x < 0
\end{cases}
\end{equation}

Given the time index $j$, and the volume indexed by $i$, the quantile of order $\tau\in [0, 1]$ of $\mathcal{M}(\kest, \mathbf{U})_{(i,j)}$ is $q_{(i,j)}(\tau)$.
Given an obse
\begin{figure*}[t]
\includegraphics[width=12cm]{Figures/forecast_metric.pdf}
\caption{Quality of forecast, depending on the calibrated parameter. The left figures show the metric averaged over time, while the right figures show}
\label{fig:forecast_metric}
\end{figure*}


% \subsection{The Lorenz-63 model}
% A classical toy problem in data assimilation is the Lorenz-63 model, introduced in \cite{lorenz_deterministic_1963}. 
% % xb
% The evolution of the state vector $(x,y,z)$ is described by the non-linear differential system of Eq.~\eqref{eq:lorenz_equations}
% \begin{align}
%   \label{eq:lorenz_equations}
%   \left\{
%     \begin{array}{l}
%       \dot{x} = \sigma\left(y-x \right) \\
%       \dot{y} = \rho x - y - xz \\
%       \dot{z} = xy - \beta z
%       \end{array}
%   \right.
% \end{align}
% Despite its apparent simplicity, this system is known to show chaotic behaviour for $(\sigma,\rho,\beta) = (10, 28, 8/3)$. 
% This differential system is solved numerically using a Runge-Kutta scheme, over $N=100$ time-steps of length $\Delta t = 0.05$. In that time-period, the chaotic behaviour is not yet too apparent.

% The inputs of the computer code are the initial conditions $\left(x_0,y_0,z_0\right)$ and $(\sigma, \beta, \rho)$. The parameter to calibrate is $\mathbf{k}=x_0-1$ with $\mathbf{k}\in[-0.05,0.05]$, and the uncertain parameter is $\mathbf{u}=\beta - 8/3$ with $\mathbf{u} \in \Uspace=[-0.05, 0.05]$. Finally, we suppose that the random variable $\mathbf{U}$ follows an uniform distribution on $\Uspace$. Solving numerically the differential system using those settings yields $x_{i}(\mathbf{k},\mathbf{u})$ for $i$ between $1$ and $N$.

% This problem of calibration will be studied in a twin experiments framework: the target state $x^{o}_i$ for $1\leq i \leq N$ has been generated by the computer code, where the parameters used to generate it are detailed in Table~\ref{tab:lorenz_modelling}. One can notice the discrepancy of $\rho$ between the observations and the model, introduced to ensure that no perfect fit can be achieved, for any set of parameters. 
% \begin{table*}[h]
%   \caption{Choice of modelling of the Lorenz-63 system}
%   \label{tab:lorenz_modelling}
% \begin{tabular}{crrl}
%   \tophline
%   Parameter & Observation: $x_{1:N}^o$ & Modelling: $\mathcal{G}(\mathbf{k},\mathbf{u})$ & Type \\
%   \middlehline
%  $\sigma$ & $10$ & $10$ & Fixed\\
%   $\rho$ & $29$ & $28$ & A priori chosen and fixed \\
%   $\beta$ & ${8}/{3}$ & ${8}/{3} + \mathbf{u}$ & A priori chosen and uncertain \\
%  $x_0$ & $1$ & $1 + \mathbf{k}$ & Calibrated \\
%   $y_0$ & $0$ & $0$ & Fixed \\
%  $z_0$ & $0$ & $0$ & Fixed \\
%   \bottomhline
% \end{tabular}
% \end{table*}


% The misfit between the observations and the output of the numerical model is quantified using the cost function:
% \begin{equation}
% \label{eq:cost_fun_lorenz}
% J(\mathbf{k},\mathbf{u}) = \frac{1}{2}\|\mathcal{G}(\mathbf{k},\mathbf{u}) \|^2 = \frac{1}{2} \|x_{1:N}(\mathbf{k},\mathbf{u}) - x^o_{1:N} \|^2 = \frac{1}{2}\sum_{i=1}^N (x_i(\mathbf{k},\mathbf{u}) - x_i^o)^2
% \end{equation}

% This toy model is cheap to evaluate, so the global minimum can be computed using numerical methods, at different starting points to avoid local minimums.
% \begin{equation}
% \label{eq:min_global_lorenz}
% \min_{(\mathbf{k},\mathbf{u}) \in \Kspace\times\Uspace} J(\mathbf{k},\mathbf{u}) = 38.886712815 \quad \argmin_{(\mathbf{k},\mathbf{u}) \in \Kspace\times\Uspace} J(\mathbf{k},\mathbf{u}) = (0.02414384, -0.00551042)
% \end{equation}

% \subsubsection{MPE and conditional moments}
% Figure~\ref{fig:lorenz_kde_moments} shows the cost function $J$, computed on a regular grid over $\Kspace\times\Uspace$. Using this grid, the conditional mean and standard deviation have been computed, while the MPE has been estimated by KDE. For a single $\mathbf{u}$, it may appear that there are multiple $\mathbf{k}^*(\mathbf{u})$. This is due to the fact that the conditional minimiser jumps back and forth between the two curves.

% \begin{figure}[!h]
% \includegraphics[width=12cm]{Figures/lorenz_kde_moments.pdf}
% \caption{Cost function, locus of conditional minimisers, MPE estimated with KDE, conditional moments and their associated estimates}
% \label{fig:lorenz_kde_moments}
% \end{figure}

% \subsubsection{Relaxation of the constraint}
% Based on the computations described before, the coefficient $\checkap$ have been computed for $p=0.9$, $0.95$ and $p=1$, and reported in Table~\ref{tab:recap_estimates_lorenz}. The curve $R_{\checka_1}$ is plotted in Figure~\ref{fig:lorenz_relax}, where the acceptable region for $\checka_1$ is filled in yellow.

% \begin{figure}[!h]
% \includegraphics[width=12cm]{Figures/lorenz_f3.pdf}
% \caption{Relaxation of the constraint for the Lorenz model. Acceptable region in yellow}
% \label{fig:lorenz_relax}
% \end{figure}

% We can see that for $\mathbf{k}$ being approximately between $0$ and $0.01$, the probability $R_{\checka_1}$ is very close to $1$. Indeed, the cost function seems to be relatively flat on this region, as $\checkk_{0.9} \approx \kmpe$.

% \begin{table*}[!h]
%   \caption{Estimates computed numerically on the Lorenz-63 model}
%   \label{tab:recap_estimates_lorenz}
%   \begin{tabular}{*{3}{r}}\tophline
%     Estimate & Value & Related quantities\\
%     \middlehline
%     $\hatkmean$        & $ 0.008779$ &$\left(\mu(\kmean),\sigma(\kmean)\right) = (42.965, 4.921)$\\
%     $\hatkvar$         & $-0.025338$ &$\left(\mu(\kvar),\sigma(\kvar)\right)= (50.704, 2.024)$\\
%     $\hatkmpe$         & $ 0.008779$ & $\Gamma_1(\kmpe) = 0.053$ \\
%     $\hat{\checkk}_{0.9}$ & $ 0.008729$ & $\hat{\checka}_{0.9}  = 1.202788$ \\
%     $\hat{\checkk}_{0.95}$& $ 0.006078$ & $\hat{\checka}_{0.95} = 1.226050$ \\
%     $\hat{\checkk}_{1}$   & $ 0.000275$ & $\hat{\checka}_{1}    = 1.311002$ \\
%     \bottomhline
%     \end{tabular}
% \end{table*}

% \subsubsection{Selection of the couple $(p,\checkap)$}
% The evolution of $\checkap$ with respect to $p$ is shown on the left plot of Figure~\ref{fig:evolution_p_lorenz}. We can see that $\checkap$ presents two abrupt positive change of slope, for $p$ set to $0.85$ and just before $1$. Those confidence levels corresponds to the local maxima of the ratio $p/\checkap$, that is shown on the right plot of Figure~\ref{fig:evolution_p_lorenz}. The profile of $\checkk_{0.85}$ corresponding to the local maximum of the ratio, has not been plotted as it corresponds to the same profile of $\kmean$.

% \begin{figure*}[!h]
% \includegraphics[width=12cm]{Figures/evol_p_lorenz.pdf}
% \caption{Evolution of $p$ and $\checkap$}
% \label{fig:evolution_p_lorenz}
% \end{figure*}
% In order to compare the values taken by the objective function, the profiles have been represented on Figure~\ref{fig:lorenz_profiles}.
% \begin{figure}[!h]
% % \includegraphics[width=12cm]{Figures/lorenz_cdf_profile.pdf}
% \includegraphics[width=12cm]{Figures/profile_lorenz_all_estimates.pdf}
% \caption{Profiles of different $\mathbf{k}$ for the Lorenz model}
% \label{fig:lorenz_profiles}
% \end{figure}
% The profile of $\kmean$ presents a good insight on the performances of this estimate: for most values of the uncertain variables, the objective is low. In contrast, for $\mathbf{u}$ close to $0.05$, a maximum is reached. 
%\begin{figure}[!h]
%\includegraphics[width=8.3cm]{Figures/cdf_lorenz_with_ratio.pdf}
%\caption{CDF corresponding to different $\mathbf{k}$ for the Lorenz model}
%\label{fig:lorenz_cdf}
%\end{figure}



% \clearpage
% \subsection{Context: Shallow water equations and bottom friction}

% The quality of water circulation modelling in coastal areas is largely dependent on the boundary conditions, (e.g.\ the tide), the bathymetry and the friction induced by the rugosity of the ocean bed \citep{boutet_estimation_2015}. The latter will then be estimated in a robust manner, by applying the criteria introduced above.


% The model studied is based on the one-dimensional shallow water equations, described Eq.~\eqref{eq:SWE_equations}, solved numerically on a domain $D$. $h$ is the height of the water column, $q$ is the discharge, and $z$ is the bathymetry, while $g$ is the usual gravitation constant. $\mathbf{k}$ is a quadratic friction term, proportional to the square of the inverse of Manning-Strickler coefficient.
% \begin{align}
%   \label{eq:SWE_equations}
%   \left\{
%     \begin{array}{l}
%       \partial_t h + \partial_x q = 0 \\
%       \partial_t q + \partial_x \left(\frac{q^2}{h} + \frac{g}{2}h^2\right) = - gh\partial_x z - \mathbf{k}q |q|h^{-7/3} \\
%       h(0, t) = 20.0 + \mathrm{Amplitude} \cdot \sin \left(\frac{2\pi}{\mathbf{u}}t\right) \\
%       q(0, t) = 0
%       \end{array}
%   \right.
% \end{align}

% The estimation of the bottom friction and/or topography has been treated in a deterministic setting, % in Section~\ref{sec:deterministic}
% using tools from variational data assimilation~\cite[e.g.][]{das_estimation_1991,das_variational_1992,honnorat_identification_2010}.
% If the boundary conditions are not well-known, the estimation will then compensate this misspecification. The resulting estimate will then be optimal for this particular configuration, but will not acknowledge the uncertainty upon the boundary conditions. Instead, we are looking for an estimate that is robust considering the variability of the uncertain boundary conditions.


% The problem of the estimation of the bottom friction will be studied using twin experiments. For this purpose, a few references simulations have been performed in order to produce different observations $\mathbf{y}^o$. Those scenarios are detailed Table~\ref{tab:optim_scenar}. The true bottom friction $\mathbf{k}^t$ is the same for all simulations, and is a non-constant function of the spatial domain $D = [0, L]$ defined as:
% \begin{equation}
%   \label{eq:defnition_kt}
%   \mathbf{k}^t= \mathbf{k}^t(x) = 0.2\cdot (1 + \sin(2\pi x/L))
% \end{equation}
% However, the space $\Kspace$ in which we wish to estimate the bottom friction is one-dimensional. This leads to the strict positivity of the cost function, as no perfect fit can be achieved.

% As presented on Table~\ref{tab:RO_modelling}, we consider two a priori chosen variable: $\mathbf{u} \in \Uspace$ that is the period of the sine wave on the left boundary. The aleatoric uncertainty on this variable is modelled by a random variable distributed uniformly on $\Uspace$.
% The amplitude of the sine wave is also chosen a priori and set to an arbitrary value, that may be different from the true amplitude used to generate $\mathbf{y}^o$. 



% \begin{table*}[t]
%   \caption{Modelling of the estimation problem}
%   \label{tab:RO_modelling}
% \begin{tabular}{rrr}
%   \tophline
%   Parameter & Modelling & Type of parameter \\
%   \middlehline
%   Bottom friction: $\mathbf{k}$  &$\Kspace=[0, 1]$ & Calibrated parameter \\
%   Period: $\mathbf{U}$ & $\mathrm{Unif}\left([14.7, 15.3]\right)$ & A priori chosen and uncertain\\
%   Amplitude &  5.0 & A priori chosen and fixed\\
%   \bottomhline
% \end{tabular}
% \end{table*}

% % ----------------------------------------------------------------------------
% \section{Deterministic optimisation}
% \label{ssec:deterministic_optimisation}
% The problem has first been studied in a deterministic setting. The following optimisation problem has been performed numerically:
% \begin{equation}
%   \label{eq:opt_deter}
%   \text{Minimise } \mathbf{k}\in\Kspace \mapsto J(\mathbf{k},\mathbf{u}^b)\quad \text{for }\mathbf{u}^b= \Ex_{\mathbf{U}}[\mathbf{U}] = 15.0
% \end{equation}
% At the same time, the background value of the amplitude is fixed and set to $5.0$.
% Different scenario have been established (i.e.\ different $\mathbf{u}^t$ and amplitude producing different $\mathbf{y}^o$), and are detailed in Table~\ref{tab:optim_scenar}. The configurations correspond to different values of the amplitude (letter~``A'') and the period (letter~``P''). If the truth value is greater (resp.\ smaller) than the background value, the lowercase letter ``p'' follows (resp.\ ``m'').

% On the same table, are shown the result $\kest$ of the optimisation defined in Eq.~\eqref{eq:opt_deter} for each one of the different operating conditions.


% \begin{table*}[t]
%   \caption{Settings for reference simulations, and optimisation results}
%   \label{tab:optim_scenar}
% \begin{tabular}{rrrrrrrr} \tophline
% Scenario for $\mathbf{u}^t$: &$\Ex_{\mathbf{U}}[\mathbf{U}]$  & Pm & ApPm & AmPm & Pp & ApPp & AmPp \\ \middlehline
%   $\mathbf{u}^t- \mathbf{u}^b$ & 0.0            & -0.1  & -0.2 & -0.1 & +0.2 & +0.1 & +0.2 \\
%   $\mathrm{Amplitude}- 5.0$   & 0.0             &  0.0  & +0.1 & -0.2 &  0.0 & +0.2 & -0.1 \\ \middlehline
%   $\kest$      & 0.1043          & 0.2028 & 0.2709 & 0.2427 & 0.0  & 0.0 & 0.0015 \\\bottomhline
% \end{tabular}
% \end{table*}


% Optimising without uncertainties (meaning that the $\mathbf{u}^b=\mathbf{u}^t$) yields an optimal value $\kest$ close to $0.10$, that we define to be $\kest_{\mathrm{opt}}$
% We can observe two different regimes, depending on the sign of $\mathbf{u}^t-\Ex_{\mathbf{U}}[\mathbf{U}]$. If this quantity is positive, optimisation will produce an estimation close to $0$. If it is negative, the estimate is larger than $0.20$.

% % ----------------------------------------------------------------------------
% \subsection{Robust estimates}
% In order to compute the robust estimates, the cost function has been evaluated on a regular grid of $150\times150$ points over $\Kspace\times\Uspace$. $\kmean$, $\kvar$, $\checkk_{.9}$, $\checkk_{.95}$ and $\checkk_{1}$ (along with their corresponding factors $\checka_p$) have been evaluated for the different configurations, detailed in Table~\ref{tab:optim_scenar}. As shown on Table~\ref{tab:res_robust}, $\checkkp$ does not vary significantly when the level of confidence $p$ increases.

% \begin{table*}[t]
%   \caption{Mean and variance optimisers, and $\checkkp$ for different levels of confidence $p$}
%   \label{tab:res_robust}
%   \begin{tabular}{rrrrrrrr} \tophline
%     Scenario&$\Ex_{\mathbf{U}}[\mathbf{U}]$ & Pm & ApPm & AmPm & Pp & ApPp & AmPp \\  \middlehline
%   $\kmean$ & 0.1678    & 0.2214  & 0.2617 & 0.2685 & 0.0872 & 0.0738 & 0.1275 \\
%   $\kvar$  & 1.0      &  1.0  & 1.0 & 1.0 &  1.0 & 1.0 & 1.0 \\ \middlehline
%   $\checkk_{.90}$ & 0.1074 & 0.1074 & 0.0872 & 0.1678 & 0.1074  & 0.0738 & 0.1275 \\
%   $\checka_{.90}$    & 1.1754  & 1.1718 & 1.1779 & 1.1293 & 1.1768  & 1.1807 & 1.1406 \\\middlehline

%   $\checkk_{.95}$ & 0.1074 & 0.1074 & 0.0872 & 0.1611 & 0.1074  & 0.0671 & 0.1275 \\
%   $\checka_{.95}$ & 1.1802 & 1.1806 & 1.1868 & 1.1320 & 1.1861  & 1.2027 & 1.1525 \\\middlehline
%     $\checkk_{1}$& 0.1074 & 0.1074 & 0.0872 & 0.1611 & 0.1074  & 0.0671 & 0.1275 \\
%   $\checka_{1}$ & 1.1944    & 1.1922 & 1.1961 & 1.1321 & 1.1976  & 1.2035 & 1.1682 \\\middlehline
% \end{tabular}
% \end{table*}


% In that case, the minimiser of the variance $\kvar$ is the right boundary of $\Kspace$, regardless of the value of $\mathbf{u}^t$. This is due to the fact that at this point, the cost function produces bad, yet steady performances. From a physical point of view, increasing the bottom friction will decrease the momentum. The movement of the water surface will be damped, and the operating conditions will have less effects.


% Looking at $\kmean$, we can see that the estimates obtained are larger than $0.22$ when $\mathbf{u}^t< \Ex_{\mathbf{U}}[\mathbf{U}]$ (that is for Pm, ApPm, and AmPm). Those results are similar to the one obtained in the deterministic setting of Table~\ref{tab:optim_scenar}. However, for $\mathbf{u}^t> \Ex_{\mathbf{U}}[\mathbf{U}]$ (Pp, ApPp, and AmPp), $\kmean$ is much closer to $\kest_{\mathrm{opt}}$, the optimal value without uncertainties.

% Now for $\checkk_1$, while the estimates for scenarios Pp, ApPp and AmPp are similar to $\kmean$, the obtained values for the other scenarios are close to $\kest$.
% Figures~\ref{fig:SWE_checkalpha} and~\ref{fig:SWE_profiles} show the contour plot and profiles of the cost function in centered conditions ($\Ex_{\mathbf{U}}[\mathbf{U}]$ scenario). Figures~\ref{fig:SWE_checkalphaApPp} and \ref{fig:SWE_profilesApPp} (resp. Figures~\ref{fig:SWE_checkalphaAmPm} and~\ref{fig:SWE_profilesAmPm}) show the contour plot and profiles of the cost function for the ApPp scenario (resp. AmPm scenario).

% The concentration of $\checkkp$ close to $\kest_{\mathrm{opt}}$ is caused by the very small value of the cost function at its minimum over $\Kspace\times \Uspace$. Close to this minimum, the region of acceptable $\mathbf{k}$ will be growing slowly as $\alpha$ increases, weighing the estimation toward $\kest_{\mathrm{opt}}$.

% The fact that we retrieved values close to the optimal one should not overlook that a robust estimate is not necessarily $\kest_{\mathrm{opt}}$, and that this result is specific to this problem.

% \clearpage
% \begin{figure*}[!h]
% \includegraphics[width=8cm]{Figures/SWE_big3.pdf}
% \caption{Top: Cost function of the SWE problem ($\Ex_{\mathbf{U}}[\mathbf{U}]$ scenario) and its conditional minimisers. The thick yellow lines represent the boundary of the region where $\phi_\alpha<0$. The red dashed line is the estimate $\checkk_{1}$. Bottom: $R_{\checka}$}
% \label{fig:SWE_checkalpha}
% \end{figure*}

% \begin{figure*}[!h]
%   \centering
%   \includegraphics[width=8cm]{Figures/profile_swe.pdf}
%   \caption{Profiles of different estimates of $\mathbf{k}$ for the $\Ex_{\mathbf{U}}[\mathbf{U}]$ scenario}
%   \label{fig:SWE_profiles}
% \end{figure*}

% \begin{figure*}[!h]
% \includegraphics[width=8cm]{Figures/SWE_ApPp3.pdf}
% \caption{Top: Cost function of the SWE problem (ApPp scenario) and its conditional minimisers. The thick yellow lines represent the boundary of the region where $\phi_\alpha<0$. The red dashed line is the estimate $\checkk_{1}$. Bottom: $R_{\checka}$}
% \label{fig:SWE_checkalphaApPp}
% \end{figure*}

% \begin{figure*}[!h]
%   \centering
%   \includegraphics[width=7cm]{Figures/SWE_ApPp_profile.pdf}
%   \caption{Profiles of different estimates of $\mathbf{k}$ for the ApPp scenario}
%   \label{fig:SWE_profilesApPp}
% \end{figure*}

% \begin{figure*}[!h]
% \includegraphics[width=8cm]{Figures/SWE_AmPm3.pdf}
% \caption{Top: Cost function of the SWE problem (AmPm scenario) and its conditional minimisers. The thick yellow lines represent the boundary of the region where $\phi_\alpha<0$. The red dashed line is the estimate $\checkk_{1}$. Bottom: $R_{\checka}$}
% \label{fig:SWE_checkalphaAmPm}
% \end{figure*}

% \begin{figure*}[!h]
%   \centering
%   \includegraphics[width=8cm]{Figures/SWE_AmPm_profile.pdf}
%   \caption{Profiles of different estimates of $\mathbf{k}$ for the AmPm scenario}
%   \label{fig:SWE_profilesAmPm}
% \end{figure*}

\clearpage
\section*{Conclusion}  
\victor{À faire à la fin}
This paper deals with the problem of robust calibration of a computer code in the presence of uncertain inputs. 

Our contribution consists in introducing a new criterion of robustness, based on the distribution of the minimisers. More specifically, this estimate is bounding the ratio between the cost function and its conditional minimum.
Experimental results show that this new estimate is able to make a compromise between the minimum of the expected value and the minimum of the variance, will also giving the range of the ratio evoked above.

From a practical perspective, this estimation is very expensive: each evaluation of $\mathbf{k}^*$ requires an optimisation procedure. The distribution of the conditional minimisers is then very complicated to compute.
In addition to that, in order to relax the constraint, the cost function must also be evaluated in the vicinity of the conditional minimisers. Computing precisely this estimate is then intractable if each run of the underlying model is expensive (longer than a couple of seconds).


A solution to explore is the use of surrogate models. Based on an initial design of experiment, the (expensive) cost function is replaced by a function, that is cheap to evaluate.
For instance,~\cite{ginsbourger_bayesian_2014}
uses Gaussian Process regression and sequential methods to explore the conditional minimisers.

Finally, the adaptibility of this method has to be studied in a higher dimensional setting. High-dimensional density estimation is a challenge in itself, but is necessary in order to tackle more realistic applications.
%% The following commands are for the statements about the availability of data sets and/or software code corresponding to the manuscript.
%% It is strongly recommended to make use of these sections in case data sets and/or software code have been part of your research the article is based on.

\codeavailability{TEXT} %% use this section when having only software code available


% \dataavailability{TEXT} %% use this section when having only data sets available


\codedataavailability{TEXT} %% use this section when having data sets and software code available


% \sampleavailability{TEXT} %% use this section when having geoscientific samples available



% \appendix
% \section{}    %% Appendix A

% \subsection{}     %% Appendix A1, A2, etc.


% \noappendix       %% use this to mark the end of the appendix section

%% Regarding figures and tables in appendices, the following two options are possible depending on your general handling of figures and tables in the manuscript environment:

%% Option 1: If you sorted all figures and tables into the sections of the text, please also sort the appendix figures and appendix tables into the respective appendix sections.
%% They will be correctly named automatically.

%% Option 2: If you put all figures after the reference list, please insert appendix tables and figures after the normal tables and figures.
%% To rename them correctly to A1, A2, etc., please add the following commands in front of them:

\appendixfigures  %% needs to be added in front of appendix figures

\appendixtables   %% needs to be added in front of appendix tables

%% Please add \clearpage between each table and/or figure. Further guidelines on figures and tables can be found below.



\authorcontribution{TEXT} %% it is strongly recommended to make use of this section

\competinginterests{TEXT} %% this section is mandatory even if you declare that no competing interests are present

\disclaimer{TEXT} %% optional section

\begin{acknowledgements}
TEXT
\end{acknowledgements}




%% REFERENCES

%% The reference list is compiled as follows:

% \begin{thebibliography}{}

% \bibitem[AUTHOR(YEAR)]{LABEL1}
% REFERENCE 1

% \bibitem[AUTHOR(YEAR)]{LABEL2}
% REFERENCE 2

% \end{thebibliography}

%% Since the Copernicus LaTeX package includes the BibTeX style file copernicus.bst,
%% authors experienced with BibTeX only have to include the following two lines:
%%

\bibliographystyle{copernicus}
\bibliography{bibzotero}

%%
%% URLs and DOIs can be entered in your BibTeX file as:
%%
%% URL = {http://www.xyz.org/~jones/idx_g.htm}
%% DOI = {10.5194/xyz}


%% LITERATURE CITATIONS
%%
%% command                        & example result
%% \citet{jones90}|               & Jones et al. (1990)
%% \citep{jones90}|               & (Jones et al., 1990)
%% \citep{jones90,jones93}|       & (Jones et al., 1990, 1993)
%% \citep[p.~32]{jones90}|        & (Jones et al., 1990, p.~32)
%% \citep[e.g.,][]{jones90}|      & (e.g., Jones et al., 1990)
%% \citep[e.g.,][p.~32]{jones90}| & (e.g., Jones et al., 1990, p.~32)
%% \citeauthor{jones90}|          & Jones et al.
%% \citeyear{jones90}|            & 1990



%% FIGURES

%% When figures and tables are placed at the end of the MS (article in one-column style), please add \clearpage
%% between bibliography and first table and/or figure as well as between each table and/or figure.


%% ONE-COLUMN FIGURES

%%f
%\begin{figure}[t]
%\includegraphics[width=8.3cm]{FILE NAME}
%\caption{TEXT}
%\end{figure}
%
%%% TWO-COLUMN FIGURES
%
%%f
%\begin{figure*}[t]
%\includegraphics[width=12cm]{FILE NAME}
%\caption{TEXT}
%\end{figure*}
%
%
%%% TABLES
%%%
%%% The different columns must be seperated with a & command and should
%%% end with \\ to identify the column brake.
%
%%% ONE-COLUMN TABLE
%
%%t
%\begin{table}[t]
%\caption{TEXT}
%\begin{tabular}{column = lcr}
%\tophline
%
%\middlehline
%
%\bottomhline
%\end{tabular}
%\belowtable{} % Table Footnotes
%\end{table}
%
%%% TWO-COLUMN TABLE
%
%%t
%\begin{table*}[t]
%\caption{TEXT}
%\begin{tabular}{column = lcr}
%\tophline
%
%\middlehline
%
%\bottomhline
%\end{tabular}
%\belowtable{} % Table Footnotes
%\end{table*}
%
%%% LANDSCAPE TABLE
%
%%t
%\begin{sidewaystable*}[t]
%\caption{TEXT}
%\begin{tabular}{column = lcr}
%\tophline
%
%\middlehline
%
%\bottomhline
%\end{tabular}
%\belowtable{} % Table Footnotes
%\end{sidewaystable*}
%
%
%%% MATHEMATICAL EXPRESSIONS
%
%%% All papers typeset by Copernicus Publications follow the math typesetting regulations
%%% given by the IUPAC Green Book (IUPAC: Quantities, Units and Symbols in Physical Chemistry,
%%% 2nd Edn., Blackwell Science, available at: http://old.iupac.org/publications/books/gbook/green_book_2ed.pdf, 1993).
%%%
%%% Physical quantities/variables are typeset in italic font (t for time, T for Temperature)
%%% Indices which are not defined are typeset in italic font (x, y, z, a, b, c)
%%% Items/objects which are defined are typeset in roman font (Car A, Car B)
%%% Descriptions/specifications which are defined by itself are typeset in roman font (abs, rel, ref, tot, net, ice)
%%% Abbreviations from 2 letters are typeset in roman font (RH, LAI)
%%% Vectors are identified in bold italic font using \vec{x}
%%% Matrices are identified in bold roman font
%%% Multiplication signs are typeset using the LaTeX commands \times (for vector products, grids, and exponential notations) or \cdot
%%% The character * should not be applied as mutliplication sign
%
%
%%% EQUATIONS
%
%%% Single-row equation
%
%\begin{equation}
%
%\end{equation}
%
%%% Multiline equation
%
%\begin{align}
%& 3 + 5 = 8\\
%& 3 + 5 = 8\\
%& 3 + 5 = 8
%\end{align}
%
%
%%% MATRICES
%
%\begin{matrix}
%x & y & z\\
%x & y & z\\
%x & y & z\\
%\end{matrix}
%
%
%%% ALGORITHM
%
%\begin{algorithm}
%\caption{...}
%\label{a1}
%\begin{algorithmic}
%...
%\end{algorithmic}
%\end{algorithm}
%
%
%%% CHEMICAL FORMULAS AND REACTIONS
%
%%% For formulas embedded in the text, please use \chem{}
%
%%% The reaction environment creates labels including the letter R, i.e. (R1), (R2), etc.
%
%\begin{reaction}
%%% \rightarrow should be used for normal (one-way) chemical reactions
%%% \rightleftharpoons should be used for equilibria
%%% \leftrightarrow should be used for resonance structures
%\end{reaction}
%
%
%%% PHYSICAL UNITS
%%%
%%% Please use \unit{} and apply the exponential notation


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

