

@article{walker_defining_2003,
  author =        {Walker, Warren E. and Harremo{\"e}s, Poul and
                   Rotmans, Jan and {van der Sluijs}, Jeroen P. and
                   {van Asselt}, Marjolein BA and Janssen, Peter and
                   {Krayer von Krauss}, Martin P.},
  journal =       {Integrated assessment},
  number =        {1},
  pages =         {5--17},
  title =         {Defining Uncertainty: A Conceptual Basis for
                   Uncertainty Management in Model-Based Decision
                   Support},
  volume =        {4},
  year =          {2003},
}

@article{das_estimation_1991,
  author =        {Das, S. K. and Lardner, R. W.},
  journal =       {Journal of Geophysical Research},
  number =        {C8},
  pages =         {15187},
  title =         {On the Estimation of Parameters of Hydraulic Models
                   by Assimilation of Periodic Tidal Data},
  volume =        {96},
  year =          {1991},
  doi =           {10.1029/91JC01318},
  issn =          {0148-0227},
  language =      {en},
}

@phdthesis{boutet_estimation_2015,
  author =        {Boutet, Martial},
  school =        {Universit{\'e} d'Aix Marseille},
  title =         {Estimation Du Frottement Sur Le Fond Pour La
                   Mod{\'e}lisation de La Mar{\'e}e Barotrope},
  year =          {2015},
}

@article{huyse_free-form_2001,
  author =        {Huyse, Luc and Bushnell, Dennis M.},
  title =         {Free-Form Airfoil Shape Optimization under
                   Uncertainty Using Maximum Expected Value and
                   Second-Order Second-Moment Strategies},
  year =          {2001},
}

@article{lelievre_consideration_2016,
  author =        {Leli{\`e}vre, Nicolas and Beaurepaire, Pierre and
                   Mattrand, C{\'e}cile and Gayton, Nicolas and
                   Otsmane, Abdelkader},
  journal =       {Structural and Multidisciplinary Optimization},
  number =        {6},
  pages =         {1423--1437},
  title =         {On the Consideration of Uncertainty in Design:
                   Optimization-Reliability-Robustness},
  volume =        {54},
  year =          {2016},
}

@article{petrone_robustness_2011,
  author =        {Petrone, Giovanni and Iaccarino, Gianluca and
                   Quagliarella, D.},
  journal =       {Evolutionary and deterministic methods for design,
                   optimization and control (EUROGEN 2011). CIRA, Capua},
  pages =         {244--252},
  title =         {Robustness Criteria in Optimization under
                   Uncertainty},
  year =          {2011},
}

@article{seshadri_density-matching_2014,
  author =        {Seshadri, Pranay and Constantine, Paul and
                   Iaccarino, Gianluca and Parks, Geoffrey},
  journal =       {arXiv:1409.7089 [math, stat]},
  month =         sep,
  title =         {A Density-Matching Approach for Optimization under
                   Uncertainty},
  year =          {2014},
  abstract =      {Modern computers enable methods for design
                   optimization that account for uncertainty in the
                   system---so-called optimization under uncertainty. We
                   propose a metric for OUU that measures the distance
                   between a designer-specified probability density
                   function of the system response the target and system
                   response's density function at a given design. We
                   study an OUU formulation that minimizes this distance
                   metric over all designs. We discretize the objective
                   function with numerical quadrature and approximate
                   the response density function with a Gaussian kernel
                   density estimate. We offer heuristics for addressing
                   issues that arise in this formulation, and we apply
                   the approach to a CFD-based airfoil shape
                   optimization problem. We qualitatively compare the
                   density-matching approach to a multi-objective robust
                   design optimization to gain insight into the method.},
}

@article{cook_horsetail_2018,
  author =        {Cook, L. W. and Jarrett, J. P.},
  journal =       {Engineering Optimization},
  month =         apr,
  number =        {4},
  pages =         {549-567},
  title =         {Horsetail Matching: A Flexible Approach to
                   Optimization under Uncertainty},
  volume =        {50},
  year =          {2018},
  abstract =      {It is important to design engineering systems to be
                   robust with respect to uncertainties in the design
                   process. Often, this is done by considering
                   statistical moments, but over-reliance on statistical
                   moments when formulating a robust optimization can
                   produce designs that are stochastically dominated by
                   other feasible designs. This article instead proposes
                   a formulation for optimization under uncertainty that
                   minimizes the difference between a design's
                   cumulative distribution function and a target. A
                   standard target is proposed that produces
                   stochastically non-dominated designs, but the
                   formulation also offers enough flexibility to recover
                   existing approaches for robust optimization. A
                   numerical implementation is developed that employs
                   kernels to give a differentiable objective function.
                   The method is applied to algebraic test problems and
                   a robust transonic airfoil design problem where it is
                   compared to multi-objective, weighted-sum and density
                   matching approaches to robust optimization; several
                   advantages over these existing methods are
                   demonstrated.},
  doi =           {10.1080/0305215X.2017.1327581},
  issn =          {0305-215X, 1029-0273},
  language =      {en},
}

@incollection{huber_robust_2011,
  author =        {Huber, Peter J.},
  booktitle =     {International {{Encyclopedia}} of {{Statistical
                   Science}}},
  pages =         {1248--1251},
  publisher =     {{Springer}},
  title =         {Robust Statistics},
  year =          {2011},
}

@article{rao_robust_2015,
  author =        {Rao, Vishwas and Sandu, Adrian and Ng, Michael and
                   {Nino-Ruiz}, Elias},
  journal =       {SciRate},
  month =         nov,
  title =         {Robust Data Assimilation Using \${{L}}\_1\$ and
                   {{Huber}} Norms},
  year =          {2015},
  abstract =      {Data assimilation is the process to fuse information
                   from priors, observations of nature, and numerical
                   models, in order to obtain best estimates of the
                   parameters or state of a physical system of interest.
                   Presence of large errors in some observational data,
                   e.g., data collected from a faulty instrument,
                   negatively affect the quality of the overall
                   assimilation results. This work develops a systematic
                   framework for robust data assimilation. The new
                   algorithms continue to produce good analyses in the
                   presence of observation outliers. The approach is
                   based on replacing the traditional
                   \$\textbackslash{}L\_2\$ norm formulation of data
                   assimilation problems with formulations based on
                   \$\textbackslash{}L\_1\$ and Huber norms. Numerical
                   experiments using the Lorenz-96 and the shallow water
                   on the sphere models illustrate how the new
                   algorithms outperform traditional data assimilation
                   approaches in the presence of data outliers.},
}

@article{berger_overview_1994,
  author =        {Berger, James O. and Moreno, El{\'i}as and
                   Pericchi, Luis Raul and Bayarri, M. Jes{\'u}s and
                   Bernardo, Jos{\'e} M. and Cano, Juan A. and
                   {De la Horra}, Juli{\'a}n and Mart{\'i}n, Jacinto and
                   {R{\'i}os-Ins{\'u}a}, David and Betr{\`o}, Bruno},
  journal =       {Test},
  number =        {1},
  pages =         {5--124},
  title =         {An Overview of Robust {{Bayesian}} Analysis},
  volume =        {3},
  year =          {1994},
}

@article{marzat_worst-case_2013,
  author =        {Marzat, Julien and Walter, Eric and
                   {Piet-Lahanier}, H{\'e}l{\`e}ne},
  journal =       {Journal of Global Optimization},
  month =         apr,
  number =        {4},
  pages =         {707-727},
  title =         {Worst-Case Global Optimization of Black-Box Functions
                   through {{Kriging}} and Relaxation},
  volume =        {55},
  year =          {2013},
  abstract =      {A new algorithm is proposed to deal with the
                   worst-case optimization of black-box functions
                   evaluated through costly computer simulations. The
                   input variables of these computer experiments are
                   assumed to be of two types. Control variables must be
                   tuned while environmental variables have an
                   undesirable effect, to which the design of the
                   control variables should be robust. The algorithm to
                   be proposed searches for a minimax solution, i.e.,
                   values of the control variables that minimize the
                   maximum of the objective function with respect to the
                   environmental variables. The problem is particularly
                   difficult when the control and environmental
                   variables live in continuous spaces. Combining a
                   relaxation procedure with Krigingbased optimization
                   makes it possible to deal with the continuity of the
                   variables and the fact that no analytical expression
                   of the objective function is available in most
                   real-case problems. Numerical experiments are
                   conducted to assess the accuracy and efficiency of
                   the algorithm, both on analytical test functions with
                   known results and on an engineering application.},
  doi =           {10.1007/s10898-012-9899-y},
  issn =          {0925-5001, 1573-2916},
  language =      {en},
}

@article{villemonteix_informational_2006,
  author =        {Villemonteix, Julien and Vazquez, Emmanuel and
                   Walter, Eric},
  journal =       {arXiv:cs/0611143},
  month =         nov,
  title =         {An Informational Approach to the Global Optimization
                   of Expensive-to-Evaluate Functions},
  year =          {2006},
  abstract =      {In many global optimization problems motivated by
                   engineering applications, the number of function
                   evaluations is severely limited by time or cost. To
                   ensure that each evaluation contributes to the
                   localization of good candidates for the role of
                   global minimizer, a sequential choice of evaluation
                   points is usually carried out. In particular, when
                   Kriging is used to interpolate past evaluations, the
                   uncertainty associated with the lack of information
                   on the function can be expressed and used to compute
                   a number of criteria accounting for the interest of
                   an additional evaluation at any given point. This
                   paper introduces minimizer entropy as a new
                   Kriging-based criterion for the sequential choice of
                   points at which the function should be evaluated.
                   Based on stepwise uncertainty reduction, it accounts
                   for the informational gain on the minimizer expected
                   from a new evaluation. The criterion is approximated
                   using conditional simulations of the Gaussian process
                   model behind Kriging, and then inserted into an
                   algorithm similar in spirit to the Efficient Global
                   Optimization (EGO) algorithm. An empirical comparison
                   is carried out between our criterion and expected
                   improvement, one of the reference criteria in the
                   literature. Experimental results indicate major
                   evaluation savings over EGO. Finally, the method,
                   which we call IAGO (for Informational Approach to
                   Global Optimization) is extended to robust
                   optimization problems, where both the factors to be
                   tuned and the function evaluations are corrupted by
                   noise.},
  language =      {en},
}

@article{hennig_entropy_2011,
  author =        {Hennig, Philipp and Schuler, Christian J.},
  month =         dec,
  title =         {Entropy {{Search}} for {{Information}}-{{Efficient
                   Global Optimization}}},
  year =          {2011},
  language =      {en},
}

@inproceedings{buhmann_robust_2013,
  address =       {{Berkeley, California, USA}},
  author =        {Buhmann, Joachim M. and Mihalak, Matus and
                   Sramek, Rastislav and Widmayer, Peter},
  booktitle =     {Proceedings of the 4th Conference on {{Innovations}}
                   in {{Theoretical Computer Science}} - {{ITCS}} '13},
  pages =         {505},
  publisher =     {{ACM Press}},
  title =         {Robust Optimization in the Presence of Uncertainty},
  year =          {2013},
  abstract =      {We study optimization in the presence of uncertainty
                   such as noise in measurements, and advocate a novel
                   approach of tackling it. The main difference to any
                   existing approach is that we do not assume any
                   knowledge about the nature of the uncertainty (such
                   as for instance a probability distribution). Instead,
                   we are given several instances of the same
                   optimization problem as input, and, assuming they are
                   typical w.r.t. the uncertainty, we make use of it in
                   order to compute a solution that is good for the
                   sample instances as well as for future (unknown)
                   typical instances.},
  doi =           {10.1145/2422436.2422491},
  isbn =          {978-1-4503-1859-4},
  language =      {en},
}

@article{kouvelis_algorithms_1992,
  author =        {Kouvelis, Panagiotis and Kurawarwala, Abbas A. and
                   Guti{\'e}rrez, Genaro J.},
  journal =       {European Journal of Operational Research},
  month =         dec,
  number =        {2},
  pages =         {287-303},
  series =        {Strategic {{Planning}} of {{Facilities}}},
  title =         {Algorithms for Robust Single and Multiple Period
                   Layout Planning for Manufacturing Systems},
  volume =        {63},
  year =          {1992},
  abstract =      {In many layout design situations, the use of
                   `optimality' with respect to a design objective, such
                   as the minimization of the material handling cost, is
                   insufficiently discriminating. Robustness of the
                   layout, in cases of demand uncertainty, is more
                   important for the manufacturing manager. A robust
                   layout is one that is close to the optimal solution
                   for a wide variety of demand scenarios even though it
                   may not be optimal under any specific demand
                   scenario. In this paper, we develop algorithms to
                   generate robust layout designs for manufacturing
                   systems. Our robustness approach to the layout
                   decision making can be applied to single and multiple
                   period problems in the presence of considerable
                   uncertainty, both in terms of products to be produced
                   as well as their production volumes. Our algorithms,
                   executed in a heuristic fashion, can be effectively
                   used for layout design of large size manufacturing
                   systems.},
  doi =           {10.1016/0377-2217(92)90032-5},
  issn =          {0377-2217},
  language =      {en},
}

@article{snyder_stochastic_2004,
  author =        {Snyder, Lawrence and Daskin, Mark},
  journal =       {Iie Transactions},
  month =         aug,
  title =         {Stochastic P-Robust Location Problems},
  volume =        {38},
  year =          {2004},
  abstract =      {Many objectives have been proposed for optimization
                   under uncertainty. The typical stochastic programming
                   ob-jective of minimizing expected cost may yield
                   solutions that are inexpensive in the long run but
                   perform poorly under certain realizations of the
                   random data. On the other hand, the typical robust
                   optimization objective of minimizing maximum cost or
                   regret tends to be overly conservative, planning
                   against a disastrous but unlikely scenario. In this
                   paper, we present facility location models that
                   combine the two objectives by minimizing the expected
                   cost while bounding the relative regret in each
                   scenario. In particular, the models seek the
                   minimum-expected-cost solution that is p-robust;
                   i.e., whose relative regret is no more than 100p\% in
                   each scenario. We present p-robust models based on
                   two classical facility location problems, the P
                   -median problem and the uncapacitated fixed-charge
                   location problem. We solve both problems using
                   variable splitting (Lagrangian decomposition), in
                   which the subproblem reduces to the multiple-choice
                   knapsack problem. Feasible solutions are found using
                   an upper-bounding heuristic. For many instances of
                   the problems, finding a feasible solution, and even
                   determining whether the instance is feasible, is
                   difficult; we discuss a mechanism for determining
                   infeasibility. We also show how the algorithm can be
                   used as a heuristic to solve minimax-regret versions
                   of the location problems.},
  doi =           {10.1080/07408170500469113},
}

@inproceedings{juditsky_stochastic_2009,
  author =        {Juditsky, Anatoli and Nemirovski, Arkadii S. and
                   Lan, Guanghui and Shapiro, Alexander},
  booktitle =     {{{ISMP}} 2009 - 20th {{International Symposium}} of
                   {{Mathematical Programming}}},
  month =         aug,
  title =         {Stochastic {{Approximation Approach}} to {{Stochastic
                   Programming}}},
  year =          {2009},
  abstract =      {A basic difficulty with solving stochastic
                   programming problems is that it requires computation
                   of expectations given by multidimensional integrals.
                   One approach, based on Monte Carlo sampling
                   techniques, is to generate a reasonably large random
                   sample and consequently to solve the constructed
                   so-called Sample Average Approximation (SAA) problem.
                   The other classical approach is based on Stochastic
                   Approximation (SA) techniques. In this talk we
                   discuss some recent advances in development of SA
                   type numerical algorithms for solving convex
                   stochastic programming problems. Numerical
                   experiments show that for some classes of problems
                   the so-called Mirror Descent SA Method can
                   significantly outperform the SAA approach.},
  language =      {en},
}

@incollection{kim_guide_2015,
  author =        {Kim, Sujin and Pasupathy, Raghu and Henderson, Shane},
  month =         jan,
  pages =         {207-243},
  title =         {A {{Guide}} to {{Sample Average Approximation}}},
  volume =        {216},
  year =          {2015},
  abstract =      {This chapter reviews the principles of sample average
                   approximation (SAA) for solving simulation
                   optimization problems. We provide an accessible
                   overview of the area and survey interesting recent
                   developments. We explain when one might want to use
                   SAA and when one might expect it to provide
                   good-quality solutions. We also review some of the
                   key theoretical properties of the solutions obtained
                   through SAA. We contrast SAA with stochastic
                   approximation (SA) methods in terms of the
                   computational effort required to obtain solutions of
                   a given quality, explaining why SA ``wins''
                   asymptotically. However, an extension of SAA known as
                   retrospective optimization can match the asymptotic
                   convergence rate of SA, at least up to a
                   multiplicative constant.},
  doi =           {10.1007/978-1-4939-1384-8-8},
}

@techreport{janusevskis_simultaneous_2010,
  author =        {Janusevskis, Janis and Le Riche, Rodolphe},
  month =         jul,
  title =         {Simultaneous Kriging-Based Sampling for Optimization
                   and Uncertainty Propagation},
  year =          {2010},
  abstract =      {Robust analysis and optimization is typically based
                   on repeated calls to a deterministic simulator that
                   aim at propagating uncertainties and finding optimal
                   design variables. Without loss of generality a double
                   set of simulation parameters can be assumed: x are
                   deterministic optimization variables, u are random
                   parameters of known probability density function and
                   f (x, u) is the objective function attached to the
                   simulator. Most robust optimization methods involve
                   two imbricated tasks, the u's uncertainty propagation
                   (e.g., Monte Carlo simulations, reliability index
                   calculation) which is recurcively performed inside
                   optimization iterations on the x's. In practice, f is
                   often calculated through a computationally expensive
                   software. This makes the computational cost one of
                   the principal obstacle to optimization in the
                   presence of uncertainties. This report proposes a new
                   efficient method for minimizing the mean objective
                   function, min E[f(x, U)]. The efficiency stems from
                   the simultaneous sampling of f for uncertainty
                   propagation and optimization, i.e., the hierarchical
                   imbrication is avoided. Y(x,u) ({$\omega$}), a
                   kriging (Gaussian process conditioned on t past
                   calculations of f) model of f (x, u) is built and the
                   mean process, Z(x) ({$\omega$}) =
                   E[Y(x,U)({$\omega$})], is analytically derived from
                   it. The sampling criterion that yields both x and u
                   is the one-step ahead minimum variance of the mean
                   process Z at the maximizer of the expected
                   improvement. The method is compared with Monte Carlo
                   and kriging-based approaches on analytical test
                   functions in two, four and six dimensions.},
}

@phdthesis{baudoui_optimisation_2012,
  author =        {Baudoui, Vincent},
  school =        {Toulouse, ISAE},
  title =         {Optimisation Robuste Multiobjectifs Par Mod{\`e}les
                   de Substitution},
  year =          {2012},
}

@article{grodzevich_normalization_2006,
  author =        {Grodzevich, Oleg and Romanko, Oleksandr},
  title =         {Normalization and Other Topics in Multi-Objective
                   Optimization},
  year =          {2006},
}

@article{marler_weighted_2010,
  author =        {Marler, R. Timothy and Arora, Jasbir S.},
  journal =       {Structural and Multidisciplinary Optimization},
  month =         jun,
  number =        {6},
  pages =         {853-862},
  title =         {The Weighted Sum Method for Multi-Objective
                   Optimization: New Insights},
  volume =        {41},
  year =          {2010},
  abstract =      {As a common concept in multi-objective optimization,
                   minimizing a weighted sum constitutes an independent
                   method as well as a component of other methods.
                   Consequently, insight into characteristics of the
                   weighted sum method has far reaching implications.
                   However, despite the many published applications for
                   this method and the literature addressing its
                   pitfalls with respect to depicting the Pareto optimal
                   set, there is little comprehensive discussion
                   concerning the conceptual significance of the weights
                   and techniques for maximizing the effectiveness of
                   the method with respect to a priori articulation of
                   preferences. Thus, in this paper, we investigate the
                   fundamental significance of the weights in terms of
                   preferences, the Pareto optimal set, and
                   objective-function values. We determine the factors
                   that dictate which solution point results from a
                   particular set of weights. Fundamental deficiencies
                   are identified in terms of a priori articulation of
                   preferences, and guidelines are provided to help
                   avoid blind use of the method.},
  doi =           {10.1007/s00158-009-0460-7},
  issn =          {1615-147X, 1615-1488},
  language =      {en},
}

@article{lehman_designing_2004,
  author =        {Lehman, Jeffrey S. and Santner, Thomas J. and
                   Notz, William I.},
  journal =       {Statistica Sinica},
  pages =         {571--590},
  title =         {Designing Computer Experiments to Determine Robust
                   Control Variables},
  year =          {2004},
}

@article{dempster_maximum_1977,
  author =        {Dempster, Arthur P. and Laird, Nan M. and
                   Rubin, Donald B.},
  journal =       {Journal of the royal statistical society. Series B
                   (methodological)},
  pages =         {1--38},
  title =         {Maximum Likelihood from Incomplete Data via the
                   {{EM}} Algorithm},
  year =          {1977},
}

@article{freedman_histogram_1981,
  author =        {Freedman, David and Diaconis, Persi},
  journal =       {Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und
                   Verwandte Gebiete},
  month =         dec,
  number =        {4},
  pages =         {453-476},
  title =         {On the Histogram as a Density Estimator:{{L2}}
                   Theory},
  volume =        {57},
  year =          {1981},
  doi =           {10.1007/BF01025868},
  issn =          {1432-2064},
  language =      {en},
}

@article{scott_optimal_1979,
  author =        {Scott, David W.},
  journal =       {Biometrika},
  month =         dec,
  number =        {3},
  pages =         {605},
  title =         {On {{Optimal}} and {{Data}}-{{Based Histograms}}},
  volume =        {66},
  year =          {1979},
  abstract =      {In thispaper the formulaforthe optimalhistogrambin
                   widthis derivedwhichasymptotically minimizesthe
                   integratedmean squared error.Monte Carlo methodsare
                   used to verify the usefulnessofthisformulaforsmall
                   samples. A data-based procedureforchoosingthe bin
                   widthparameteris proposed,whichassumes a Gaussian
                   referencestandard and requiresonly the sample size
                   and an estimateofthe standard deviation. The
                   sensitivityofthe procedureis investigatedusingseveral
                   probabilitymodelswhichviolate the Gaussian
                   assumption.},
  doi =           {10.2307/2335182},
  issn =          {00063444},
  language =      {en},
}

@techreport{rockafellar_deviation_2002,
  address =       {{Rochester, NY}},
  author =        {Rockafellar, R. Tyrrell and Uryasev, Stanislav P. and
                   Zabarankin, Michael},
  institution =   {{Social Science Research Network}},
  month =         dec,
  number =        {ID 365640},
  type =          {{{SSRN Scholarly Paper}}},
  title =         {Deviation {{Measures}} in {{Risk Analysis}} and
                   {{Optimization}}},
  year =          {2002},
  abstract =      {General deviation measures, which include standard
                   deviation as a special case but need not be symmetric
                   with respect to ups and downs, are defined and shown
                   to correspond to risk measures in the sense of
                   Artzner, Delbaen, Eber and Heath when those are
                   applied to the difference between a random variable
                   and its expectation, instead of to the random
                   variable itself. A property called
                   expectation-boundedness of the risk measure is
                   uncovered as essential for this correspondence. It is
                   shown to be satisfied by conditional value-at-risk
                   and by worst-case risk, as well as various mixtures,
                   although not by ordinary value-at-risk.},
  language =      {en},
}

@article{kennedy_bayesian_2001,
  author =        {Kennedy, Marc C. and O'Hagan, Anthony},
  journal =       {Journal of the Royal Statistical Society: Series B
                   (Statistical Methodology)},
  month =         jan,
  number =        {3},
  pages =         {425-464},
  title =         {Bayesian Calibration of Computer Models},
  volume =        {63},
  year =          {2001},
  abstract =      {We consider prediction and uncertainty analysis for
                   systems which are approximated using complex
                   mathematical models. Such models, implemented as
                   computer codes, are often generic in the sense that
                   by a suitable choice of some of the model's input
                   parameters the code can be used to predict the
                   behaviour of the system in a variety of specific
                   applications. However, in any specific application
                   the values of necessary parameters may be unknown. In
                   this case, physical observations of the system in the
                   specific context are used to learn about the unknown
                   parameters. The process of fitting the model to the
                   observed data by adjusting the parameters is known as
                   calibration. Calibration is typically effected by ad
                   hoc fitting, and after calibration the model is used,
                   with the fitted input values, to predict the future
                   behaviour of the system. We present a Bayesian
                   calibration technique which improves on this
                   traditional approach in two respects. First, the
                   predictions allow for all sources of uncertainty,
                   including the remaining uncertainty over the fitted
                   parameters. Second, they attempt to correct for any
                   inadequacy of the model which is revealed by a
                   discrepancy between the observed data and the model
                   predictions from even the best-fitting parameter
                   values. The method is illustrated by using data from
                   a nuclear radiation release at Tomsk, and from a more
                   complex simulated nuclear accident exercise.},
  doi =           {10.1111/1467-9868.00294},
  issn =          {1467-9868},
  language =      {en},
}

@article{ginsbourger_bayesian_2014,
  author =        {Ginsbourger, David and Baccou, Jean and
                   Chevalier, Cl{\'e}ment and Perales, Fr{\'e}d{\'e}ric and
                   Garland, Nicolas and Monerie, Yann},
  journal =       {SIAM/ASA Journal on Uncertainty Quantification},
  month =         jan,
  number =        {1},
  pages =         {490-510},
  title =         {Bayesian {{Adaptive Reconstruction}} of {{Profile
                   Optima}} and {{Optimizers}}},
  volume =        {2},
  year =          {2014},
  abstract =      {Given a function depending both on decision
                   parameters and nuisance variables, we consider the
                   issue of estimating and quantifying uncertainty on
                   profile optima and/or optimal points as functions of
                   the nuisance variables. The proposed methods base on
                   interpolations of the objective function constructed
                   from a finite set of evaluations. Here the functions
                   of interest are reconstructed relying on a kriging
                   model, but also using Gaussian field conditional
                   simulations, that allow a quantification of
                   uncertainties in the Bayesian framework. Besides, we
                   elaborate a variant of the Expected Improvement
                   criterion, that proves efficient for adaptively
                   learning the set of profile optima and optimizers.
                   The results are illustrated on a toy example and
                   through a physics case study on the optimal packing
                   of polydisperse frictionless spheres.},
  doi =           {10.1137/130949555},
  issn =          {2166-2525},
  language =      {en},
}

@inproceedings{bossek_learning_2015,
  address =       {{New York, NY, USA}},
  author =        {Bossek, Jakob and Bischl, Bernd and Wagner, Tobias and
                   Rudolph, G{\"u}nter},
  booktitle =     {Proceedings of the 2015 {{Annual Conference}} on
                   {{Genetic}} and {{Evolutionary Computation}}},
  pages =         {1319--1326},
  publisher =     {{ACM}},
  series =        {{{GECCO}} '15},
  title =         {Learning {{Feature}}-{{Parameter Mappings}} for
                   {{Parameter Tuning}} via the {{Profile Expected
                   Improvement}}},
  year =          {2015},
  abstract =      {The majority of algorithms can be controlled or
                   adjusted by parameters. Their values can
                   substantially affect the algorithms' performance.
                   Since the manual exploration of the parameter space
                   is tedious -- even for few parameters -- several
                   automatic procedures for parameter tuning have been
                   proposed. Recent approaches also take into account
                   some characteristic properties of the problem
                   instances, frequently termed instance features. Our
                   contribution is the proposal of a novel concept for
                   feature-based algorithm parameter tuning, which
                   applies an approximating surrogate model for learning
                   the continuous feature-parameter mapping. To
                   accomplish this, we learn a joint model of the
                   algorithm performance based on both the algorithm
                   parameters and the instance features. The required
                   data is gathered using a recently proposed
                   acquisition function for model refinement in
                   surrogate-based optimization: the profile expected
                   improvement. This function provides an avenue for
                   maximizing the information required for the
                   feature-parameter mapping, i.e., the mapping from
                   instance features to the corresponding optimal
                   algorithm parameters. The approach is validated by
                   applying the tuner to exemplary evolutionary
                   algorithms and problems, for which theoretically
                   grounded or heuristically determined
                   feature-parameter mappings are available.},
  doi =           {10.1145/2739480.2754673},
  isbn =          {978-1-4503-3472-3},
}


@book{silverman2018density,
  title={Density estimation for statistics and data analysis},
  author={Silverman, Bernard W},
  year={2018},
  publisher={Routledge}
}
