
@article{kullback_information_1951,
  title = {On {{Information}} and {{Sufficiency}}},
  volume = {22},
  issn = {0003-4851, 2168-8990},
  abstract = {Project Euclid - mathematics and statistics online},
  language = {EN},
  number = {1},
  journal = {The Annals of Mathematical Statistics},
  doi = {10.1214/aoms/1177729694},
  author = {Kullback, S. and Leibler, R. A.},
  month = mar,
  year = {1951},
  pages = {79-86},
  file = {/home/victor/Zotero/storage/JPHCNDHI/1177729694.html}
}

@book{rachdi_apprentissage_2011,
  title = {Apprentissage Statistique et Computer Experiments : Approche Quantitative Du Risque et Des Incertitudes En Mod{\'e}lisation},
  shorttitle = {Apprentissage Statistique et Computer Experiments},
  abstract = {Cette th{\`e}se s'inscrit dans le domaine de l'apprentissage statistique et dans celui des exp{\'e}riences simul{\'e}es (computer experiments). Son objet est de proposer un cadre g{\'e}n{\'e}ral permettant d'estimer les param{\`e}tres d'un code de simulation num{\'e}rique de fa{\c c}on {\`a} reproduire au mieux certaines caract{\'e}ristiques d'int{\'e}r{\^e}t extraites de donn{\'e}es observ{\'e}es. Ce travail recouvre le cadre classique de l'estimation param{\'e}trique dans un mod{\`e}le de r{\'e}gression et {\'e}galement la calibration de la densit{\'e} de probabilit{\'e} des variables d'entr{\'e}e d'un code num{\'e}rique afin de reproduire une loi de probabilit{\'e} donn{\'e}e en sortie. Une partie importante de ce travail consiste dans l'estimation param{\'e}trique d'un code num{\'e}rique {\`a} partir d'observations. Nous proposons une classe de m{\'e}thode originale n{\'e}cessitant une simulation intensive du code num{\'e}rique, que l'on remplacera par un m{\'e}ta-mod{\`e}le s'il est trop co{\^u}teux. Nous validons th{\'e}oriquement les algorithmes propos{\'e}s du point de vue non-asymptotique, en prouvant des bornes sur l'exc{\`e}s de risque. Ces r{\'e}sultats reposent entres autres sur des in{\'e}galit{\'e}s de concentration. Un second probl{\`e}me que nous abordons est celui de l'{\'e}tude d'une dualit{\'e} entre proc{\'e}dure d'estimation et nature de la pr{\'e}diction recherch{\'e}e. Il s'agit ici de mieux comprendre l'effet d'une proc{\'e}dure d'estimation des param{\`e}tres d'un code num{\'e}rique sur une caract{\'e}ristique d'int{\'e}r{\^e}t donn{\'e}e. Enfin, en pratique la d{\'e}termination des param{\`e}tres optimaux au sens du crit{\`e}re donn{\'e} par le risque empirique n{\'e}cessite la recherche du minimum d'une fonction g{\'e}n{\'e}ralement non convexe et poss{\'e}dant plusieurs minima locaux. Nous proposons un algorithme stochastique consistant {\`a} combiner une r{\'e}gularisation du crit{\`e}re par convolution avec un noyau gaussien, de variance d{\'e}croissante au fil des it{\'e}rations, avec une m{\'e}thode d'approximation stochastique du type Kiefer-Wolfowitz.},
  publisher = {{Toulouse 3}},
  author = {Rachdi, Nabil},
  month = jan,
  year = {2011},
  file = {/home/victor/Zotero/storage/2FJ9UWPY/2011TOU30283.pdf;/home/victor/Zotero/storage/H6T3HW7X/2011TOU30283.html}
}

@article{shah_student-t_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1402.4306},
  primaryClass = {cs, stat},
  title = {Student-t {{Processes}} as {{Alternatives}} to {{Gaussian Processes}}},
  abstract = {We investigate the Student-t process as an alternative to the Gaussian process as a nonparametric prior over functions. We derive closed form expressions for the marginal likelihood and predictive distribution of a Student-t process, by integrating away an inverse Wishart process prior over the covariance kernel of a Gaussian process model. We show surprising equivalences between different hierarchical Gaussian process models leading to Student-t processes, and derive a new sampling scheme for the inverse Wishart process, which helps elucidate these equivalences. Overall, we show that a Student-t process can retain the attractive properties of a Gaussian process -- a nonparametric representation, analytic marginal and predictive distributions, and easy model selection through covariance kernels -- but has enhanced flexibility, and predictive covariances that, unlike a Gaussian process, explicitly depend on the values of training observations. We verify empirically that a Student-t process is especially useful in situations where there are changes in covariance structure, or in applications like Bayesian optimization, where accurate predictive covariances are critical for good performance. These advantages come at no additional computational cost over Gaussian processes.},
  journal = {arXiv:1402.4306 [cs, stat]},
  author = {Shah, Amar and Wilson, Andrew Gordon and Ghahramani, Zoubin},
  month = feb,
  year = {2014},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/victor/Zotero/storage/VPPMUUCH/Shah et al. - 2014 - Student-t Processes as Alternatives to Gaussian Pr.pdf;/home/victor/Zotero/storage/ZIFUTJNY/1402.html}
}

@techreport{paquet_bayesian_2008,
  title = {Bayesian Inference for Latent Variable Models},
  institution = {{University of Cambridge, Computer Laboratory}},
  author = {Paquet, Ulrich},
  year = {2008},
  file = {/home/victor/Zotero/storage/9S33KVNC/UCAM-CL-TR-724.pdf}
}

@article{orbanz_nonparametric_2008,
  title = {Nonparametric {{Bayesian Image Segmentation}}},
  volume = {77},
  issn = {0920-5691, 1573-1405},
  abstract = {Image segmentation algorithms partition the set of pixels of an image into a specific number of different, spatially homogeneous groups. We propose a nonparametric Bayesian model for histogram clustering which automatically determines the number of segments when spatial smoothness constraints on the class assignments are enforced by a Markov Random Field. A Dirichlet process prior controls the level of resolution which corresponds to the number of clusters in data with a unique cluster structure. The resulting posterior is efficiently sampled by a variant of a conjugate-case sampling algorithm for Dirichlet process mixture models. Experimental results are provided for real-world gray value images, synthetic aperture radar images and magnetic resonance imaging data.},
  language = {en},
  number = {1-3},
  journal = {International Journal of Computer Vision},
  doi = {10.1007/s11263-007-0061-0},
  author = {Orbanz, Peter and Buhmann, Joachim M.},
  month = may,
  year = {2008},
  pages = {25-45},
  file = {/home/victor/Zotero/storage/QAEDKYMY/porbanz_OrbanzBuhmann_2008_1.pdf;/home/victor/Zotero/storage/D96J2TFF/10.html}
}

@article{trossman_impact_2013,
  title = {Impact of Parameterized Lee Wave Drag on the Energy Budget of an Eddying Global Ocean Model},
  volume = {72},
  journal = {Ocean Modelling},
  author = {Trossman, David S. and Arbic, Brian K. and Garner, Stephen T. and Goff, John A. and Jayne, Steven R. and Metzger, E. Joseph and Wallcraft, Alan J.},
  year = {2013},
  pages = {119--142},
  file = {/home/victor/Zotero/storage/5DLGJ7RB/Trossman et al. - 2013 - Impact of parameterized lee wave drag on the energ.pdf;/home/victor/Zotero/storage/Q2GEN43H/S1463500313001595.html}
}

@article{kennedy_bayesian_2001,
  title = {Bayesian Calibration of Computer Models},
  volume = {63},
  issn = {1467-9868},
  abstract = {We consider prediction and uncertainty analysis for systems which are approximated using complex mathematical models. Such models, implemented as computer codes, are often generic in the sense that by a suitable choice of some of the model's input parameters the code can be used to predict the behaviour of the system in a variety of specific applications. However, in any specific application the values of necessary parameters may be unknown. In this case, physical observations of the system in the specific context are used to learn about the unknown parameters. The process of fitting the model to the observed data by adjusting the parameters is known as calibration. Calibration is typically effected by ad hoc fitting, and after calibration the model is used, with the fitted input values, to predict the future behaviour of the system. We present a Bayesian calibration technique which improves on this traditional approach in two respects. First, the predictions allow for all sources of uncertainty, including the remaining uncertainty over the fitted parameters. Second, they attempt to correct for any inadequacy of the model which is revealed by a discrepancy between the observed data and the model predictions from even the best-fitting parameter values. The method is illustrated by using data from a nuclear radiation release at Tomsk, and from a more complex simulated nuclear accident exercise.},
  language = {en},
  number = {3},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  doi = {10.1111/1467-9868.00294},
  author = {Kennedy, Marc C. and O'Hagan, Anthony},
  month = jan,
  year = {2001},
  keywords = {Calibration,Computer experiments,Deterministic models,Gaussian process,Interpolation,Model inadequacy,Sensitivity analysis,Uncertainty analysis},
  pages = {425-464},
  file = {/home/victor/Zotero/storage/ZYFTNCPQ/Kennedy et O'Hagan - 2001 - Bayesian calibration of computer models.pdf;/home/victor/Zotero/storage/NXU9WKLY/abstract.html}
}

@article{hourdin_art_2017,
  title = {The Art and Science of Climate Model Tuning},
  volume = {98},
  number = {3},
  journal = {Bulletin of the American Meteorological Society},
  author = {Hourdin, Frederic and Mauritsen, Thorsten and Gettelman, Andrew and Golaz, Jean-Christophe and Balaji, Venkatramani and Duan, Qingyun and Folini, Doris and Ji, Duoying and Klocke, Daniel and Qian, Yun},
  year = {2017},
  pages = {589--602},
  file = {/home/victor/Zotero/storage/WM7J3PJU/Tuning2016.pdf}
}

@book{rossant_learning_2013,
  address = {{Birmingham}},
  series = {Community Experience Distilled},
  title = {Learning {{IPython}} for Interactive Computing and Data Visualization: Learn {{IPython}} for Interactive {{Python}} Programming, High-Performance Numerical Computing, and Data Visualization},
  isbn = {978-1-78216-993-2},
  shorttitle = {Learning {{IPython}} for Interactive Computing and Data Visualization},
  language = {eng},
  publisher = {{Packt Publ}},
  author = {Rossant, Cyrille},
  year = {2013},
  file = {/home/victor/Zotero/storage/IAMR59A7/Learning IPython for Interactive Computing and Data Visualization [Rossant 2013-04-25].pdf},
  note = {OCLC: 854711306}
}

@article{murphy_conjugate_2007,
  title = {Conjugate {{Bayesian}} Analysis of the {{Gaussian}} Distribution},
  volume = {1},
  number = {2{$\sigma$}2},
  journal = {def},
  author = {Murphy, Kevin P.},
  year = {2007},
  pages = {16},
  file = {/home/victor/Zotero/storage/JXBXPSL6/bayesGauss.pdf}
}

@article{penny_bayesian_2014,
  title = {Bayesian {{Inference}} for the {{Multivariate Normal}}},
  author = {Penny, Will},
  year = {2014},
  file = {/home/victor/Zotero/storage/F5IYKDPX/bmn.pdf}
}

@book{bishop_pattern_2006,
  title = {Pattern Recognition and Machine Learning},
  publisher = {{springer}},
  author = {Bishop, Christopher M.},
  year = {2006},
  file = {/home/victor/Zotero/storage/DDVRV3WH/Bishop - 2006 - Pattern recognition and machine learning.pdf;/home/victor/Zotero/storage/XW2ADTWV/Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@article{fletcher_data_2006,
  title = {A Data Assimilation Method for Log-Normally Distributed Observational Errors},
  volume = {132},
  issn = {1477-870X},
  abstract = {In this paper we change the standard assumption made in the Bayesian framework of variational data assimilation to allow for observational errors that are log-normally distributed. We address the question of which statistic best describes the distribution for the univariate and multivariate cases to justify our choice of the mode. From this choice we derive the associated cost function, Jacobian and Hessian with a normal background. We also find the solution to the Jacobian equal to zero in both model and observational space. Given the Hessian that we derive, we define a preconditioner to aid in the minimization of the cost function. We extend this to define a general form for the preconditioner, given a certain type of cost function. Copyright \textcopyright{} 2006 Royal Meteorological Society},
  language = {en},
  number = {621},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  doi = {10.1256/qj.05.222},
  author = {Fletcher, S. J. and Zupanski, M.},
  month = oct,
  year = {2006},
  keywords = {Hessian,Jacobian,Preconditioner},
  pages = {2505-2519},
  file = {/home/victor/Zotero/storage/FD53NXLW/Fletcher et Zupanski - 2006 - A data assimilation method for log-normally distri.pdf;/home/victor/Zotero/storage/ZUS2Z2T2/abstract.html}
}

@article{polavarapu_data_2005,
  title = {Data Assimilation with the {{Canadian Middle Atmosphere Model}}},
  volume = {43},
  abstract = {A data assimilation scheme has been coupled to the Canadian Middle Atmosphere Model, providing, for the first time, the capability of assimilating data from the ground to the top of the mesosphere (about 95 km). This model is a full general circulation model with on-line fully interactive chemistry involving 127 gas-phase and heterogeneous reactions. Thus, feedback between dynamics, chemistry and radiation occurs in every model time step. In this work, validation of the system for tropospheric and lower stratospheric analyses is undertaken with the standard observation set used in operational weather forecasting. Results are found to agree reasonably well with radiosonde observations and with Met Office (UK) analyses. Although ozone is not assimilated, ozone fields match total column observations well in terms of synoptic patterns. However, due to model biases, total column values are too large at mid-latitudes and too small in the tropics. Since the assimilation scheme was designed for tropospheric weather prediction, its application to a middle atmosphere model can help to identify the challenges of assimilating data from this region of the atmosphere.},
  journal = {Atmosphere-ocean - ATMOS OCEAN},
  doi = {10.3137/ao.430105},
  author = {Polavarapu, Saroja and Ren, Shuzhan and Rochon, Y and Sankey, David and Ek, Nils and Koshyk, John and Tarasick, David},
  month = mar,
  year = {2005},
  pages = {77-100},
  file = {/home/victor/Zotero/storage/2XL957FF/Polavarapu et al. - 2005 - Data assimilation with the Canadian Middle Atmosph.pdf}
}

@inproceedings{lin_bayesian_2006,
  title = {Bayesian {{L1}}-{{Norm Sparse Learning}}},
  volume = {5},
  abstract = {We propose a Bayesian framework for learning the optimal regularization parameter in the L1-norm penalized least-mean-square (LMS) problem, also known as LASSO (R. Tibshirani, 1996) or basis pursuit (S.S. Chen et al., 1998). The setting of the regularization parameter is critical for deriving a correct solution. In most existing methods, the scalar regularization parameter is often determined in a heuristic manner; in contrast, our approach infers the optimal regularization setting under a Bayesian framework. Furthermore, Bayesian inference enables an independent regularization scheme where each coefficient (or weight) is associated with an independent regularization parameter. Simulations illustrate the improvement using our method in discovering sparse structure from noisy data},
  doi = {10.1109/ICASSP.2006.1661348},
  author = {Lin, Yuanqing and Lee, Daniel},
  month = jun,
  year = {2006},
  pages = {V-V},
  file = {/home/victor/Zotero/storage/UVXTNNG2/Lin et Lee - 2006 - Bayesian L1-Norm Sparse Learning.pdf}
}

@article{fletcher_hybrid_2006,
  title = {A Hybrid Multivariate {{Normal}} and Lognormal Distribution for Data Assimilation},
  volume = {7},
  issn = {1530-261X},
  abstract = {In this article, we define and prove a distribution, which is a combination of a multivariate Normal and lognormal distribution. From this distribution, we apply a Bayesian probability framework to derive a non-linear cost function similar to the one that is in current variational data assimilation (DA) applications. Copyright \textcopyright{} 2006 Royal Meteorological Society},
  language = {en},
  number = {2},
  journal = {Atmospheric Science Letters},
  doi = {10.1002/asl.128},
  author = {Fletcher, Steven J. and Zupanski, Milija},
  month = apr,
  year = {2006},
  keywords = {data assimilation,lognormal,non-Normal,probability},
  pages = {43-46},
  file = {/home/victor/Zotero/storage/3SFBYEXR/Fletcher et Zupanski - 2006 - A hybrid multivariate Normal and lognormal distrib.pdf;/home/victor/Zotero/storage/5AFA4GSM/abstract\;jsessionid=7A1211F56DF3620C35EAF009D18E8855.html}
}

@article{cohn_introduction_1997,
  title = {An {{Introduction}} to {{Estimation Theory}} ({{gtSpecial IssueltData Assimilation}} in {{Meteology}} and {{Oceanography}}: {{Theory}} and {{Practice}})},
  volume = {75},
  shorttitle = {An {{Introduction}} to {{Estimation Theory}} ({{gtSpecial IssueltData Assimilation}} in {{Meteology}} and {{Oceanography}}},
  number = {1B},
  journal = {Journal of the Meteorological Society of Japan. Ser. II},
  author = {Cohn, Stephen E.},
  year = {1997},
  pages = {257--288},
  file = {/home/victor/Zotero/storage/488E3CR6/Cohn - 1997 - An Introduction to Estimation Theory (gtSpecial Is.pdf;/home/victor/Zotero/storage/3ACGSNZS/ja.html}
}

@inproceedings{blanchard_polynomial_,
  title = {A {{Polynomial Chaos}} Based {{Bayesian Approach}} for {{Estimating Uncertain Parameters}} of {{Mechanical Systems}}},
  author = {Blanchard, Emmanuel and Sandu, Corina and Sandu, Adrian},
  file = {/home/victor/Zotero/storage/WPFGV2WC/Blanchard_DETC2007_34600.pdf}
}

@misc{mackay_bayesian_,
  title = {Bayesian {{Interpolation}}},
  howpublished = {https://authors.library.caltech.edu/13792/1/MACnc92a.pdf},
  author = {MacKay, David J.C.},
  file = {/home/victor/Zotero/storage/8VR5W2PJ/MACnc92a.pdf}
}

@misc{tipping_bayesian_,
  title = {Bayesian {{Inference}}: {{An Introduction}} to {{Principles}} and {{Practice}} in {{Machine Learning}}},
  howpublished = {http://www.miketipping.com/papers/met-mlbayes.pdf},
  author = {Tipping, Michael E.},
  file = {/home/victor/Zotero/storage/FEGY8BPK/met-mlbayes.pdf}
}

@article{fletcher_mixed_2010,
  title = {Mixed {{Gaussian}}-Lognormal Four-Dimensional Data Assimilation},
  volume = {62},
  issn = {02806495, 16000870},
  language = {en},
  number = {3},
  journal = {Tellus A},
  doi = {10.1111/j.1600-0870.2010.00439.x},
  author = {Fletcher, S. J.},
  month = may,
  year = {2010},
  pages = {266-287},
  file = {/home/victor/Zotero/storage/6J6U69Y3/~.pdf}
}

@book{park_robust_2006,
  title = {Robust {{Design}}: {{An Overview}}},
  volume = {44},
  shorttitle = {Robust {{Design}}},
  abstract = {Robust design has been developed with the expectation that an insensitive design can be obtained. That is, a product designed by robust design should be insensitive to external noises or tolerances. An insensitive design has more probability to obtain a target value, although there are uncertain noises. Theories of robust design have been developed by adopting the theories of other fields. Based on the theories, robust design can be classified into three methods: 1) the Taguchi method, 2) robust optimization, and 3) robust design with the axiomatic approach. Each method is reviewed and investigated. The methods are examined from a theoretical viewpoint and are discussed from an application viewpoint. The advantages and drawbacks of each method are discussed, and future directions for development are proposed. Copyright \textcopyright{} 2005 by the American Institute of Aeronautics and Astronautics, Inc. All rights reserved.},
  author = {Park, Gyung-Jin and Hwang, Kwang-Hyeon and Lee, Tae and Lee, Kwon-Hee},
  month = jan,
  year = {2006},
  keywords = {overview,Robust optimization},
  doi = {10.2514/1.13639}
}

@inproceedings{sohns_efficient_2006,
  title = {Efficient Parameterization of Large-Scale Dynamic Models through the Use of Activity Analysis},
  booktitle = {Proceedings of the {{ASME IMECE}}},
  author = {Sohns, B. and Allison, James and Fathy, Hosam K. and Stein, Jeffrey L.},
  year = {2006},
  pages = {5--10},
  file = {/home/victor/Zotero/storage/MX4BU3SH/Sohns et al. - 2006 - Efficient parameterization of large-scale dynamic .pdf;/home/victor/Zotero/storage/GYA5TDAS/proceeding.html}
}

@article{frangos_surrogate_2010,
  title = {Surrogate and Reduced-Order Modeling: A Comparison of Approaches for Large-Scale Statistical Inverse Problems [{{Chapter}} 7]},
  shorttitle = {Surrogate and Reduced-Order Modeling},
  author = {Frangos, M. and Marzouk, Y. and Willcox, K. and {van Bloemen Waanders}, B.},
  year = {2010},
  file = {/home/victor/Zotero/storage/RQW32K5F/Frangos et al. - 2010 - Surrogate and reduced-order modeling a comparison.pdf;/home/victor/Zotero/storage/U8J84R4V/105500.html}
}

@article{higdon_posterior_2011,
  title = {Posterior Exploration for Computationally Intensive Forward Models},
  journal = {Handbook of Markov Chain Monte Carlo},
  author = {Higdon, David and Reese, C. Shane and Moulton, J. David and Vrugt, Jasper A. and Fox, Colin},
  year = {2011},
  keywords = {MCMC},
  pages = {401--418},
  file = {/home/victor/Zotero/storage/SBUQHLQH/Higdon et al. - 2011 - Posterior exploration for computationally intensiv.pdf;/home/victor/Zotero/storage/9GARKPTV/books.html}
}

@article{higdon_computer_2008,
  title = {Computer Model Calibration Using High-Dimensional Output},
  volume = {103},
  number = {482},
  journal = {Journal of the American Statistical Association},
  author = {Higdon, Dave and Gattiker, James and Williams, Brian and Rightley, Maria},
  year = {2008},
  pages = {570--583},
  file = {/home/victor/Zotero/storage/J724KP8I/Higdon et al. - 2008 - Computer model calibration using high-dimensional .pdf;/home/victor/Zotero/storage/IWJUKBKK/016214507000000888.html}
}

@article{higdon_combining_2004,
  title = {Combining Field Data and Computer Simulations for Calibration and Prediction},
  volume = {26},
  number = {2},
  journal = {SIAM Journal on Scientific Computing},
  author = {Higdon, Dave and Kennedy, Marc and Cavendish, James C. and Cafeo, John A. and Ryne, Robert D.},
  year = {2004},
  pages = {448--466},
  file = {/home/victor/Zotero/storage/FTF7QNLB/Higdon et al. - 2004 - Combining field data and computer simulations for .pdf;/home/victor/Zotero/storage/SF5KN75N/S1064827503426693.html}
}

@article{diez_design-space_2015,
  title = {Design-Space Dimensionality Reduction in Shape Optimization by {{Karhunen}}\textendash{{Lo{\`e}ve}} Expansion},
  volume = {283},
  issn = {00457825},
  language = {en},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  doi = {10.1016/j.cma.2014.10.042},
  author = {Diez, Matteo and Campana, Emilio F. and Stern, Frederick},
  month = jan,
  year = {2015},
  keywords = {Dimensionality reduction,Karhunen–Loève expansion},
  pages = {1525-1544},
  file = {/home/victor/Zotero/storage/8B582P7U/2015-CMAME-Diez_etal.pdf}
}

@article{dashti_bayesian_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1302.6989},
  primaryClass = {math},
  title = {The {{Bayesian Approach To Inverse Problems}}},
  abstract = {These lecture notes highlight the mathematical and computational structure relating to the formulation of, and development of algorithms for, the Bayesian approach to inverse problems in differential equations. This approach is fundamental in the quantification of uncertainty within applications involving the blending of mathematical models with data.},
  journal = {arXiv:1302.6989 [math]},
  author = {Dashti, Masoumeh and Stuart, Andrew M.},
  month = feb,
  year = {2013},
  keywords = {Mathematics - Probability},
  file = {/home/victor/Zotero/storage/WJAQBC8F/Dashti et Stuart - 2013 - The Bayesian Approach To Inverse Problems.pdf;/home/victor/Zotero/storage/TYUV9KQE/1302.html}
}

@article{marzouk_dimensionality_2009,
  title = {Dimensionality Reduction and Polynomial Chaos Acceleration of {{Bayesian}} Inference in Inverse Problems},
  volume = {228},
  issn = {0021-9991},
  abstract = {We consider a Bayesian approach to nonlinear inverse problems in which the unknown quantity is a spatial or temporal field, endowed with a hierarchical Gaussian process prior. Computational challenges in this construction arise from the need for repeated evaluations of the forward model (e.g., in the context of Markov chain Monte Carlo) and are compounded by high dimensionality of the posterior. We address these challenges by introducing truncated Karhunen\textendash{}Lo{\`e}ve expansions, based on the prior distribution, to efficiently parameterize the unknown field and to specify a stochastic forward problem whose solution captures that of the deterministic forward model over the support of the prior. We seek a solution of this problem using Galerkin projection on a polynomial chaos basis, and use the solution to construct a reduced-dimensionality surrogate posterior density that is inexpensive to evaluate. We demonstrate the formulation on a transient diffusion equation with prescribed source terms, inferring the spatially-varying diffusivity of the medium from limited and noisy data.},
  number = {6},
  journal = {Journal of Computational Physics},
  doi = {10.1016/j.jcp.2008.11.024},
  author = {Marzouk, Youssef M. and Najm, Habib N.},
  month = apr,
  year = {2009},
  keywords = {MCMC,Dimensionality reduction,Karhunen–Loève expansion,Bayesian inference,Galerkin projection,Gaussian processes,Inverse problems,Polynomial chaos,RKHS},
  pages = {1862-1902},
  file = {C:\\Users\\Victor\\Downloads\\59814.pdf;/home/victor/Zotero/storage/XJ4K288U/S0021999108006062.html}
}

@inproceedings{bond_parameterized_2005,
  title = {Parameterized Model Order Reduction of Nonlinear Dynamical Systems},
  booktitle = {Proceedings of the 2005 {{IEEE}}/{{ACM International}} Conference on {{Computer}}-Aided Design},
  publisher = {{IEEE Computer Society}},
  author = {Bond, Brad and Daniel, Luca},
  year = {2005},
  keywords = {Moment matching,MOR methods},
  pages = {487--494},
  file = {/home/victor/Zotero/storage/SFL2S7Y6/pub219_000.pdf}
}

@article{du_centroidal_1999,
  title = {Centroidal {{Voronoi}} Tessellations: {{Applications}} and Algorithms},
  volume = {41},
  shorttitle = {Centroidal {{Voronoi}} Tessellations},
  number = {4},
  journal = {SIAM review},
  author = {Du, Qiang and Faber, Vance and Gunzburger, Max},
  year = {1999},
  keywords = {CVT,Voronoi},
  pages = {637--676},
  file = {/home/victor/Zotero/storage/P76KIDGT/dfg99sirv.pdf}
}

@unpublished{marrel_global_2010,
  title = {Global Sensitivity Analysis for Models with Spatially Dependent Outputs},
  abstract = {The global sensitivity analysis of a complex numerical model often requires the estimation of variance-based importance measures, called Sobol' indices. Metamodel-based techniques have been developed in order to replace the cpu time expensive computer code with an inexpensive mathematical function, predicting the computer code output. The common metamodel-based sensitivity analysis methods are appropriate with computer codes having scalar model output. However, in the environmental domain, as in many areas of application, numerical models often give as output a spatial map, which is sometimes a spatio-temporal evolution, of some interest variables. In this paper, we introduce a novel way to obtain a spatial map of Sobol' indices with a minimal number of numerical model computations. It is based on the functional decomposition of the spatial output onto a wavelet basis and the metamodeling of the wavelet coefficients by Gaussian process. An analytical example allows us to clarify the various steps of our methodology. This technique is then applied to a real case of hydrogeological modeling: for each model input variable, a spatial map of Sobol' indices is thus obtained.},
  author = {Marrel, Amandine and Iooss, Bertrand and Jullien, Michel and Laurent, Beatrice and Volkova, Elena},
  month = feb,
  year = {2010},
  keywords = {Gaussian process,AS,Kriging,Monte-Carlo estimation,Sobol',wavelet},
  file = {/home/victor/Zotero/storage/Q6NIH5TD/Marrel et al. - 2010 - Global sensitivity analysis for models with spatia.pdf}
}

@article{abramovich_wavelet_1998,
  title = {Wavelet Decomposition Approaches to Statistical Inverse Problems},
  volume = {85},
  number = {1},
  journal = {Biometrika},
  author = {u Abramovich, F. and Silverman, B. W.},
  year = {1998},
  keywords = {wavelet,stochastic inverse problem},
  pages = {115--129},
  file = {/home/victor/Zotero/storage/NEFQHAGV/biometrika1998.pdf}
}

@incollection{mayerhofer_reduced_2017,
  title = {A {{Reduced Basis Method}} for {{Parameter Functions Using Wavelet Approximations}}},
  booktitle = {Model {{Reduction}} of {{Parametrized Systems}}},
  publisher = {{Springer}},
  author = {Mayerhofer, Antonia and Urban, Karsten},
  year = {2017},
  keywords = {wavelet,reduced basis},
  pages = {77--90},
  file = {/home/victor/Zotero/storage/BTAFLU6L/MU2-final.pdf}
}

@article{chen_reduced_,
  title = {Reduced {{Collocation Methods}}: {{Reduced Basis Methods}} in the {{Collocation Framework}}},
  volume = {55},
  issn = {0885-7474},
  shorttitle = {Reduced {{Collocation Methods}}},
  abstract = {Abstract: In this paper, we present \{\textbackslash{} em the first\} reduced basis method well-suited for the collocation framework. Two fundamentally different algorithms are presented: the so-called Least Squares Reduced Collocation Method (LSRCM) and Empirical},
  number = {3},
  journal = {Journal of Scientific Computing},
  author = {Chen, Yanlai and Gottlieb, Sigal},
  pages = {718-737},
  file = {/home/victor/Zotero/storage/BXLVDHK5/Reduced_Collocation_Methods_Reduced_Basis_Methods_in_the_Collocation_Framework.html}
}

@phdthesis{boutet_estimation_2015,
  title = {Estimation Du Frottement Sur Le Fond Pour La Mod{\'e}lisation de La Mar{\'e}e Barotrope},
  school = {Universit{\'e} d'Aix Marseille},
  author = {Boutet, Martial},
  year = {2015},
  keywords = {Marée,Thèse},
  file = {/home/victor/Zotero/storage/CSL2KG7Y/these_martial_boutet.pdf}
}

@book{saad_iterative_2003,
  title = {Iterative Methods for Sparse Linear Systems},
  publisher = {{SIAM}},
  author = {Saad, Yousef},
  year = {2003},
  keywords = {Iterative methods,Krylov},
  file = {/home/victor/Zotero/storage/4V35HM3M/IterMethBook_2ndEd.pdf}
}

@article{lieberman_parameter_2010,
  title = {Parameter and {{State Model Reduction}} for {{Large}}-{{Scale Statistical Inverse Problems}}},
  volume = {32},
  issn = {1064-8275, 1095-7197},
  language = {en},
  number = {5},
  journal = {SIAM Journal on Scientific Computing},
  doi = {10.1137/090775622},
  author = {Lieberman, Chad and Willcox, Karen and Ghattas, Omar},
  month = jan,
  year = {2010},
  keywords = {MCMC,MOR methods,stochastic inverse problem,reduction parameter space},
  pages = {2523-2542},
  file = {C:\\Users\\Victor\\Downloads\\60569.pdf}
}

@article{bui-thanh_model_2008,
  title = {Model Reduction for Large-Scale Systems with High-Dimensional Parametric Input Space},
  volume = {30},
  number = {6},
  journal = {SIAM Journal on Scientific Computing},
  author = {{Bui-Thanh}, Tan and Willcox, Karen and Ghattas, Omar},
  year = {2008},
  keywords = {MOR methods,reduction parameter space},
  pages = {3270--3288},
  file = {/home/victor/Zotero/storage/U7L8XC6G/bwg08.pdf}
}

@techreport{janusevskis_simultaneous_2010,
  title = {Simultaneous Kriging-Based Sampling for Optimization and Uncertainty Propagation},
  abstract = {Robust analysis and optimization is typically based on repeated calls to a deterministic simulator that aim at propagating uncertainties and finding optimal design variables. Without loss of generality a double set of simulation parameters can be assumed: x are deterministic optimization variables, u are random parameters of known probability density function and f (x, u) is the objective function attached to the simulator. Most robust optimization methods involve two imbricated tasks, the u's uncertainty propagation (e.g., Monte Carlo simulations, reliability index calculation) which is recurcively performed inside optimization iterations on the x's. In practice, f is often calculated through a computationally expensive software. This makes the computational cost one of the principal obstacle to optimization in the presence of uncertainties. This report proposes a new efficient method for minimizing the mean objective function, min E[f(x, U)]. The efficiency stems from the simultaneous sampling of f for uncertainty propagation and optimization, i.e., the hierarchical imbrication is avoided. Y(x,u) ({$\omega$}), a kriging (Gaussian process conditioned on t past calculations of f) model of f (x, u) is built and the mean process, Z(x) ({$\omega$}) = E[Y(x,U)({$\omega$})], is analytically derived from it. The sampling criterion that yields both x and u is the one-step ahead minimum variance of the mean process Z at the maximizer of the expected improvement. The method is compared with Monte Carlo and kriging-based approaches on analytical test functions in two, four and six dimensions.},
  author = {Janusevskis, Janis and Le Riche, Rodolphe},
  month = jul,
  year = {2010},
  keywords = {Gaussian process,Robust optimization,Kriging,Efficient Global Optimization,EGO,Expected improvement,Optimization under uncertainty,Uncertainty Propagation},
  file = {/home/victor/Zotero/storage/B2SDTQSK/Janusevskis et Le Riche - 2010 - Simultaneous kriging-based sampling for optimizati.pdf}
}

@book{glasserman_monte_2013,
  title = {Monte {{Carlo}} Methods in Financial Engineering},
  volume = {53},
  publisher = {{Springer Science \& Business Media}},
  author = {Glasserman, Paul},
  year = {2013},
  keywords = {MCMC,Gibbs,LHS,Sampling},
  file = {C:\\Users\\Victor\\Documents\\[Paul_Glasserman]_Monte_Carlo_Methods_in_Financial_Engineering_Springer_2003.pdf}
}

@article{volkwein_model_2011,
  title = {Model Reduction Using Proper Orthogonal Decomposition},
  journal = {Lecture Notes, Institute of Mathematics and Scientific Computing, University of Graz. see http://www. uni-graz. at/imawww/volkwein/POD. pdf},
  author = {Volkwein, Stefan},
  year = {2011},
  keywords = {MOR methods,POD},
  file = {/home/victor/Zotero/storage/PXXNM5ZI/POD-Vorlesung.pdf}
}

@article{taddy_fast_2009,
  title = {Fast Inference for Statistical Inverse Problems},
  volume = {25},
  issn = {0266-5611, 1361-6420},
  number = {8},
  journal = {Inverse Problems},
  doi = {10.1088/0266-5611/25/8/085001},
  author = {Taddy, Matthew A and Lee, Herbert K H and Sans{\'o}, Bruno},
  month = aug,
  year = {2009},
  keywords = {stochastic inverse problem},
  pages = {085001},
  file = {/home/victor/Zotero/storage/C7AZEST2/invprob09.pdf}
}

@article{koutsourelakis_multi-resolution_2009,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0810.0744},
  title = {A Multi-Resolution, Non-Parametric, {{Bayesian}} Framework for Identification of Spatially-Varying Model Parameters},
  volume = {228},
  issn = {00219991},
  abstract = {This paper proposes a hierarchical, multi-resolution framework for the identification of model parameters and their spatially variability from noisy measurements of the response or output. Such parameters are frequently encountered in PDE-based models and correspond to quantities such as density or pressure fields, elasto-plastic moduli and internal variables in solid mechanics, conductivity fields in heat diffusion problems, permeability fields in fluid flow through porous media etc. The proposed model has all the advantages of traditional Bayesian formulations such as the ability to produce measures of confidence for the inferences made and providing not only predictive estimates but also quantitative measures of the predictive uncertainty. In contrast to existing approaches it utilizes a parsimonious, non-parametric formulation that favors sparse representations and whose complexity can be determined from the data. The proposed framework in non-intrusive and makes use of a sequence of forward solvers operating at various resolutions. As a result, inexpensive, coarse solvers are used to identify the most salient features of the unknown field(s) which are subsequently enriched by invoking solvers operating at finer resolutions. This leads to significant computational savings particularly in problems involving computationally demanding forward models but also improvements in accuracy. It is based on a novel, adaptive scheme based on Sequential Monte Carlo sampling which is embarrassingly parallelizable and circumvents issues with slow mixing encountered in Markov Chain Monte Carlo schemes.},
  number = {17},
  journal = {Journal of Computational Physics},
  doi = {10.1016/j.jcp.2009.05.016},
  author = {Koutsourelakis, P. S.},
  month = sep,
  year = {2009},
  keywords = {stochastic inverse problem},
  pages = {6184-6211},
  file = {/home/victor/Zotero/storage/2S4Q82F6/Koutsourelakis - 2009 - A multi-resolution, non-parametric, Bayesian frame.pdf}
}

@book{tarantola_inverse_2005,
  address = {{Philadelphia, Pa}},
  title = {Inverse Problem Theory and Methods for Model Parameter Estimation},
  isbn = {978-0-89871-572-9},
  language = {eng},
  publisher = {{SIAM, Society for Industrial and Applied Mathematics}},
  author = {Tarantola, Albert},
  year = {2005},
  keywords = {stochastic inverse problem},
  file = {/home/victor/Zotero/storage/XZX7CY5V/InverseProblemTheory.pdf},
  note = {OCLC: 265659758}
}

@unpublished{gerbeau_moment-matching_2016,
  title = {A Moment-Matching Method to Study the Variability of Phenomena Described by Partial Differential Equations},
  abstract = {Many phenomena are modeled by deterministic differential equations , whereas the observation of these phenomena, in particular in life sciences, exhibits an important variability. This paper addresses the following question: how can the model be adapted to reflect the observed variability? Given an adequate model, it is possible to account for this variability by allowing some parameters to adopt a stochastic behavior. Finding the parameters probability density function that explains the observed variability is a difficult stochastic inverse problem, especially when the computational cost of the forward problem is high. In this paper, a non-parametric and non-intrusive procedure based on offline computations of the forward model is proposed. It infers the probability density function of the uncertain parameters from the matching of the statistical moments of observable degrees of freedom (DOFs) of the model. This inverse procedure is improved by incorporating an algorithm that selects a subset of the model DOFs that both reduces its computational cost and increases its robustness. This algorithm uses the pre-computed model outputs to build an approximation of the local sensitivities. The DOFs are selected so that the maximum information on the sensitivities is conserved. The proposed approach is illustrated with elliptic and parabolic PDEs. In the Appendix, an nonlinear ODE is considered and the strategy is compared with two existing ones.},
  author = {Gerbeau, Jean-Fr{\'e}d{\'e}ric and Lombardi, Damiano and Tixier, Eliott},
  month = nov,
  year = {2016},
  keywords = {Moment matching,stochastic inverse problem,backward uncertainty quantification,maximum entropy},
  file = {/home/victor/Zotero/storage/M4DPXLDQ/Gerbeau et al. - 2016 - A moment-matching method to study the variability .pdf}
}

@phdthesis{couderc_dassfow-shallow_2013,
  title = {Dassfow-Shallow, Variational Data Assimilation for Shallow-Water Models: {{Numerical}} Schemes, User and Developer Guides},
  shorttitle = {Dassfow-Shallow, Variational Data Assimilation for Shallow-Water Models},
  school = {University of Toulouse, CNRS, IMT, INSA, ANR},
  author = {Couderc, Fr{\'e}d{\'e}ric and Madec, Ronan and Monnier, J{\'e}r{\^o}me and Vila, Jean-Paul},
  year = {2013},
  file = {C:\\Users\\Victor\\Dropbox\\INRIA 2017\\Littérature\\DassFow-Shallow_Variational_Data_Assimilation_for_.pdf}
}

@article{santarelli_framework_2010,
  title = {A Framework for Reduced Order Modeling with Mixed Moment Matching and Peak Error Objectives},
  volume = {32},
  number = {2},
  journal = {SIAM Journal on Scientific Computing},
  author = {Santarelli, Keith R.},
  year = {2010},
  keywords = {Moment matching,MOR methods},
  pages = {745--773},
  file = {/home/victor/Zotero/storage/PXPVHUJV/l1_mod_reduction.pdf}
}

@phdthesis{ngodock_assimilation_1996,
  title = {{Assimilation de donn{\'e}es et analyse de sensibilit{\'e}. Une application {\`a} la circulation oc{\'e}anique}},
  abstract = {Le travail men{\'e} dans cette th{\`e}se porte sur l'{\'e}tude "{\`a} posteriori" de l'assimilation variationnelle de donn{\'e}es. Il s'agit d'une d{\'e}marche de faisabilit{\'e} pour la mise au point des outils permettant de faire une analyse diagnostique (qualitative et quantitative) du processus d'assimilation variationnelle, notamment en ce qui concerne l'influence du bruit des observations sur le processus d'assimilation ainsi que sa propagation sur les champs reconstitu{\'e}s (nous sommes alors amen{\'e}s {\`a} faire une {\'e}tude de sensibilit{\'e}), et l'influence de la configuration spatio-temporelle des observations sur le processus d'assimilation. L'application usuelle des {\'e}quations adjointes pour l'analyse de sensibilit{\'e} est revis{\'e}e, car dans le contexte de l'assimilation variationnelle, nous avons montr{\'e} par un exemple simple qu'il faut s'y prendre diff{\'e}remment. Nous proposons alors une m{\'e}thode pour mener correctement cette analyse de sensibilit{\'e}. Cette m{\'e}thode est bas{\'e}e sur l'utilisation des {\'e}quations adjointes au second ordre, obtenues en prenant l'adjoint du syst{\`e}me d'optimalit{\'e}. La sensibilit{\'e} en est d{\'e}duite par inversion du Hessien de la fonction co{\^u}t via la minimisation d'une fonctionnelle quadratique. L'application est faite sur un mod{\`e}le de circulation g{\'e}n{\'e}rale oc{\'e}anique de type quasi-g{\'e}ostrophique, et nous faisons aussi l'{\'e}tude de l'existence et l'unicit{\'e} de la solution de l'{\'e}quation adjointe au second ordre du mod{\`e}le consid{\'e}r{\'e}, pour justifier l'utilisation du Hessien et l'applicabilit{\'e} de notre m{\'e}thode. Nous {\'e}tudions aussi l'influence de la configuration spatio-temporelle des observations sur le processus d'assimilation au travers du Hessien ({\`a} l'optimum) dont les {\'e}l{\'e}ments propres varient lorsqu'on fait varier la configuration. Enfin, nous {\'e}tudions la pr{\'e}dicibilit{\'e} du syst{\`e}me d'optimalit{\'e}.},
  language = {fr},
  school = {Universit{\'e} Joseph-Fourier - Grenoble I},
  author = {Ngodock, Hans Emmanuel},
  month = mar,
  year = {1996},
  keywords = {AS,Thèse,Assimilation de données},
  file = {/home/victor/Zotero/storage/JWJL94ZA/Ngodock - 1996 - Assimilation de données et analyse de sensibilité..pdf;/home/victor/Zotero/storage/5YDEDP49/tel-00005006.html}
}

@article{mckay_comparison_2000,
  title = {A {{Comparison}} of {{Three Methods}} for {{Selecting Values}} of {{Input Variables}} in the {{Analysis}} of {{Output}} from a {{Computer Code}}},
  volume = {42},
  issn = {00401706},
  number = {1},
  journal = {Technometrics},
  doi = {10.2307/1271432},
  author = {Mckay, M. D. and Beckman, R. J. and Conover, W. J.},
  month = feb,
  year = {2000},
  keywords = {Sampling},
  pages = {55},
  file = {C:\\Users\\Victor\\Downloads\\McKayConoverBeckman.pdf}
}

@article{kramer_feedback_2016,
  title = {Feedback {{Control}} for {{Systems}} with {{Uncertain Parameters Using Online}}-{{Adaptive Reduced Models}}},
  author = {Kramer, Boris and Peherstorfer, Benjamin and Willcox, Karen},
  year = {2016},
  keywords = {Optim Robuste},
  file = {C:\\Users\\Victor\\Downloads\\adaptive-model-reduction-control-kramer-peherstorfer-willcox.pdf}
}

@article{volkwein_proper_2005,
  title = {Proper Orthogonal Decomposition ({{POD}}) for Nonlinear Dynamical Systems},
  journal = {Dutch Institute of Systems and Control Summerschool},
  author = {Volkwein, Stefan},
  year = {2005},
  keywords = {MOR methods,POD},
  file = {/home/victor/Zotero/storage/WA26LTST/Volkwein-1.pdf}
}

@article{dellino_robust_2012,
  title = {Robust {{Optimization}} in {{Simulation}}: {{Taguchi}} and {{Krige Combined}}},
  volume = {24},
  issn = {1091-9856, 1526-5528},
  shorttitle = {Robust {{Optimization}} in {{Simulation}}},
  language = {en},
  number = {3},
  journal = {INFORMS Journal on Computing},
  doi = {10.1287/ijoc.1110.0465},
  author = {Dellino, Gabriella and Kleijnen, Jack P. C. and Meloni, Carlo},
  month = aug,
  year = {2012},
  keywords = {Optim Robuste},
  pages = {471-484},
  file = {/home/victor/Zotero/storage/6RZ7KGTL/Dellinoetal_IJOC_24_3_2012.pdf}
}

@article{lehman_designing_2004,
  title = {Designing Computer Experiments to Determine Robust Control Variables},
  journal = {Statistica Sinica},
  author = {Lehman, Jeffrey S. and Santner, Thomas J. and Notz, William I.},
  year = {2004},
  keywords = {Optim Robuste},
  pages = {571--590},
  file = {/home/victor/Zotero/storage/SC8QPVVA/DESIGNING COMP_EXP ROBUST CONTROL VARIABLES.pdf;C:\\Users\\Victor\\Documents\\A14n213.pdf}
}

@phdthesis{baudoui_optimisation_2012,
  title = {Optimisation Robuste Multiobjectifs Par Mod{\`e}les de Substitution},
  school = {Toulouse, ISAE},
  author = {Baudoui, Vincent},
  year = {2012},
  keywords = {Thèse,Optim Robuste},
  file = {/home/victor/Zotero/storage/DIHXGHU4/2012_Baudoui_Vincent.pdf}
}

@phdthesis{zahm_model_2015,
  title = {Model Order Reduction Methods for Parameter-Dependent Equations\textendash{{Applications}} in {{Uncertainty Quantification}}.},
  school = {{\'E}cole Centrale Nantes},
  author = {Zahm, Olivier},
  year = {2015},
  keywords = {MOR methods,Thèse},
  file = {C:\\Users\\Victor\\Downloads\\Zahm_thesis.pdf}
}

@book{toro_riemann_2009,
  address = {{Berlin}},
  edition = {3. ed},
  title = {Riemann Solvers and Numerical Methods for Fluid Dynamics: A Practical Introduction},
  isbn = {978-3-540-25202-3 978-3-540-49834-6},
  shorttitle = {Riemann Solvers and Numerical Methods for Fluid Dynamics},
  abstract = {High resolution upwind and centered methods are today a mature generation of computational techniques applicable to a wide range of engineering and scientific disciplines, Computational Fluid Dynamics (CFD) being the most prominent up to now. This textbook gives a comprehensive, coherent and practical presentation of this class of techniques. The book is designed to provide readers with an understanding of the basic concepts, some of the underlying theory, the ability to critically use the current research papers on the subject, and, above all, with the required information for the practical implementation of the methods. Applications include: compressible, steady, unsteady, reactive, viscous, non-viscous and free surface flows. For this third edition, the book was thoroughly revised. It contains substantial more and new material both in its fundamental as well as in its applied part},
  language = {eng},
  publisher = {{Springer}},
  author = {Toro, Eleuterio F.},
  year = {2009},
  keywords = {Finite Volume},
  file = {C:\\Users\\Victor\\Documents\\Eleuterio_F._Toro_Riemann_Solvers_and_Numerical_Methods_for_Fluid_Dynamics.pdf},
  note = {OCLC: 391057413}
}

@book{rogers_backlund_2002,
  title = {B{\"a}cklund and {{Darboux}} Transformations: Geometry and Modern Applications in Soliton Theory},
  volume = {30},
  shorttitle = {B{\"a}cklund and {{Darboux}} Transformations},
  publisher = {{Cambridge University Press}},
  author = {Rogers, Colin and Schief, Wolfgang Karl},
  year = {2002},
  keywords = {Finite Volume},
  file = {C:\\Users\\Victor\\Documents\\Randall_J._LeVeque_Finite_Volume_Methods_for_Hyperbolic_Problems.pdf}
}

@article{rao_robust_2015,
  title = {Robust Data Assimilation Using \${{L}}\_1\$ and {{Huber}} Norms},
  abstract = {Data assimilation is the process to fuse information from priors, observations of nature, and numerical models, in order to obtain best estimates of the parameters or state of a physical system of interest. Presence of large errors in some observational data, e.g., data collected from a faulty instrument, negatively affect the quality of the overall assimilation results. This work develops a systematic framework for robust data assimilation. The new algorithms continue to produce good analyses in the presence of observation outliers. The approach is based on replacing the traditional \$\textbackslash{}L\_2\$ norm formulation of data assimilation problems with formulations based on \$\textbackslash{}L\_1\$ and Huber norms. Numerical experiments using the Lorenz-96 and the shallow water on the sphere models illustrate how the new algorithms outperform traditional data assimilation approaches in the presence of data outliers.},
  journal = {SciRate},
  author = {Rao, Vishwas and Sandu, Adrian and Ng, Michael and {Nino-Ruiz}, Elias},
  month = nov,
  year = {2015},
  file = {/home/victor/Zotero/storage/A7T67T3Z/Rao et al. - 2015 - Robust data assimilation using $L_1$ and Huber nor.pdf;/home/victor/Zotero/storage/ELRSCLTJ/1511.html}
}

@misc{perrier_robust_,
  title = {Robust {{Bayesian}} Filter with Student-t Likelihood Noise},
  author = {Perrier, Regis},
  file = {/home/victor/Documents/robustBF.pdf}
}

@article{sniedovich_solution_1991,
  title = {Solution Strategies for Variance Minimization Problems},
  volume = {21},
  issn = {0898-1221},
  abstract = {We outline two solutions strategies for optimization problems requiring the minimization of an objective function with a variance or variance-like term.},
  number = {2},
  journal = {Computers \& Mathematics with Applications},
  doi = {10.1016/0898-1221(91)90080-N},
  author = {Sniedovich, Moshe},
  month = jan,
  year = {1991},
  pages = {49-56},
  file = {/home/victor/Zotero/storage/Y9PIPQAH/Sniedovich - 1991 - Solution strategies for variance minimization prob.pdf;/home/victor/Zotero/storage/2WMLP4CY/089812219190080N.html}
}

@book{lehmann_theory_2006,
  title = {Theory of Point Estimation},
  publisher = {{Springer Science \& Business Media}},
  author = {Lehmann, Erich L. and Casella, George},
  year = {2006},
  file = {/home/victor/Zotero/storage/PIU47W8I/Lehmann E.L., Casella G. Theory of point estimation (2ed., Springer, 1998)(ISBN 0387985026)(O)(617s)_MVsa_.pdf;/home/victor/Zotero/storage/QTZDRT6E/books.html}
}

@book{casella_statistical_2002,
  title = {Statistical Inference},
  volume = {2},
  publisher = {{Duxbury Pacific Grove, CA}},
  author = {Casella, George and Berger, Roger L.},
  year = {2002},
  file = {/home/victor/Zotero/storage/SQK9PBTE/Casella et Berger - 2002 - Statistical inference.pdf;/home/victor/Zotero/storage/X9I3J2BZ/Casella et Berger - 2002 - Statistical inference.pdf}
}

@incollection{huber_robust_2011,
  title = {Robust Statistics},
  booktitle = {International {{Encyclopedia}} of {{Statistical Science}}},
  publisher = {{Springer}},
  author = {Huber, Peter J.},
  year = {2011},
  pages = {1248--1251},
  file = {/home/victor/Zotero/storage/48JTUEST/Huber - 2011 - Robust statistics.pdf;/home/victor/Zotero/storage/SA65GIYR/10.html}
}

@article{basu_robust_1998,
  title = {Robust and Efficient Estimation by Minimising a Density Power Divergence},
  volume = {85},
  number = {3},
  journal = {Biometrika},
  author = {Basu, Ayanendranath and Harris, Ian R. and Hjort, Nils L. and Jones, M. C.},
  year = {1998},
  pages = {549--559},
  file = {/home/victor/Zotero/storage/IMF8SAEK/1997-7.pdf}
}

@inproceedings{benneyan_probability_2006,
  title = {Probability Distributions and Variances of Quadratic Loss Functions},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Computers}} and {{Industrial Engineering}}},
  author = {Benneyan, James and Aksezer, {\c C}a{\u g}lar},
  year = {2006},
  pages = {2890--2899},
  file = {/home/victor/Zotero/storage/LTKZVHNL/5db30bc0fe22538899a233d598828eac3e88.pdf}
}

@article{vapnik_overview_1999,
  title = {An Overview of Statistical Learning Theory},
  volume = {10},
  number = {5},
  journal = {IEEE transactions on neural networks},
  author = {Vapnik, Vladimir N.},
  year = {1999},
  pages = {988--999},
  file = {/home/victor/Zotero/storage/G66K28RE/slt.pdf}
}

@inproceedings{landrieu_cut_2016,
  address = {{Cadix, Spain}},
  title = {Cut {{Pursuit}}: Fast Algorithms to Learn Piecewise Constant Functions},
  shorttitle = {Cut {{Pursuit}}},
  abstract = {We propose working-set/greedy algorithms to efficiently find the solutions to convex optimization problems penalized respectively by the total variation and the Mumford Shah boundary size. Our algorithms exploit the piecewise constant structure of the level-sets of the solutions by recursively splitting them using graph cuts. We obtain significant speed up on images that can be approximated with few level-sets compared to state-of-the-art algorithms .},
  booktitle = {19th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}} 2016)},
  author = {Landrieu, Loic and Obozinski, Guillaume},
  month = may,
  year = {2016},
  keywords = {greedy algorithm,minimal partition,Mumford-Shah,optimization,working-set},
  file = {/home/victor/Zotero/storage/GPVV5DNS/Landrieu et Obozinski - 2016 - Cut Pursuit fast algorithms to learn piecewise co.pdf}
}

@article{verdu_minimax_1984,
  title = {On Minimax Robustness: {{A}} General Approach and Applications},
  volume = {30},
  shorttitle = {On Minimax Robustness},
  number = {2},
  journal = {IEEE Transactions on Information Theory},
  author = {Verdu, Sergio and Poor, H.},
  year = {1984},
  pages = {328--340},
  file = {/home/victor/Zotero/storage/X55UAWLR/Verdu et Poor - 1984 - On minimax robustness A general approach and appl.pdf;/home/victor/Zotero/storage/XCNS6EWW/1056876.html}
}

@article{over_strategy_2013,
  title = {A Strategy for Improved Computational Efficiency of the Method of Anchored Distributions},
  volume = {49},
  issn = {1944-7973},
  abstract = {This paper proposes a strategy for improving the computational efficiency of model inversion using the method of anchored distributions (MAD) by ``bundling'' similar model parametrizations in the likelihood function. Inferring the likelihood function typically requires a large number of forward model (FM) simulations for each possible model parametrization; as a result, the process is quite expensive. To ease this prohibitive cost, we present an approximation for the likelihood function called bundling that relaxes the requirement for high quantities of FM simulations. This approximation redefines the conditional statement of the likelihood function as the probability of a set of similar model parametrizations ``bundle'' replicating field measurements, which we show is neither a model reduction nor a sampling approach to improving the computational efficiency of model inversion. To evaluate the effectiveness of these modifications, we compare the quality of predictions and computational cost of bundling relative to a baseline MAD inversion of 3-D flow and transport model parameters. Additionally, to aid understanding of the implementation we provide a tutorial for bundling in the form of a sample data set and script for the R statistical computing language. For our synthetic experiment, bundling achieved a 35\% reduction in overall computational cost and had a limited negative impact on predicted probability distributions of the model parameters. Strategies for minimizing error in the bundling approximation, for enforcing similarity among the sets of model parametrizations, and for identifying convergence of the likelihood function are also presented.},
  language = {en},
  number = {6},
  journal = {Water Resources Research},
  doi = {10.1002/wrcr.20182},
  author = {Over, Matthew William and Yang, Yarong and Chen, Xingyuan and Rubin, Yoram},
  month = jun,
  year = {2013},
  keywords = {Bayesian,approximation,clustering,computational efficiency,inversion modeling},
  pages = {3257-3275},
  file = {/home/victor/Zotero/storage/68NCTQID/Over et al. - 2013 - A strategy for improved computational efficiency o.pdf;/home/victor/Zotero/storage/HXLC3EW5/abstract.html}
}

@article{rubin_bayesian_2010,
  title = {A {{Bayesian}} Approach for Inverse Modeling, Data Assimilation, and Conditional Simulation of Spatial Random Fields: {{METHOD OF ANCHORED DISTRIBUTIONS}}},
  volume = {46},
  issn = {00431397},
  shorttitle = {A {{Bayesian}} Approach for Inverse Modeling, Data Assimilation, and Conditional Simulation of Spatial Random Fields},
  language = {en},
  number = {10},
  journal = {Water Resources Research},
  doi = {10.1029/2009WR008799},
  author = {Rubin, Yoram and Chen, Xingyuan and Murakami, Haruko and Hahn, Melanie},
  month = oct,
  year = {2010},
  file = {/home/victor/Zotero/storage/CNUY359C/wrcr12530.pdf}
}

@phdthesis{asri_etude_2014,
  title = {{{\'E}tude des M-estimateurs et leurs versions pond{\'e}r{\'e}es pour des donn{\'e}es clusteris{\'e}es}},
  abstract = {La classe des M-estimateurs engendre des estimateurs classiques d'un param{\`e}tre de localisation multidimensionnel tels que l'estimateur du maximum de vraisemblance, la moyenne empirique et la m{\'e}diane spatiale. Huber (1964) introduit les M-estimateurs dans le cadre de l'{\'e}tude des estimateurs robustes. Parmi la litt{\'e}rature d{\'e}di{\'e}e {\`a} ces estimateurs, on trouve en particulier les ouvrages de Huber (1981) et de Hampel et al. (1986) sur le comportement asymptotique et la robustesse via le point de rupture et la fonction d'influence (voir Ruiz-Gazen (2012) pour une synth{\`e}se sur ces notions). Plus r{\'e}cemment, des r{\'e}sultats sur la convergence et la normalit{\'e} asymptotique sont {\'e}tablis par Van der Vaart (2000) dans le cadre multidimensionnel. Nevalainen et al. (2006, 2007) {\'e}tudient le cas particulier de la m{\'e}diane spatiale pond{\'e}r{\'e}e et non-pond{\'e}r{\'e}e dans le cas clusteris{\'e}. Nous g{\'e}n{\'e}ralisons ces r{\'e}sultats aux M-estimateurs pond{\'e}r{\'e}s. Nous {\'e}tudions leur convergence presque s{\^u}re, leur normalit{\'e} asymptotique ainsi que leur robustesse dans le cas de donn{\'e}es clusteris{\'e}es.},
  language = {fr},
  school = {Universit{\'e} d'Avignon},
  author = {Asri, Mohamed El},
  month = dec,
  year = {2014},
  file = {/home/victor/Zotero/storage/7NP8Z3XT/Asri - 2014 - Étude des M-estimateurs et leurs versions pondérée.pdf;/home/victor/Zotero/storage/EJV47BT2/tel-01202540.html}
}

@article{wang_asymptotic_1999,
  title = {Asymptotic {{Properties}} of {{M}}-Estimators {{Based}} on {{Estimating Equations}} and {{Censored Data}}},
  volume = {26},
  number = {2},
  journal = {Scandinavian journal of statistics},
  author = {Wang, Jane-Ling},
  year = {1999},
  pages = {297--318},
  file = {/home/victor/Zotero/storage/7KCAXALE/Wang - 1999 - Asymptotic Properties of M-estimators Based on Est.pdf;/home/victor/Zotero/storage/HT2CNHF4/Wang - 1999 - Asymptotic Properties of M-estimators Based on Est}
}

@article{stefanski_calculus_2002,
  title = {The Calculus of {{M}}-Estimation},
  volume = {56},
  number = {1},
  journal = {The American Statistician},
  author = {Stefanski, Leonard A. and Boos, Dennis D.},
  year = {2002},
  pages = {29--38},
  file = {/home/victor/Zotero/storage/Z5S4DBRT/Stefanski et Boos - 2002 - The calculus of M-estimation.pdf;/home/victor/Zotero/storage/V9N9YLHS/000313002753631330.html}
}

@misc{gassiat_statistiques_,
  title = {Statistiques {{Asymptotiques}} - {{Notes}} de {{Cours M2}}},
  author = {Gassiat, Elisabeth},
  file = {/home/victor/Zotero/storage/LCNS66GE/notes de cours- M2Stat.pdf}
}

@article{tran_spectral_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.08815},
  primaryClass = {cs, stat},
  title = {Spectral {{M}}-Estimation with {{Applications}} to {{Hidden Markov Models}}},
  abstract = {Method of moment estimators exhibit appealing statistical properties, such as asymptotic unbiasedness, for nonconvex problems. However, they typically require a large number of samples and are extremely sensitive to model misspecification. In this paper, we apply the framework of M-estimation to develop both a generalized method of moments procedure and a principled method for regularization. Our proposed M-estimator obtains optimal sample efficiency rates (in the class of moment-based estimators) and the same well-known rates on prediction accuracy as other spectral estimators. It also makes it straightforward to incorporate regularization into the sample moment conditions. We demonstrate empirically the gains in sample efficiency from our approach on hidden Markov models.},
  journal = {arXiv:1603.08815 [cs, stat]},
  author = {Tran, Dustin and Kim, Minjae and {Doshi-Velez}, Finale},
  month = mar,
  year = {2016},
  keywords = {Computer Science - Learning,Statistics - Methodology,Statistics - Computation},
  file = {/home/victor/Zotero/storage/NJYEM92J/Tran et al. - 2016 - Spectral M-estimation with Applications to Hidden .pdf;/home/victor/Zotero/storage/6MMGQQSJ/1603.html}
}

@article{lu_limitations_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1404.7188},
  title = {Limitations of Polynomial Chaos Expansions in the {{Bayesian}} Solution of Inverse Problems},
  volume = {282},
  issn = {00219991},
  abstract = {Polynomial chaos expansions are used to reduce the computational cost in the Bayesian solutions of inverse problems by creating a surrogate posterior that can be evaluated inexpensively. We show, by analysis and example, that when the data contain significant information beyond what is assumed in the prior, the surrogate posterior can be very different from the posterior, and the resulting estimates become inaccurate. One can improve the accuracy by adaptively increasing the order of the polynomial chaos, but the cost may increase too fast for this to be cost effective compared to Monte Carlo sampling without a surrogate posterior.},
  journal = {Journal of Computational Physics},
  doi = {10.1016/j.jcp.2014.11.010},
  author = {Lu, Fei and Morzfeld, Matthias and Tu, Xuemin and Chorin, Alexandre J.},
  month = feb,
  year = {2015},
  keywords = {Statistics - Computation,Mathematics - Numerical Analysis},
  pages = {138-147},
  file = {/home/victor/Zotero/storage/7UYTAW8U/Lu et al. - 2015 - Limitations of polynomial chaos expansions in the .pdf;/home/victor/Zotero/storage/IX3UP4AL/LMTC14.pdf;/home/victor/Zotero/storage/K987393G/1404.html}
}

@article{atkins_implicit_2012,
  title = {Implicit {{Particle Methods}} and {{Their Connection}} with {{Variational Data Assimilation}}},
  volume = {141},
  issn = {0027-0644},
  abstract = {The implicit particle filter is a sequential Monte Carlo method for data assimilation that guides the particles to the high-probability regions via a sequence of steps that includes minimizations. A new and more general derivation of this approach is presented and the method is extended to particle smoothing as well as to data assimilation for perfect models. Minimizations required by implicit particle methods are shown to be similar to those that one encounters in variational data assimilation, and the connection of implicit particle methods with variational data assimilation is explored. In particular, it is argued that existing variational codes can be converted into implicit particle methods at a low additional cost, often yielding better estimates that are also equipped with quantitative measures of the uncertainty. A detailed example is presented.},
  number = {6},
  journal = {Monthly Weather Review},
  doi = {10.1175/MWR-D-12-00145.1},
  author = {Atkins, Ethan and Morzfeld, Matthias and Chorin, Alexandre J.},
  month = nov,
  year = {2012},
  pages = {1786-1803},
  file = {/home/victor/Zotero/storage/PXUNNLB6/Atkins et al. - 2012 - Implicit Particle Methods and Their Connection wit.pdf;/home/victor/Zotero/storage/WLSWGZPR/MWR-D-12-00145.html}
}

@article{bilionis_solution_2013,
  title = {Solution of Inverse Problems with Limited Forward Solver Evaluations: A {{Bayesian}} Perspective},
  volume = {30},
  shorttitle = {Solution of Inverse Problems with Limited Forward Solver Evaluations},
  number = {1},
  journal = {Inverse Problems},
  author = {Bilionis, I. and Zabaras, N.},
  year = {2013},
  pages = {015004},
  file = {/home/victor/Zotero/storage/SUA66K2E/Bilionis et Zabaras - 2013 - Solution of inverse problems with limited forward .pdf;/home/victor/Zotero/storage/TMEH4W3B/Bilionis et Zabaras - 2013 - Solution of inverse problems with limited forward}
}

@phdthesis{villemonteix_optimisation_2008,
  title = {{Optimisation de fonctions co{\^u}teuses{$<$}br /{$>$}Mod{\`e}les gaussiens pour une utilisation efficace du budget d'{\'e}valuations : th{\'e}orie et pratique industrielle}},
  shorttitle = {{Optimisation de fonctions co{\^u}teuses{$<$}br /{$>$}Mod{\`e}les gaussiens pour une utilisation efficace du budget d'{\'e}valuations}},
  abstract = {Cette th{\`e}se traite d'une question centrale dans de nombreux probl{\`e}mes d'optimisation, en particulier{$<$}br /{$>$}en ing{\'e}nierie. Comment optimiser une fonction lorsque le nombre d'{\'e}valuations autoris{\'e} est tr{\`e}s limit{\'e} au regard de la dimension et de la complexit{\'e} du probl{\`e}me ? Par exemple, lorsque le budget d'{\'e}valuations est limit{\'e} par la dur{\'e}e des simulations num{\'e}riques du syst{\`e}me {\`a} optimiser, il n'est pas rare de devoir optimiser trente param{\`e}tres avec moins{$<$}br /{$>$}de cent {\'e}valuations. Ce travail traite d'algorithmes d'optimisation sp{\'e}cifiques {\`a} ce contexte pour lequel la plupart des m{\'e}thodes classiques sont inadapt{\'e}es.{$<$}br /{$>$}Le principe commun aux m{\'e}thodes propos{\'e}es est d'exploiter les propri{\'e}t{\'e}s des processus gaussiens et du krigeage pour construire une approximation peu co{\^u}teuse de la fonction {\`a} optimiser. Cette approximation est ensuite utilis{\'e}e pour choisir it{\'e}rativement les {\'e}valuations {\`a} r{\'e}aliser. Ce choix est dict{\'e} par un crit{\`e}re d'{\'e}chantillonnage qui combine recherche locale, {\`a} proximit{\'e} des r{\'e}sultats prometteurs, et recherche globale, dans les zones non explor{\'e}es. La plupart des crit{\`e}res propos{\'e}s dans la litt{\'e}rature, tel celui de l'algorithme EGO (pour Efficient Global Optimization), cherchent {\`a} {\'e}chantillonner la fonction l{\`a} o{\`u} l'apparition d'un optimum est jug{\'e}e la plus probable. En comparaison, l'algorithme IAGO (pour Informational Approach to Global Optimization), principale contribution de nos travaux, cherche {\`a} maximiser la quantit{\'e} d'information apport{\'e}e, sur la position de l'optimum, par l'{\'e}valuation r{\'e}alis{\'e}e. Des probl{\'e}matiques industrielles ont guid{\'e} l'organisation de ce m{\'e}moire, qui se destine {\`a} la communaut{\'e} de l'optimisation{$<$}br /{$>$}tout comme aux praticiens confront{\'e}s {\`a} des fonctions {\`a} l'{\'e}valuation co{\^u}teuse. Aussi les applications industrielles y tiennent-elles une place importante tout comme la mise en place de l'algorithme IAGO. Nous d{\'e}taillons non seulement le cas standard de l'optimisation d'une fonction r{\'e}elle, mais aussi la prise en compte de contraintes, de{$<$}br /{$>$}bruit sur les r{\'e}sultats des {\'e}valuations, de r{\'e}sultats d'{\'e}valuation du gradient, de probl{\`e}mes multi-objectifs, ou encore d'incertitudes de fabrication significatives.},
  language = {fr},
  school = {Universit{\'e} Paris Sud - Paris XI},
  author = {Villemonteix, Julien},
  month = dec,
  year = {2008},
  file = {/home/victor/Zotero/storage/TLCKEE4N/Villemonteix - 2008 - Optimisation de fonctions coûteusesbr Modèles g.pdf;/home/victor/Zotero/storage/9LIXMHQU/tel-00351406.html}
}

@inproceedings{shankaran_robust_2011,
  title = {Robust Optimal Control Using Polynomial Chaos and Adjoints for Systems with Uncertain Inputs},
  booktitle = {20th {{AIAA Computational Fluid Dynamics Conference}}},
  author = {Shankaran, Sriram and Jameson, Antony},
  year = {2011},
  pages = {3069},
  file = {/home/victor/Zotero/storage/P4J2SVCR/Shankaran et Jameson - 2011 - Robust optimal control using polynomial chaos and .pdf;/home/victor/Zotero/storage/NLRPI8UX/6.html}
}

@article{ulaganathan_high_2016,
  title = {High Dimensional {{Kriging}} Metamodelling Utilising Gradient Information},
  volume = {40},
  number = {9},
  journal = {Applied Mathematical Modelling},
  author = {Ulaganathan, Selvakumar and Couckuyt, Ivo and Dhaene, Tom and Degroote, Joris and Laermans, Eric},
  year = {2016},
  pages = {5256--5270},
  file = {/home/victor/Zotero/storage/Z94MSB8G/8159395.pdf}
}

@book{j._bichonyyk_reliability-based_2009,
  title = {Reliability-{{Based Design Optimization Using Efficient Global Reliability Analysis}}},
  abstract = {Finding the optimal (lightest, least expensive, etc.) design for an engineered component that meets or exceeds a specified level of reliability is a problem of obvious interest across a wide spectrum of engineering fields. Various methods for this reliability-based design optimization problem have been proposed. Unfortunately, this problem is rarely solved in practice because, regardless of the method used, solving the problem is too expensive or the final solution is too inaccurate to ensure that the reliability constraint is actually satisfied. This is especially true for engineering applications involving expensive, implicit, and possibly nonlinear performance functions (such as large finite element models). The Efficient Global Reliability Analysis method was recently introduced to improve both the accuracy and efficiency of reliability analysis for this type of performance function. This paper explores how this new reliability analysis method can be used in a design optimization context to create a method of sufficient accuracy and efficiency to enable the use of reliability-based design optimization as a practical design tool.},
  author = {J. Bichonyyk, Barron and Mahadevanky, Sankaran and Eldred, Michael},
  month = may,
  year = {2009},
  doi = {10.2514/6.2009-2261}
}

@article{kumar_adjoint_,
  title = {Adjoint Based Multi-Objective Shape Optimization of a Transonic Airfoil under Uncertainties},
  author = {Kumar, D. and Miranda, J. and Raisee, M. and Lacor, C.},
  file = {/home/victor/Zotero/storage/P6MU9IAM/10873.pdf;/home/victor/Zotero/storage/Y84GAN9M/310.pdf}
}

@article{feliot_bayesian_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1510.00503},
  title = {A {{Bayesian}} Approach to Constrained Single- and Multi-Objective Optimization},
  volume = {67},
  issn = {0925-5001, 1573-2916},
  abstract = {This article addresses the problem of derivative-free (single- or multi-objective) optimization subject to multiple inequality constraints. Both the objective and constraint functions are assumed to be smooth, non-linear and expensive to evaluate. As a consequence, the number of evaluations that can be used to carry out the optimization is very limited, as in complex industrial design optimization problems. The method we propose to overcome this difficulty has its roots in both the Bayesian and the multi-objective optimization literatures. More specifically, an extended domination rule is used to handle objectives and constraints in a unified way, and a corresponding expected hyper-volume improvement sampling criterion is proposed. This new criterion is naturally adapted to the search of a feasible point when none is available, and reduces to existing Bayesian sampling criteria---the classical Expected Improvement (EI) criterion and some of its constrained/multi-objective extensions---as soon as at least one feasible point is available. The calculation and optimization of the criterion are performed using Sequential Monte Carlo techniques. In particular, an algorithm similar to the subset simulation method, which is well known in the field of structural reliability, is used to estimate the criterion. The method, which we call BMOO (for Bayesian Multi-Objective Optimization), is compared to state-of-the-art algorithms for single- and multi-objective constrained optimization.},
  number = {1-2},
  journal = {Journal of Global Optimization},
  doi = {10.1007/s10898-016-0427-3},
  author = {Feliot, Paul and Bect, Julien and Vazquez, Emmanuel},
  month = jan,
  year = {2017},
  keywords = {Statistics - Machine Learning,Statistics - Computation},
  pages = {97-133},
  file = {/home/victor/Zotero/storage/FT8S2KHS/Feliot et al. - 2017 - A Bayesian approach to constrained single- and mul.pdf;/home/victor/Zotero/storage/ACBZ689B/1510.html}
}

@phdthesis{bettinger_inversion_2009,
  type = {{{PhD Thesis}}},
  title = {Inversion d'un Syst{\`e}me Par Krigeage: Application {\`a} La Synth{\`e}se Des Catalyseurs {\`a} Haut D{\'e}bit},
  shorttitle = {Inversion d'un Syst{\`e}me Par Krigeage},
  school = {Universit{\'e} de Nice Sophia Antipolis},
  author = {Bettinger, R{\'e}gis},
  year = {2009},
  file = {/home/victor/Zotero/storage/3RXCDSE8/Bettinger.pdf}
}

@article{rue_approximate_2009,
  title = {Approximate {{Bayesian}} Inference for Latent {{Gaussian}} Models by Using Integrated Nested {{Laplace}} Approximations},
  volume = {71},
  number = {2},
  journal = {Journal of the royal statistical society: Series b (statistical methodology)},
  author = {avard Rue, H\textbackslash{}a and Martino, Sara and Chopin, Nicolas},
  year = {2009},
  pages = {319--392},
  file = {/home/victor/Zotero/storage/AEMPKWJW/Rue et al. - 2009 - Approximate Bayesian inference for latent Gaussian.pdf;/home/victor/Zotero/storage/XZ5P7BEX/Rue et al. - 2009 - Approximate Bayesian inference for latent Gaussian}
}

@article{guhaniyogi_divide-and-conquer_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.09767},
  primaryClass = {stat},
  title = {A {{Divide}}-and-{{Conquer Bayesian Approach}} to {{Large}}-{{Scale Kriging}}},
  abstract = {Flexible hierarchical Bayesian modeling of massive data is challenging due to poorly scaling computations in large sample size settings. This article is motivated by spatial process models for analyzing geostatistical data, which typically entail computations that become prohibitive as the number of spatial locations becomes large. We propose a three-step divide-and-conquer strategy within the Bayesian paradigm to achieve massive scalability for any spatial process model. We partition the data into a large number of subsets, apply a readily available Bayesian spatial process model on every subset in parallel, and optimally combine the posterior distributions estimated across all the subsets into a pseudo-posterior distribution that conditions on the entire data. The combined pseudo posterior distribution is used for predicting the responses at arbitrary locations and for performing posterior inference on the model parameters and the residual spatial surface. We call this approach "Distributed Kriging" (DISK). It offers significant advantages in applications where the entire data are or can be stored on multiple machines. Under the standard theoretical setup, we show that if the number of subsets is not too large, then the Bayes risk of estimating the true residual spatial surface using the DISK posterior distribution decays to zero at a nearly optimal rate. While DISK is a general approach to distributed nonparametric regression, we focus on its applications in spatial statistics and demonstrate its empirical performance using a stationary full-rank and a nonstationary low-rank model based on Gaussian process (GP) prior. A variety of simulations and a geostatistical analysis of the Pacific Ocean sea surface temperature data validate our theoretical results.},
  journal = {arXiv:1712.09767 [stat]},
  author = {Guhaniyogi, Rajarshi and Li, Cheng and Savitsky, Terrance D. and Srivastava, Sanvesh},
  month = dec,
  year = {2017},
  keywords = {Statistics - Methodology},
  file = {/home/victor/Zotero/storage/95I6757C/Guhaniyogi et al. - 2017 - A Divide-and-Conquer Bayesian Approach to Large-Sc.pdf;/home/victor/Zotero/storage/2H2UGTP6/1712.html}
}

@article{kitanidis_parameter_1986,
  title = {Parameter Uncertainty in Estimation of Spatial Functions: {{Bayesian}} Analysis},
  volume = {22},
  shorttitle = {Parameter Uncertainty in Estimation of Spatial Functions},
  number = {4},
  journal = {Water resources research},
  author = {Kitanidis, Peter K.},
  year = {1986},
  pages = {499--507},
  file = {/home/victor/Zotero/storage/5NLQMAEG/Kitanidis - 1986 - Parameter uncertainty in estimation of spatial fun;/home/victor/Zotero/storage/U8Q6L32Z/Kitanidis - 1986 - Parameter uncertainty in estimation of spatial fun.pdf}
}

@book{d._morris_bayesian_2012,
  title = {Bayesian {{Design}} and {{Analysis}} of {{Computer Experiments}}: {{Use}} of {{Derivatives}} in {{Surface Prediction}}},
  volume = {35},
  shorttitle = {Bayesian {{Design}} and {{Analysis}} of {{Computer Experiments}}},
  abstract = {The work of Currin et al. and others in developing fast predictive approximations'' of computer models is extended for the case in which derivatives of the output variable of interest with respect to input variables are available. In addition to describing the calculations required for the Bayesian analysis, the issue of experimental design is also discussed, and an algorithm is described for constructing maximin distance'' designs. An example is given based on a demonstration model of eight inputs and one output, in which predictions based on a maximin design, a Latin hypercube design, and two compromise'' designs are evaluated and compared. 12 refs., 2 figs., 6 tabs.},
  author = {D. Morris, Max and J. Mitchell, Toby and Ylvisaker, Don},
  month = mar,
  year = {2012},
  doi = {10.1080/00401706.1993.10485320}
}

@book{tikhonov_solutions_1977,
  title = {Solutions of Ill-Posed Problems},
  volume = {14},
  author = {Tikhonov, Andrei and Arsenin, Vasily},
  year = {1977},
  keywords = {Regularization,Tikhonov}
}

@article{berger_overview_1994,
  title = {An Overview of Robust {{Bayesian}} Analysis},
  volume = {3},
  number = {1},
  journal = {Test},
  author = {Berger, James O. and Moreno, El{\'i}as and Pericchi, Luis Raul and Bayarri, M. Jes{\'u}s and Bernardo, Jos{\'e} M. and Cano, Juan A. and {De la Horra}, Juli{\'a}n and Mart{\'i}n, Jacinto and {R{\'i}os-Ins{\'u}a}, David and Betr{\`o}, Bruno},
  year = {1994},
  keywords = {Bayesian inference},
  pages = {5--124},
  file = {/home/victor/Zotero/storage/8IEPC4A3/tr93-53c.pdf;/home/victor/Zotero/storage/KN3BLS4A/BF02562676.html}
}

@article{van_parys_data_2017,
  title = {From {{Data}} to {{Decisions}}: {{Distributionally Robust Optimization}} Is {{Optimal}}},
  shorttitle = {From {{Data}} to {{Decisions}}},
  journal = {arXiv preprint arXiv:1704.04118},
  author = {Van Parys, Bart PG and Esfahani, Peyman Mohajerin and Kuhn, Daniel},
  year = {2017},
  file = {/home/victor/Zotero/storage/KMNWRXK6/5961.pdf}
}

@article{gould_differentiating_2016,
  title = {On Differentiating Parameterized Argmin and Argmax Problems with Application to Bi-Level Optimization},
  journal = {arXiv preprint arXiv:1607.05447},
  author = {Gould, Stephen and Fernando, Basura and Cherian, Anoop and Anderson, Peter and Cruz, Rodrigo Santa and Guo, Edison},
  year = {2016},
  file = {/home/victor/Zotero/storage/5TLGQHHW/argmin-TR-2016.pdf}
}

@book{shapiro_lectures_2009,
  title = {Lectures on {{Stochastic Programming}}: {{Modeling}} and {{Theory}}},
  isbn = {978-0-89871-687-0 978-0-89871-875-1},
  shorttitle = {Lectures on {{Stochastic Programming}}},
  language = {en},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Shapiro, Alexander and Dentcheva, Darinka and Ruszczy{\'n}ski, Andrzej},
  month = jan,
  year = {2009},
  keywords = {Stochastic programming},
  file = {/home/victor/Zotero/storage/QD5HH7UZ/Lectures-on-stochastic-programming-Modeling-and-theory.pdf},
  doi = {10.1137/1.9780898718751}
}

@article{wit_all_2012,
  title = {`{{All}} Models Are Wrong...': An Introduction to Model Uncertainty: {{{\emph{Introduction}}}}{\emph{ to Model Uncertainty}}},
  volume = {66},
  issn = {00390402},
  shorttitle = {`{{All}} Models Are Wrong...'},
  language = {en},
  number = {3},
  journal = {Statistica Neerlandica},
  doi = {10.1111/j.1467-9574.2012.00530.x},
  author = {Wit, Ernst and van den Heuvel, Edwin and Romeijn, Jan-Willem},
  month = aug,
  year = {2012},
  keywords = {Model inadequacy,Model selection},
  pages = {217-236},
  file = {/home/victor/Zotero/storage/E2B7ZAJV/2012_wit_et_al_-_all_models_are_wrong.pdf}
}

@book{massart_concentration_2007,
  title = {Concentration Inequalities and Model Selection},
  volume = {6},
  publisher = {{Springer}},
  author = {Massart, Pascal},
  year = {2007},
  keywords = {Model selection},
  file = {/home/victor/Zotero/storage/D6LDMJUI/flour.pdf}
}

@phdthesis{oakley_bayesian_1999,
  type = {{{PhD Thesis}}},
  title = {Bayesian Uncertainty Analysis for Complex Computer Codes},
  school = {University of Sheffield},
  author = {Oakley, Jeremy},
  year = {1999},
  keywords = {Bayesian inference},
  file = {/home/victor/Zotero/storage/GDRTYMEY/jeothesis.pdf}
}

@article{damblin_adaptive_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.07252},
  primaryClass = {stat},
  title = {Adaptive Numerical Designs for the Calibration of Computer Codes},
  abstract = {Making good predictions of a physical system using a computer code requires the inputs to be carefully specified. Some of these inputs called control variables have to reproduce physical conditions whereas other inputs, called parameters, are specific to the computer code and most often uncertain. The goal of statistical calibration consists in estimating these parameters with the help of a statistical model which links the code outputs with the field measurements. In a Bayesian setting, the posterior distribution of these parameters is normally sampled using MCMC methods. However, they are impractical when the code runs are high time-consuming. A way to circumvent this issue consists of replacing the computer code with a Gaussian process emulator, then sampling a cheap-to-evaluate posterior distribution based on it. Doing so, calibration is subject to an error which strongly depends on the numerical design of experiments used to fit the emulator. We aim at reducing this error by building a proper sequential design by means of the Expected Improvement criterion. Numerical illustrations in several dimensions assess the efficiency of such sequential strategies.},
  journal = {arXiv:1502.07252 [stat]},
  author = {Damblin, Guillaume and Barbillon, Pierre and Keller, Merlin and Pasanisi, Alberto and Parent, Eric},
  month = feb,
  year = {2015},
  keywords = {Calibration,Numerical design,Adaptive sampling},
  file = {/home/victor/Zotero/storage/9YULXTPU/Damblin et al. - 2015 - Adaptive numerical designs for the calibration of .pdf;/home/victor/Zotero/storage/LVYNJDWT/1502.html}
}

@article{bayarri_framework_2007,
  title = {A Framework for Validation of Computer Models},
  volume = {49},
  number = {2},
  journal = {Technometrics},
  author = {Bayarri, Maria J. and Berger, James O. and Paulo, Rui and Sacks, Jerry and Cafeo, John A. and Cavendish, James and Lin, Chin-Hsu and Tu, Jian},
  year = {2007},
  keywords = {Validation,Verification},
  pages = {138--154},
  file = {/home/victor/Zotero/storage/6RNCHIIS/Bayarri et al. - 2007 - A framework for validation of computer models.pdf}
}

@article{brynjarsdottir_learning_2014,
  title = {Learning about Physical Parameters: The Importance of Model Discrepancy},
  volume = {30},
  issn = {0266-5611, 1361-6420},
  shorttitle = {Learning about Physical Parameters},
  number = {11},
  journal = {Inverse Problems},
  doi = {10.1088/0266-5611/30/11/114007},
  author = {Brynjarsd{\'o}ttir, Jenn{\'y} and O'Hagan, Anthony},
  month = nov,
  year = {2014},
  keywords = {Model inadequacy,Discrepancy},
  pages = {114007},
  file = {/home/victor/Zotero/storage/97P352BQ/Learning-about-physical-parameters-The-importance-of-model-discrepancy.pdf}
}

@article{wong_frequentist_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.4723},
  primaryClass = {stat},
  title = {A {{Frequentist Approach}} to {{Computer Model Calibration}}},
  abstract = {This paper considers the computer model calibration problem and provides a general frequentist solution. Under the proposed framework, the data model is semi-parametric with a nonparametric discrepancy function which accounts for any discrepancy between the physical reality and the computer model. In an attempt to solve a fundamentally important (but often ignored) identifiability issue between the computer model parameters and the discrepancy function, this paper proposes a new and identifiable parametrization of the calibration problem. It also develops a two-step procedure for estimating all the relevant quantities under the new parameterization. This estimation procedure is shown to enjoy excellent rates of convergence and can be straightforwardly implemented with existing software. For uncertainty quantification, bootstrapping is adopted to construct confidence regions for the quantities of interest. The practical performance of the proposed methodology is illustrated through simulation examples and an application to a computational fluid dynamics model.},
  journal = {arXiv:1411.4723 [stat]},
  author = {Wong, Raymond K. W. and Storlie, Curtis B. and Lee, Thomas C. M.},
  month = nov,
  year = {2014},
  keywords = {Calibration,Frequentist},
  file = {/home/victor/Zotero/storage/H2NR9V7X/Wong et al. - 2014 - A Frequentist Approach to Computer Model Calibrati.pdf;/home/victor/Zotero/storage/J3MIEC85/1411.html}
}

@article{gong_adaptive_2017,
  title = {An Adaptive Surrogate Modeling-Based Sampling Strategy for Parameter Optimization and Distribution Estimation ({{ASMO}}-{{PODE}})},
  volume = {95},
  issn = {1364-8152},
  abstract = {Parameter distribution estimation has long been a hot issue for the uncertainty quantification of environmental models. Traditional approaches such as MCMC (Markov Chain Monte Carlo) are prohibitive to be applied to large complex dynamic models because of the high computational cost of computing resources. To reduce the number of model evaluations required, we proposed an adaptive surrogate modeling-based sampling strategy for parameter distribution estimation, named ASMO-PODE (Adaptive Surrogate Modeling-based Optimization \textendash{} Parameter Optimization and Distribution Estimation). The ASMO-PODE can provide an estimation of the parameter distribution using as little as one percent of the model evaluations required by a regular MCMC approach. The effectiveness and efficiency of the ASMO-PODE approach have been evaluated with 2 test problems and one land surface model, the Common Land Model. The results demonstrated that the ASMO-PODE method is an economic way for parameter optimization and distribution estimation.},
  journal = {Environmental Modelling \& Software},
  doi = {10.1016/j.envsoft.2017.05.005},
  author = {Gong, Wei and Duan, Qingyun},
  month = sep,
  year = {2017},
  keywords = {MCMC,Adaptive sampling,Gaussian processes regression,Surrogate model},
  pages = {61-75},
  file = {/home/victor/Zotero/storage/XLDRYL8N/Gong et Duan - 2017 - An adaptive surrogate modeling-based sampling stra.pdf;/home/victor/Zotero/storage/GLLWW75R/S1364815216310830.html}
}

@article{angelikopoulos_x-tmcmc_2015,
  title = {X-{{TMCMC}}: {{Adaptive}} Kriging for {{Bayesian}} Inverse Modeling},
  volume = {289},
  issn = {00457825},
  shorttitle = {X-{{TMCMC}}},
  language = {en},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  doi = {10.1016/j.cma.2015.01.015},
  author = {Angelikopoulos, Panagiotis and Papadimitriou, Costas and Koumoutsakos, Petros},
  month = jun,
  year = {2015},
  keywords = {Bayesian inference,Kriging,Adaptative Kriging},
  pages = {409-428},
  file = {/home/victor/Zotero/storage/YZ3W8HHH/angelikopoulos2015a.pdf}
}

@article{laloy_efficient_2013,
  title = {Efficient Posterior Exploration of a High-Dimensional Groundwater Model from Two-Stage {{Markov}} Chain {{Monte Carlo}} Simulation and Polynomial Chaos Expansion},
  volume = {49},
  issn = {1944-7973},
  abstract = {This study reports on two strategies for accelerating posterior inference of a highly parameterized and CPU-demanding groundwater flow model. Our method builds on previous stochastic collocation approaches, e.g., Marzouk and Xiu (2009) and Marzouk and Najm (2009), and uses generalized polynomial chaos (gPC) theory and dimensionality reduction to emulate the output of a large-scale groundwater flow model. The resulting surrogate model is CPU efficient and serves to explore the posterior distribution at a much lower computational cost using two-stage MCMC simulation. The case study reported in this paper demonstrates a two to five times speed-up in sampling efficiency.},
  language = {en},
  number = {5},
  journal = {Water Resources Research},
  doi = {10.1002/wrcr.20226},
  author = {Laloy, Eric and Rogiers, Bart and Vrugt, Jasper A. and Mallants, Dirk and Jacques, Diederik},
  month = may,
  year = {2013},
  keywords = {MCMC,PCE},
  pages = {2664-2682},
  file = {/home/victor/Zotero/storage/TNSMADNB/Laloy et al. - 2013 - Efficient posterior exploration of a high-dimensio.pdf;/home/victor/Zotero/storage/7282JYD5/abstract.html}
}

@article{habibi_exact_2011,
  title = {Exact Distribution of Argmax (Argmin)},
  volume = {26},
  number = {2},
  journal = {Economic Quality Control},
  author = {Habibi, Reza},
  year = {2011},
  keywords = {argmax distribution},
  pages = {155--162},
  file = {/home/victor/Zotero/storage/FCTZLU2W/EQC.2011.015.pdf}
}

@article{liu_variational_2013,
  title = {Variational Algorithms for Marginal {{MAP}}},
  volume = {14},
  number = {1},
  journal = {The Journal of Machine Learning Research},
  author = {Liu, Qiang and Ihler, Alexander},
  year = {2013},
  keywords = {MMAP,EM algorithm},
  pages = {3165--3200},
  file = {/home/victor/Zotero/storage/USN5RNR6/liu13b.pdf}
}

@inproceedings{robert_marginal_1999,
  title = {Marginal {{MAP}} Estimation Using {{Markov}} Chain {{Monte Carlo}}},
  volume = {3},
  booktitle = {Acoustics, {{Speech}}, and {{Signal Processing}}, 1999. {{Proceedings}}., 1999 {{IEEE International Conference}} On},
  publisher = {{IEEE}},
  author = {Robert, Christian P. and Doucet, Arnaud and Godsill, Simon J.},
  year = {1999},
  keywords = {MCMC,MMAP},
  pages = {1753--1756},
  file = {/home/victor/Zotero/storage/H5FADK4V/Robert et al. - 1999 - Marginal MAP estimation using Markov chain Monte C.pdf;/home/victor/Zotero/storage/AII4L2FS/756334.html}
}

@article{borman_expectation_2004,
  title = {The Expectation Maximization Algorithm-a Short Tutorial},
  journal = {Submitted for publication},
  author = {Borman, Sean},
  year = {2004},
  keywords = {EM algorithm},
  pages = {1--9},
  file = {/home/victor/Zotero/storage/HGJ62YFV/EM_algorithm.pdf}
}

@article{stuart_inverse_2010,
  title = {Inverse Problems: {{A Bayesian}} Perspective},
  volume = {19},
  issn = {0962-4929, 1474-0508},
  shorttitle = {Inverse Problems},
  language = {en},
  journal = {Acta Numerica},
  doi = {10.1017/S0962492910000061},
  author = {Stuart, A. M.},
  month = may,
  year = {2010},
  keywords = {Bayesian inference,Inverse problems},
  pages = {451-559},
  file = {/home/victor/Zotero/storage/EAVUSLYB/stuart15c.pdf}
}

@article{rockafellar_conditional_2002,
  title = {Conditional Value-at-Risk for General Loss Distributions},
  volume = {26},
  number = {7},
  journal = {Journal of banking \& finance},
  author = {Rockafellar, R. Tyrrell and Uryasev, Stanislav},
  year = {2002},
  keywords = {VaR},
  pages = {1443--1471},
  file = {/home/victor/Zotero/storage/RPR7LHY8/790b149edfb586db318363e28182a6fedc80.pdf}
}

@article{krokhmal_modeling_2011,
  title = {Modeling and Optimization of Risk},
  volume = {16},
  issn = {1876-7354},
  abstract = {This paper surveys the most recent advances in the context of decision making under uncertainty, with an emphasis on the modeling of risk-averse preferences using the apparatus of axiomatically defined risk functionals, such as coherent measures of risk and deviation measures, and their connection to utility theory, stochastic dominance, and other more established methods.},
  number = {2},
  journal = {Surveys in Operations Research and Management Science},
  doi = {10.1016/j.sorms.2010.08.001},
  author = {Krokhmal, Pavlo and Zabarankin, Michael and Uryasev, Stan},
  month = jul,
  year = {2011},
  pages = {49-66},
  file = {/home/victor/Zotero/storage/46S2PMER/1-s2.0-S187673541000005X-main.pdf;/home/victor/Zotero/storage/KLYHLRYI/S187673541000005X.html}
}

@article{wu_inverse_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1801.01782},
  primaryClass = {stat},
  title = {Inverse {{Uncertainty Quantification}} Using the {{Modular Bayesian Approach}} Based on {{Gaussian Process}}, {{Part}} 1: {{Theory}}},
  shorttitle = {Inverse {{Uncertainty Quantification}} Using the {{Modular Bayesian Approach}} Based on {{Gaussian Process}}, {{Part}} 1},
  abstract = {In nuclear reactor system design and safety analysis, the Best Estimate plus Uncertainty (BEPU) methodology requires that computer model output uncertainties must be quantified in order to prove that the investigated design stays within acceptance criteria. "Expert opinion" and "user self-evaluation" have been widely used to specify computer model input uncertainties in previous uncertainty, sensitivity and validation studies. Inverse Uncertainty Quantification (UQ) is the process to inversely quantify input uncertainties based on experimental data in order to more precisely quantify such ad-hoc specifications of the input uncertainty information. In this paper, we used Bayesian analysis to establish the inverse UQ formulation, with systematic and rigorously derived metamodels constructed by Gaussian Process (GP). Due to incomplete or inaccurate underlying physics, as well as numerical approximation errors, computer models always have discrepancy/bias in representing the realities, which can cause over-fitting if neglected in the inverse UQ process. The model discrepancy term is accounted for in our formulation through the "model updating equation". We provided a detailed introduction and comparison of the full and modular Bayesian approaches for inverse UQ, as well as pointed out their limitations when extrapolated to the validation/prediction domain. Finally, we proposed an improved modular Bayesian approach that can avoid extrapolating the model discrepancy that is learnt from the inverse UQ domain to the validation/prediction domain.},
  journal = {arXiv:1801.01782 [stat]},
  author = {Wu, Xu and Kozlowski, Tomasz and Meidani, Hadi and Shirvan, Koroush},
  month = jan,
  year = {2018},
  keywords = {inversion modeling,Statistics - Computation,UQ,Inverse UQ},
  file = {/home/victor/Zotero/storage/SSSI56TQ/Wu et al. - 2018 - Inverse Uncertainty Quantification using the Modul.pdf;/home/victor/Zotero/storage/ZKX8968Q/1801.html}
}

@article{han_simultaneous_2009,
  title = {Simultaneous {{Determination}} of {{Tuning}} and {{Calibration Parameters}} for {{Computer Experiments}}},
  volume = {51},
  issn = {0040-1706},
  abstract = {Tuning and calibration are processes for improving the representativeness of a computer simulation code to a physical phenomenon. This article introduces a statistical methodology for simultaneously determining tuning and calibration parameters in settings where data are available from a computer code and the associated physical experiment. Tuning parameters are set by minimizing a discrepancy measure while the distribution of the calibration parameters are determined based on a hierarchical Bayesian model. The proposed Bayesian model views the output as a realization of a Gaussian stochastic process with hyperpriors. Draws from the resulting posterior distribution are obtained by the Markov chain Monte Carlo simulation. Our methodology is compared with an alternative approach in examples and is illustrated in a biomechanical engineering application. Supplemental materials, including the software and a user manual, are available online and can be requested from the first author.},
  number = {4},
  journal = {Technometrics : a journal of statistics for the physical, chemical, and engineering sciences},
  doi = {10.1198/TECH.2009.08126},
  author = {Han, Gang and Santner, Thomas J. and Rawlinson, Jeremy J.},
  month = nov,
  year = {2009},
  keywords = {Calibration},
  pages = {464-474},
  file = {/home/victor/Zotero/storage/WHDGXHCE/Han et al. - 2009 - Simultaneous Determination of Tuning and Calibrati.pdf},
  pmid = {20523754},
  pmcid = {PMC2879656}
}

@article{friel_estimating_2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1111.1957},
  primaryClass = {stat},
  title = {Estimating the Evidence -- a Review},
  abstract = {The model evidence is a vital quantity in the comparison of statistical models under the Bayesian paradigm. This paper presents a review of commonly used methods. We outline some guidelines and offer some practical advice. The reviewed methods are compared for two examples; non-nested Gaussian linear regression and covariate subset selection in logistic regression.},
  journal = {arXiv:1111.1957 [stat]},
  author = {Friel, Nial and Wyse, Jason},
  month = nov,
  year = {2011},
  keywords = {Evidence estimation},
  file = {/home/victor/Zotero/storage/W9R587RW/Friel et Wyse - 2011 - Estimating the evidence -- a review.pdf;/home/victor/Zotero/storage/ZV9QA6PC/1111.html}
}

@article{markowitz_portfolio_1952,
  title = {Portfolio Selection},
  volume = {7},
  number = {1},
  journal = {The journal of finance},
  author = {Markowitz, Harry},
  year = {1952},
  keywords = {Portfolio selection,Bias Variance tradeoff},
  pages = {77--91},
  file = {/home/victor/Zotero/storage/HWSVRRWD/Markowitz - 1952 - Portfolio selection;/home/victor/Zotero/storage/T6TEETNQ/Markowitz - 1952 - Portfolio selection.pdf}
}

@article{kim_tractable_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.08582},
  primaryClass = {stat},
  title = {Tractable {{Fully Bayesian Inference}} via {{Convex Optimization}} and {{Optimal Transport Theory}}},
  abstract = {We consider the problem of transforming samples from one continuous source distribution into samples from another target distribution. We demonstrate with optimal transport theory that when the source distribution can be easily sampled from and the target distribution is log-concave, this can be tractably solved with convex optimization. We show that a special case of this, when the source is the prior and the target is the posterior, is Bayesian inference. Here, we can tractably calculate the normalization constant and draw posterior i.i.d. samples. Remarkably, our Bayesian tractability criterion is simply log concavity of the prior and likelihood: the same criterion for tractable calculation of the maximum a posteriori point estimate. With simulated data, we demonstrate how we can attain the Bayes risk in simulations. With physiologic data, we demonstrate improvements over point estimation in intensive care unit outcome prediction and electroencephalography-based sleep staging.},
  journal = {arXiv:1509.08582 [stat]},
  author = {Kim, Sanggyun and Mesa, Diego and Ma, Rui and Coleman, Todd P.},
  month = sep,
  year = {2015},
  keywords = {Statistics - Machine Learning,Bayesian inference},
  file = {/home/victor/Zotero/storage/UV4VLRVW/Kim et al. - 2015 - Tractable Fully Bayesian Inference via Convex Opti.pdf;/home/victor/Zotero/storage/CWFHYF8C/1509.html}
}

@article{walker_defining_2003,
  title = {Defining Uncertainty: A Conceptual Basis for Uncertainty Management in Model-Based Decision Support},
  volume = {4},
  shorttitle = {Defining Uncertainty},
  number = {1},
  journal = {Integrated assessment},
  author = {Walker, Warren E. and Harremo{\"e}s, Poul and Rotmans, Jan and {van der Sluijs}, Jeroen P. and {van Asselt}, Marjolein BA and Janssen, Peter and {Krayer von Krauss}, Martin P.},
  year = {2003},
  keywords = {Uncertainty analysis,UQ},
  pages = {5--17},
  file = {/home/victor/Zotero/storage/E6Q2GRNH/79.pdf}
}

@book{armagan_bayesian_2010,
  title = {Bayesian Generalized Double {{Pareto}} Shrinkage},
  abstract = {We propose a generalized double Pareto prior for shrinkage estimation in linear models. The prior can be obtained via a scale mixture of Laplace or normal distributions, while forming a bridge between the Laplace and Normal-Jeffreys ' priors. While it has a spike at zero like the Laplace density, it also has a Student-t-like tail behavior. We show strong consistency of the posterior in regression models with a diverging number of parameters, providing a template to be used for other priors in similar settings. Bayesian computation is straightforward via a simple Gibbs sampling algorithm. We also investigate the properties of the maximum a posteriori estimator and reveal connections with some well-established regularization procedures. The performance of the new prior is tested through simulations.},
  author = {Armagan, A. and {al}, et},
  year = {2010},
  keywords = {Bayesian inference},
  file = {/home/victor/Zotero/storage/W6N3EVXB/Armagan et al - 2010 - Bayesian generalized double Pareto shrinkage.pdf;/home/victor/Zotero/storage/2UH922I9/summary.html}
}

@article{doucet_marginal_2002,
  title = {Marginal Maximum a Posteriori Estimation Using {{Markov}} Chain {{Monte Carlo}}},
  volume = {12},
  issn = {0960-3174, 1573-1375},
  abstract = {Markov chain Monte Carlo (MCMC) methods, while facilitating the solution of many complex problems in Bayesian inference, are not currently well adapted to the problem of marginal maximum a posteriori (MMAP) estimation, especially when the number of parameters is large. We present here a simple and novel MCMC strategy, called State-Augmentation for Marginal Estimation (SAME), which leads to MMAP estimates for Bayesian models. We illustrate the simplicity and utility of the approach for missing data interpolation in autoregressive time series and blind deconvolution of impulsive processes.},
  language = {en},
  number = {1},
  journal = {Statistics and Computing},
  doi = {10.1023/A:1013172322619},
  author = {Doucet, Arnaud and Godsill, Simon J. and Robert, Christian P.},
  month = jan,
  year = {2002},
  keywords = {MCMC,MMAP,SAME},
  pages = {77-84},
  file = {/home/victor/Zotero/storage/8DPAVYIP/Doucet et al. - 2002 - Marginal maximum a posteriori estimation using Mar.pdf;/home/victor/Zotero/storage/LB5L4CSS/A1013172322619.html}
}

@misc{malmberg_argmax_,
  title = {Argmax over {{Continuous Indices}} of {{Random Variables}} : {{An Approach Us}}- Ing {{Random Fields}}},
  author = {Malmberg, Hannes},
  keywords = {argmax measure},
  file = {/home/victor/Zotero/storage/QD8FUL8W/report.pdf}
}

@article{seijo_continuous_2011,
  title = {A Continuous Mapping Theorem for the Smallest Argmax Functional},
  volume = {5},
  issn = {1935-7524},
  abstract = {This paper introduces a version of the argmax continuous mapping theorem that applies to M-estimation problems in which the objective functions converge to a limiting process with multiple maximizers. The concept of the smallest maximizer of a function in the d-dimensional Skorohod space is introduced and its main properties are studied. The resulting continuous mapping theorem is applied to three problems arising in change-point regression analysis. Some of the results proved in connection to the d-dimensional Skorohod space are also of independent interest.},
  language = {EN},
  journal = {Electronic Journal of Statistics},
  doi = {10.1214/11-EJS613},
  author = {Seijo, Emilio and Sen, Bodhisattva},
  year = {2011},
  keywords = {Change-point,compound Poisson process,Cox proportional hazards model,multiple maximizers,Skorohod spaces with multidimensional parameter},
  pages = {421-439},
  file = {/home/victor/Zotero/storage/WZANPFUU/Seijo et Sen - 2011 - A continuous mapping theorem for the smallest argm.pdf;/home/victor/Zotero/storage/AHU5JSRH/1305034909.html}
}

@techreport{hanson_markov_2001,
  title = {{{MARKOV CHAIN MONTE CARLO POSTERIOR SAMPLING WITH THE HAMILTONIAN METHOD}}},
  abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
  language = {English},
  number = {LA-UR-01-1016},
  institution = {{Los Alamos National Lab., NM (US)}},
  author = {Hanson, K.},
  month = feb,
  year = {2001},
  keywords = {MCMC,Hamiltonian MCMC},
  file = {/home/victor/Zotero/storage/H3Y9Y8VD/Hanson - 2001 - MARKOV CHAIN MONTE CARLO POSTERIOR SAMPLING WITH T.pdf;/home/victor/Zotero/storage/AMUBRP74/775292.html}
}

@inproceedings{murphy_loopy_1999,
  title = {Loopy Belief Propagation for Approximate Inference: {{An}} Empirical Study},
  shorttitle = {Loopy Belief Propagation for Approximate Inference},
  booktitle = {Proceedings of the {{Fifteenth}} Conference on {{Uncertainty}} in Artificial Intelligence},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  author = {Murphy, Kevin P. and Weiss, Yair and Jordan, Michael I.},
  year = {1999},
  pages = {467--475},
  file = {/home/victor/Zotero/storage/4YGEXIA8/qt92p8w3xb.pdf}
}

@article{brookes_comparison_,
  title = {A Comparison of {{Fuzzy}}, {{Bayesian}} and {{Weighted Average}} Formulations of an in-Stream Habitat Suitability Model.},
  abstract = {Three variations of a simple in-stream habitat suitability model were implemented and the effect on output for one organism at 22 sites on one short section of a river was examined. The model uses only two factors, depth and velocity, to calculate quality and a third factor, width to quantify utility. The implementations were based on 3 multi-criteria evaluation approaches: Weighted Average, Fuzzy Sets and Bayesian probability. There was broad agreement between the formulations but important differences in detail. The model outputs indicate that uncertainty arising from model formulation is significant and can have a bearing on planning decisions. There is a complex interaction between the formulations and the characteristics of the sites. Suitability models should be used thoughtfully and implemented in ways that: facilitate exploratory analysis; present ranges of possible outputs; present indicators of uncertainty; and facilitate back tracking to explain the outputs.},
  author = {Brookes, C J and Kumar, Vikas and Lane, S N},
  keywords = {Bayesian,Fuzzy},
  pages = {8},
  file = {/home/victor/Zotero/storage/TEAUII5N/Brookes et al. - A comparison of Fuzzy, Bayesian and Weighted Avera.pdf}
}

@unpublished{nguyen_nonparametric_2018,
  title = {Nonparametric Method for Sparse Conditional Density Estimation in Moderately Large Dimensions},
  abstract = {In this paper, we consider the problem of estimating a conditional density in moderately large dimensions. Much more informative than regression functions, conditional densities are of main interest in recent methods, particularly in the Bayesian framework (studying the posterior distribution, finding its modes...). Considering a recently studied family of kernel estimators, we select a pointwise multivariate bandwidth by revisiting the greedy algorithm Rodeo (Regularisation Of Derivative Expectation Operator). The method addresses several issues: being greedy and computationally efficient by an iterative procedure, avoiding the curse of high dimensionality under some suitably defined sparsity conditions by early variable selection during the procedure, converging at a quasi-optimal minimax rate.},
  author = {Nguyen, Minh-Lien Jeanne},
  month = jan,
  year = {2018},
  keywords = {greedy algorithm,conditional density,kernel density estimators,nonparametric inference,Density estimation},
  file = {/home/victor/Zotero/storage/4RZ5J4QG/Nguyen - 2018 - Nonparametric method for sparse conditional densit.pdf}
}

@article{nagler_evading_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1503.03305},
  title = {Evading the Curse of Dimensionality in Nonparametric Density Estimation with Simplified Vine Copulas},
  volume = {151},
  issn = {0047259X},
  abstract = {Practical applications of nonparametric density estimators in more than three dimensions suffer a great deal from the well-known curse of dimensionality: convergence slows down as dimension increases. We show that one can evade the curse of dimensionality by assuming a simplified vine copula model for the dependence between variables. We formulate a general nonparametric estimator for such a model and show under high-level assumptions that the speed of convergence is independent of dimension. We further discuss a particular implementation for which we validate the high-level assumptions and establish its asymptotic normality. Simulation experiments illustrate a large gain in finite sample performance when the simplifying assumption is at least approximately true. But even when it is severely violated, the vine copula based approach proves advantageous as soon as more than a few variables are involved. Lastly, we give an application of the estimator to a classification problem from astrophysics.},
  journal = {Journal of Multivariate Analysis},
  doi = {10.1016/j.jmva.2016.07.003},
  author = {Nagler, Thomas and Czado, Claudia},
  month = oct,
  year = {2016},
  keywords = {Density estimation},
  pages = {69-89},
  file = {/home/victor/Zotero/storage/T63XS699/Nagler et Czado - 2016 - Evading the curse of dimensionality in nonparametr.pdf;/home/victor/Zotero/storage/NMMKIWIW/1503.html}
}

@article{bouezmarni_nonparametric_2010,
  title = {Nonparametric Density Estimation for Multivariate Bounded Data},
  volume = {140},
  issn = {0378-3758},
  abstract = {We propose a new nonparametric estimator for the density function of multivariate bounded data. As frequently observed in practice, the variables may be partially bounded (e.g. nonnegative) or completely bounded (e.g. in the unit interval). In addition, the variables may have a point mass. We reduce the conditions on the underlying density to a minimum by proposing a nonparametric approach. By using a gamma, a beta, or a local linear kernel (also called boundary kernels), in a product kernel, the suggested estimator becomes simple in implementation and robust to the well known boundary bias problem. We investigate the mean integrated squared error properties, including the rate of convergence, uniform strong consistency and asymptotic normality. We establish consistency of the least squares cross-validation method to select optimal bandwidth parameters. A detailed simulation study investigates the performance of the estimators. Applications using lottery and corporate finance data are provided.},
  number = {1},
  journal = {Journal of Statistical Planning and Inference},
  doi = {10.1016/j.jspi.2009.07.013},
  author = {Bouezmarni, Taoufik and Rombouts, Jeroen V. K.},
  month = jan,
  year = {2010},
  keywords = {Asymmetric kernels,Asymptotic properties,Bandwidth selection,Least squares cross-validation,Multivariate boundary bias,Nonparametric multivariate density estimation,Density estimation},
  pages = {139-152},
  file = {/home/victor/Zotero/storage/3Z77C6KS/Bouezmarni et Rombouts - 2010 - Nonparametric density estimation for multivariate .pdf;/home/victor/Zotero/storage/UANRQMPP/S0378375809002249.html}
}

@article{chen_beta_1999,
  title = {Beta Kernel Estimators for Density Functions},
  volume = {31},
  issn = {0167-9473},
  abstract = {Kernel estimators using non-negative kernels are considered to estimate probability density functions with compact supports. The kernels are chosen from a family of beta densities. The beta kernel estimators are free of boundary bias, non-negative and achieve the optimal rate of convergence for the mean integrated squared error. The proposed beta kernel estimators have two features. One is that the different amount of smoothing is allocated by naturally varying kernel shape without explicitly changing the value of the smoothing bandwidth. Another feature is that the support of the beta kernels can match the support of the density function; this leads to larger effective sample sizes used in the density estimation and can produce density estimates that have smaller finite-sample variance than some other estimators.},
  number = {2},
  journal = {Computational Statistics \& Data Analysis},
  doi = {10.1016/S0167-9473(99)00010-9},
  author = {Chen, Song Xi},
  month = aug,
  year = {1999},
  keywords = {Beta kernels,Boundary bias,Density estimation,Local linear estimators,Variable kernels},
  pages = {131-145},
  file = {/home/victor/Zotero/storage/7GIVZIWE/Chen - 1999 - Beta kernel estimators for density functions.pdf;/home/victor/Zotero/storage/EHYXW4JH/S0167947399000109.html}
}

@article{jones_kernel-type_,
  title = {Kernel-Type Density Estimation on the Unit Interval},
  abstract = {We consider kernel-type methods for estimation of a density on [0, 1] which eschew explicit boundary correction. Our starting point is the successful implementation of beta kernel density estimators of Chen (1999). We propose and investigate two alternatives. For the first, we reverse the roles of estimation point x and datapoint Xi in each summand of the estimator. For the second, we provide kernels that are symmetric in x and X; these kernels are conditional densities of bivariate copulas. We develop asymptotic theory for the new estimators and compare them with Chen's in a substantial simulation study. We also develop automatic bandwidth selection in the form of `rule-of-thumb' bandwidths for all three estimators. We find that our second proposal, that based on `copula kernels', seems particularly competitive with the beta kernel method of Chen in integrated squared error performance terms. Advantages include its greater range of possible values at 0 and 1, the fact that it is a bona fide density, and that the individual kernels and resulting estimator are comprehensible in terms of a direct single picture (as is ordinary kernel density estimation on the line).},
  author = {JONES, M C and HENDERSON, D A},
  keywords = {Beta kernels,Density estimation},
  pages = {35},
  file = {/home/victor/Zotero/storage/YSHMWFLM/JONES et HENDERSON - Kernel-type density estimation on the unit interva.pdf}
}

@phdthesis{kumar_sequential_2008,
  title = {Sequential {{Calibration Of Computer Models}}},
  language = {en},
  school = {The Ohio State University},
  author = {Kumar, Arun},
  year = {2008},
  file = {/home/victor/Zotero/storage/RLQ6FWEZ/Kumar - 2008 - Sequential Calibration Of Computer Models.pdf;/home/victor/Zotero/storage/5YQ8FL22/pg_10.html}
}

@article{bickel_fast_2005,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {math/0505419},
  title = {On a {{Fast}}, {{Robust Estimator}} of the {{Mode}}: {{Comparisons}} to {{Other Robust Estimators}} with {{Applications}}},
  shorttitle = {On a {{Fast}}, {{Robust Estimator}} of the {{Mode}}},
  abstract = {Advances in computing power enable more widespread use of the mode, which is a natural measure of central tendency since, as the most probable value, it is not influenced by the tails in the distribution. The properties of the half-sample mode, which is a simple and fast estimator of the mode of a continuous distribution, are studied. The half-sample mode is less sensitive to outliers than most other estimators of location, including many other low-bias estimators of the mode. Its breakdown point is one half, equal to that of the median. However, because of its finite rejection point, the half-sample mode is much less sensitive to outliers that are all either greater or less than the other values of the sample. This is confirmed by applying the mode estimator and the median to samples drawn from normal, lognormal, and Pareto distributions contaminated by outliers. It is also shown that the half-sample mode, in combination with a robust scale estimator, is a highly robust starting point for iterative robust location estimators such as Huber's M-estimator. The half-sample mode can easily be generalized to modal intervals containing more or less than half of the sample. An application of such an estimator to the finding of collision points in high-energy proton-proton interactions is presented.},
  journal = {arXiv:math/0505419},
  author = {Bickel, David R. and Fruehwirth, Rudolf},
  month = may,
  year = {2005},
  keywords = {Mode estimation},
  file = {/home/victor/Zotero/storage/QZFLB585/Bickel et Fruehwirth - 2005 - On a Fast, Robust Estimator of the Mode Compariso.pdf;/home/victor/Zotero/storage/M5EJDIBY/0505419.html}
}

@article{dobrovidov_data-driven_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1401.6801},
  primaryClass = {math, stat},
  title = {Data-Driven Bandwidth Choice for Gamma Kernel Estimates of Density Derivatives on the Positive Semi-Axis},
  abstract = {In some applications it is necessary to estimate derivatives of probability densities defined on the positive semi-axis. The quality of nonparametric estimates of the probability densities and their derivatives are strongly influenced by smoothing parameters (bandwidths). In this paper an expression for the optimal smoothing parameter of the gamma kernel estimate of the density derivative is obtained. For this parameter data-driven estimates based on methods called "rule of thumb" and "cross-validation" are constructed. The quality of the estimates is verified and demonstrated on examples of density derivatives generated by Maxwell and Weibull distributions.},
  journal = {arXiv:1401.6801 [math, stat]},
  author = {Dobrovidov, A. V. and Markovich, L. A.},
  month = jan,
  year = {2014},
  keywords = {Bandwidth selection,Density estimation,Gamma kernel},
  file = {/home/victor/Zotero/storage/APEW2FA4/Dobrovidov et Markovich - 2014 - Data-driven bandwidth choice for gamma kernel esti.pdf;/home/victor/Zotero/storage/SDV5GMA9/1401.html}
}

@article{afshar_probabilistic_,
  title = {Probabilistic {{Inference}} in {{Piecewise Graphical Models}}},
  author = {Afshar, Hadi Mohasel},
  pages = {165},
  file = {/home/victor/Zotero/storage/PR9NVEI4/Afshar - Probabilistic Inference in Piecewise Graphical Mod.pdf}
}

@article{dunlop_map_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.03136},
  title = {{{MAP Estimators}} for {{Piecewise Continuous Inversion}}},
  volume = {32},
  issn = {0266-5611, 1361-6420},
  abstract = {We study the inverse problem of estimating a field \$u\$ from data comprising a finite set of nonlinear functionals of \$u\$, subject to additive noise; we denote this observed data by \$y\$. Our interest is in the reconstruction of piecewise continuous fields in which the discontinuity set is described by a finite number of geometric parameters. Natural applications include groundwater flow and electrical impedance tomography. We take a Bayesian approach, placing a prior distribution on \$u\$ and determining the conditional distribution on \$u\$ given the data \$y\$. It is then natural to study maximum a posterior (MAP) estimators. Recently (Dashti et al 2013) it has been shown that MAP estimators can be characterised as minimisers of a generalised Onsager-Machlup functional, in the case where the prior measure is a Gaussian random field. We extend this theory to a more general class of prior distributions which allows for piecewise continuous fields. Specifically, the prior field is assumed to be piecewise Gaussian with random interfaces between the different Gaussians defined by a finite number of parameters. We also make connections with recent work on MAP estimators for linear problems and possibly non-Gaussian priors (Helin, Burger 2015) which employs the notion of Fomin derivative. In showing applicability of our theory we focus on the groundwater flow and EIT models, though the theory holds more generally. Numerical experiments are implemented for the groundwater flow model, demonstrating the feasibility of determining MAP estimators for these piecewise continuous models, but also that the geometric formulation can lead to multiple nearby (local) MAP estimators. We relate these MAP estimators to the behaviour of output from MCMC samples of the posterior, obtained using a state-of-the-art function space Metropolis-Hastings method.},
  number = {10},
  journal = {Inverse Problems},
  doi = {10.1088/0266-5611/32/10/105003},
  author = {Dunlop, Matthew M. and Stuart, Andrew M.},
  month = oct,
  year = {2016},
  keywords = {MAP},
  pages = {105003},
  file = {/home/victor/Zotero/storage/IHB7KHYR/Dunlop et Stuart - 2016 - MAP Estimators for Piecewise Continuous Inversion.pdf;/home/victor/Zotero/storage/7CHTAAK8/1509.html}
}

@article{taddy_fast_2008,
  title = {Fast {{Bayesian Inference}} for {{Computer Simulation Inverse Problems}}},
  journal = {submitted to Inverse Problems},
  author = {Taddy, Matthew and Lee, Herbert KH and Sans{\'o}, Bruno},
  year = {2008},
  file = {/home/victor/Zotero/storage/BUF9L83Q/Taddy et al. - 2008 - Fast Bayesian Inference for Computer Simulation In.pdf;/home/victor/Zotero/storage/EM6S6634/Taddy et al. - 2008 - Fast Bayesian Inference for Computer Simulation In.pdf;/home/victor/Zotero/storage/UUJJQSNS/Fast Bayesian Inference for Computer Simulation Inverse Problems.pdf}
}

@article{hutter_bayesian_2006,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {math/0606315},
  title = {Bayesian {{Regression}} of {{Piecewise Constant Functions}}},
  abstract = {We derive an exact and efficient Bayesian regression algorithm for piecewise constant functions of unknown segment number, boundary location, and levels. It works for any noise and segment level prior, e.g. Cauchy which can handle outliers. We derive simple but good estimates for the in-segment variance. We also propose a Bayesian regression curve as a better way of smoothing data without blurring boundaries. The Bayesian approach also allows straightforward determination of the evidence, break probabilities and error estimates, useful for model selection and significance and robustness studies. We discuss the performance on synthetic and real-world examples. Many possible extensions will be discussed.},
  journal = {arXiv:math/0606315},
  author = {Hutter, Marcus},
  month = jun,
  year = {2006},
  keywords = {Computer Science - Learning,Mathematics - Probability,Mathematics - Statistics Theory},
  file = {/home/victor/Zotero/storage/E9LCVMVB/Hutter - 2006 - Bayesian Regression of Piecewise Constant Function.pdf;/home/victor/Zotero/storage/G94863ZU/0606315.html}
}

@article{berger_formal_2009,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0904.0156},
  title = {The Formal Definition of Reference Priors},
  volume = {37},
  issn = {0090-5364},
  abstract = {Reference analysis produces objective Bayesian inference, in the sense that inferential statements depend only on the assumed model and the available data, and the prior distribution used to make an inference is least informative in a certain information-theoretic sense. Reference priors have been rigorously defined in specific contexts and heuristically defined in general, but a rigorous general definition has been lacking. We produce a rigorous general definition here and then show how an explicit expression for the reference prior can be obtained under very weak regularity conditions. The explicit expression can be used to derive new reference priors both analytically and numerically.},
  number = {2},
  journal = {The Annals of Statistics},
  doi = {10.1214/07-AOS587},
  author = {Berger, James O. and Bernardo, Jos{\'e} M. and Sun, Dongchu},
  month = apr,
  year = {2009},
  keywords = {Mathematics - Statistics Theory,62F15 (Primary) 62A01; 62B10 (Secondary)},
  pages = {905-938},
  file = {/home/victor/Zotero/storage/WJGEVP2V/Berger et al. - 2009 - The formal definition of reference priors.pdf;/home/victor/Zotero/storage/YC35QS73/0904.html}
}

@article{bickel_robust_2003,
  title = {Robust and Efficient Estimation of the Mode of Continuous Data: The Mode as a Viable Measure of Central Tendency},
  volume = {73},
  issn = {0094-9655, 1563-5163},
  shorttitle = {Robust and Efficient Estimation of the Mode of Continuous Data},
  abstract = {Although a natural measure of the central tendency of a sample of continuous data is its mode (the most probable value), the mean and median are the most popular measures of location due to their simplicity and ease of estimation. The median is often used instead of the mean for asymmetric data because it is closer to the mode and is insensitive to extreme values in the sample. However, the mode itself can be reliably estimated by first transforming the data into approximately normal data by raising the values to a real power, and then estimating the mean and standard deviation of the transformed data. With this method, two estimators of the mode of the original data are proposed: a simple estimator based on estimating the mean by the sample mean and the standard deviation by the sample standard deviation, and a more robust estimator based on estimating the mean by the median and the standard deviation by the standardized median absolute deviation.},
  language = {en},
  number = {12},
  journal = {Journal of Statistical Computation and Simulation},
  doi = {10.1080/0094965031000097809},
  author = {Bickel, David R.},
  month = dec,
  year = {2003},
  pages = {899-912},
  file = {/home/victor/Zotero/storage/54JEYDX5/Bickel - 2003 - Robust and efficient estimation of the mode of con.pdf}
}

@book{manski_structural_1981,
  title = {Structural Analysis of Discrete Data with Econometric Applications},
  publisher = {{Mit Press Cambridge, MA}},
  author = {Manski, Charles F. and McFadden, Daniel},
  year = {1981}
}

@article{jacquier_maximum_,
  title = {Maximum {{Expected Utility}} via {{MCMC}}},
  abstract = {In this paper we provide a new simulation-based approach to maximum expected utility (MEU) portfolio allocation problems. MEU requires computation of expected utility and its optimization over the decision variable. In portfolio problems, the expected utility is generally not analytically available. Traditional methods resort to gradient-based estimates of expected utility with its ensuing derivatives. This leads to computational inefficiencies which are particularly acute in portfolio problems with parameter uncertainty (a.k.a. estimation risk). Our simulation-based method avoids the calculation of derivative and also allows for functional optimization. The algorithm combines Markov Chain Monte Carlo (MCMC) with the insights of simulated annealing and evolutionary Monte Carlo. It can exploit conjugate utility functions and latent variables in the relevant predictive density for efficient simulation. We also show how slice sampling naturally allows for constraints in the portfolio weights. We illustrate our methodology with a portfolio problem with estimation risk and CARA utility.},
  language = {en},
  author = {Jacquier, Eric and Johannes, Michael and Polson, Nicholas},
  pages = {26},
  file = {/home/victor/Zotero/storage/2DXK3GLL/Jacquier et al. - Maximum Expected Utility via MCMC.pdf}
}

@article{nagel_bayesian_,
  title = {Bayesian Techniques for Inverse Uncertainty Quantification},
  author = {Nagel, Joseph Benjamin},
  pages = {211},
  file = {/home/victor/Zotero/storage/93SNZ7CK/Nagel - Bayesian techniques for inverse uncertainty quanti.pdf}
}

@article{nagel_spectral_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.07564},
  title = {Spectral Likelihood Expansions for {{Bayesian}} Inference},
  volume = {309},
  issn = {00219991},
  abstract = {A spectral approach to Bayesian inference is presented. It pursues the emulation of the posterior probability density. The starting point is a series expansion of the likelihood function in terms of orthogonal polynomials. From this spectral likelihood expansion all statistical quantities of interest can be calculated semi-analytically. The posterior is formally represented as the product of a reference density and a linear combination of polynomial basis functions. Both the model evidence and the posterior moments are related to the expansion coefficients. This formulation avoids Markov chain Monte Carlo simulation and allows one to make use of linear least squares instead. The pros and cons of spectral Bayesian inference are discussed and demonstrated on the basis of simple applications from classical statistics and inverse modeling.},
  journal = {Journal of Computational Physics},
  doi = {10.1016/j.jcp.2015.12.047},
  author = {Nagel, Joseph B. and Sudret, Bruno},
  month = mar,
  year = {2016},
  keywords = {Statistics - Computation},
  pages = {267-294},
  file = {/home/victor/Zotero/storage/5NHUE98M/Nagel et Sudret - 2016 - Spectral likelihood expansions for Bayesian infere.pdf;/home/victor/Zotero/storage/7L887RQ8/1506.html}
}

@book{li_markov_2009,
  title = {Markov Random Field Modeling in Image Analysis},
  publisher = {{Springer Science \& Business Media}},
  author = {Li, Stan Z.},
  year = {2009},
  file = {/home/victor/Zotero/storage/S35XCXY8/Li - 2009 - Markov random field modeling in image analysis.pdf;/home/victor/Zotero/storage/LHKNVA3F/books.html}
}

@article{cui_likelihood-informed_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1403.4680},
  title = {Likelihood-Informed Dimension Reduction for Nonlinear Inverse Problems},
  volume = {30},
  issn = {0266-5611, 1361-6420},
  abstract = {The intrinsic dimensionality of an inverse problem is affected by prior information, the accuracy and number of observations, and the smoothing properties of the forward operator. From a Bayesian perspective, changes from the prior to the posterior may, in many problems, be confined to a relatively low-dimensional subspace of the parameter space. We present a dimension reduction approach that defines and identifies such a subspace, called the "likelihood-informed subspace" (LIS), by characterizing the relative influences of the prior and the likelihood over the support of the posterior distribution. This identification enables new and more efficient computational methods for Bayesian inference with nonlinear forward models and Gaussian priors. In particular, we approximate the posterior distribution as the product of a lower-dimensional posterior defined on the LIS and the prior distribution marginalized onto the complementary subspace. Markov chain Monte Carlo sampling can then proceed in lower dimensions, with significant gains in computational efficiency. We also introduce a Rao-Blackwellization strategy that de-randomizes Monte Carlo estimates of posterior expectations for additional variance reduction. We demonstrate the efficiency of our methods using two numerical examples: inference of permeability in a groundwater system governed by an elliptic PDE, and an atmospheric remote sensing problem based on Global Ozone Monitoring System (GOMOS) observations.},
  number = {11},
  journal = {Inverse Problems},
  doi = {10.1088/0266-5611/30/11/114015},
  author = {Cui, Tiangang and Martin, James and Marzouk, Youssef M. and Solonen, Antti and Spantini, Alessio},
  month = nov,
  year = {2014},
  keywords = {Statistics - Methodology,Statistics - Computation,Mathematics - Numerical Analysis},
  pages = {114015},
  file = {/home/victor/Zotero/storage/47LK8WJT/Cui et al. - 2014 - Likelihood-informed dimension reduction for nonlin.pdf;/home/victor/Zotero/storage/RNW3P6CB/1403.html}
}

@article{petra_computational_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1308.6221},
  primaryClass = {math, stat},
  title = {A Computational Framework for Infinite-Dimensional {{Bayesian}} Inverse Problems: {{Part II}}. {{Stochastic Newton MCMC}} with Application to Ice Sheet Flow Inverse Problems},
  shorttitle = {A Computational Framework for Infinite-Dimensional {{Bayesian}} Inverse Problems},
  abstract = {We address the numerical solution of infinite-dimensional inverse problems in the framework of Bayesian inference. In the Part I companion to this paper (arXiv.org:1308.1313), we considered the linearized infinite-dimensional inverse problem. Here in Part II, we relax the linearization assumption and consider the fully nonlinear infinite-dimensional inverse problem using a Markov chain Monte Carlo (MCMC) sampling method. To address the challenges of sampling high-dimensional pdfs arising from Bayesian inverse problems governed by PDEs, we build on the stochastic Newton MCMC method. This method exploits problem structure by taking as a proposal density a local Gaussian approximation of the posterior pdf, whose construction is made tractable by invoking a low-rank approximation of its data misfit component of the Hessian. Here we introduce an approximation of the stochastic Newton proposal in which we compute the low-rank-based Hessian at just the MAP point, and then reuse this Hessian at each MCMC step. We compare the performance of the proposed method to the original stochastic Newton MCMC method and to an independence sampler. The comparison of the three methods is conducted on a synthetic ice sheet inverse problem. For this problem, the stochastic Newton MCMC method with a MAP-based Hessian converges at least as rapidly as the original stochastic Newton MCMC method, but is far cheaper since it avoids recomputing the Hessian at each step. On the other hand, it is more expensive per sample than the independence sampler; however, its convergence is significantly more rapid, and thus overall it is much cheaper. Finally, we present extensive analysis and interpretation of the posterior distribution, and classify directions in parameter space based on the extent to which they are informed by the prior or the observations.},
  journal = {arXiv:1308.6221 [math, stat]},
  author = {Petra, Noemi and Martin, James and Stadler, Georg and Ghattas, Omar},
  month = aug,
  year = {2013},
  file = {/home/victor/Zotero/storage/Z22783B7/Petra et al. - 2013 - A computational framework for infinite-dimensional.pdf;/home/victor/Zotero/storage/C555J7FY/1308.html}
}

@article{betancourt_conceptual_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.02434},
  primaryClass = {stat},
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous under- standing of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is con- fined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any ex- haustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
  journal = {arXiv:1701.02434 [stat]},
  author = {Betancourt, Michael},
  month = jan,
  year = {2017},
  keywords = {Statistics - Methodology},
  file = {/home/victor/Zotero/storage/97IXVFQ5/Betancourt - 2017 - A Conceptual Introduction to Hamiltonian Monte Car.pdf;/home/victor/Zotero/storage/3VVTEM7T/1701.html}
}

@book{rios_insua_robust_2000,
  address = {{New York}},
  series = {Lecture Notes in Statistics},
  title = {Robust {{Bayesian}} Analysis},
  isbn = {978-0-387-98866-5},
  lccn = {QA279.5 .R64 2000},
  language = {en},
  number = {152},
  publisher = {{Springer}},
  editor = {R{\'i}os Insua, David and Ruggeri, Fabrizio},
  year = {2000},
  keywords = {Bayesian statistical decision theory},
  file = {/home/victor/Zotero/storage/UVCAEIW7/Ríos Insua et Ruggeri - 2000 - Robust Bayesian analysis.pdf}
}

@article{valpine_monte_2004,
  title = {Monte {{Carlo State}}-{{Space Likelihoods}} by {{Weighted Posterior Lernel Density Estimation}}},
  abstract = {Maximum likelihood estimation and likelihood ratio tests for nonlinear, non-Gaussian state-space models require numerical integration for likelihood calculations. Several methods, including Monte Carlo (MC) expectation maximization, MC likelihood ratios, direct MC integra-tion, and particle  lter likelihoods, are inef  cient for the motivating problem of stage-structured population dynamics models in experimen-tal settings. An MC kernel likelihood (MCKL) method is presented that estimates classical likelihoods up to a constant by weighted kernel density estimates of Bayesian posteriors. MCKL is derived by using Bayesian posteriors as importance sampling densities for unnormalized kernel smoothing integrals. MC error and mode bias due to kernel smoothing are discussed and two methods for reducing mode bias are proposed: ``zooming in '' on the maximum likelihood parameters using a focused prior based on an initial estimate and using a posterior cumulant-based approximation of mode bias. A simulated example shows that MCKL can be much more ef  cient than previous approaches for the population dynamics problem. The zooming-in and cumulant-based corrections are illustrated with a multivariate variance estimation problem for which accurate results are obtained even in 20 parameter dimensions.},
  journal = {Journal of the American Statistical Association},
  author = {Valpine, Perry De},
  year = {2004},
  pages = {523--536},
  file = {/home/victor/Zotero/storage/258GQKR7/Valpine - 2004 - Monte Carlo State-Space Likelihoods by Weighted Po.pdf;/home/victor/Zotero/storage/8UFFC2QL/summary.html}
}

@article{yizong_cheng_mean_1995,
  title = {Mean Shift, Mode Seeking, and Clustering},
  volume = {17},
  issn = {01628828},
  abstract = {Mean shift, a simple iterative procedure that shifts each data point to the average of data points in its neighborhood, is generalized and analyzed in this paper. This generalization makes some k-means like clustering algorithms its special cases. It is shown that mean shift is a mode-seeking process on a surface constructed with a ``shadow'' kernel. For Gaussian kernels, mean shift is a gradient mapping. Convergence is studied for mean shift iterations. Cluster analysis is treated as a deterministic problem of finding a fixed point of mean shift that characterizes the data. Applications in clustering and Hough transform are demonstrated. Mean shift is also considered as an evolutionary strategy that performs multistart global optimization.},
  language = {en},
  number = {8},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi = {10.1109/34.400568},
  author = {{Yizong Cheng}},
  year = {Aug./1995},
  pages = {790-799},
  file = {/home/victor/Zotero/storage/A2Z9TT4S/Yizong Cheng - 1995 - Mean shift, mode seeking, and clustering.pdf}
}

@book{scott_multivariate_2015,
  title = {Multivariate Density Estimation: Theory, Practice, and Visualization},
  shorttitle = {Multivariate Density Estimation},
  publisher = {{John Wiley \& Sons}},
  author = {Scott, David W.},
  year = {2015},
  file = {/home/victor/Zotero/storage/MZV5PRKU/multivariate-density-estimation-theory-practice-and-visualization.html;/home/victor/Zotero/storage/UWH7X4SX/books.html}
}

@incollection{forsyth_quick_2008,
  address = {{Berlin, Heidelberg}},
  title = {Quick {{Shift}} and {{Kernel Methods}} for {{Mode Seeking}}},
  volume = {5305},
  isbn = {978-3-540-88692-1 978-3-540-88693-8},
  abstract = {We show that the complexity of the recently introduced medoid-shift algorithm in clustering N points is O(N 2), with a small constant, if the underlying distance is Euclidean. This makes medoid shift considerably faster than mean shift, contrarily to what previously believed. We then exploit kernel methods to extend both mean shift and the improved medoid shift to a large family of distances, with complexity bounded by the effective rank of the resulting kernel matrix, and with explicit regularization constraints. Finally, we show that, under certain conditions, medoid shift fails to cluster data points belonging to the same mode, resulting in over-fragmentation. We propose remedies for this problem, by introducing a novel, simple and extremely efficient clustering algorithm, called quick shift, that explicitly trades off under- and overfragmentation. Like medoid shift, quick shift operates in non-Euclidean spaces in a straightforward manner. We also show that the accelerated medoid shift can be used to initialize mean shift for increased efficiency. We illustrate our algorithms to clustering data on manifolds, image segmentation, and the automatic discovery of visual categories.},
  language = {en},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2008},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Vedaldi, Andrea and Soatto, Stefano},
  editor = {Forsyth, David and Torr, Philip and Zisserman, Andrew},
  year = {2008},
  pages = {705-718},
  file = {/home/victor/Zotero/storage/54BBK8MD/Vedaldi et Soatto - 2008 - Quick Shift and Kernel Methods for Mode Seeking.pdf},
  doi = {10.1007/978-3-540-88693-8_52}
}

@article{celeux_stochastic_1992,
  title = {On {{Stochastic Versions}} of the {{EM Algorithm}}},
  volume = {37},
  issn = {0759-1063, 2070-2779},
  abstract = {We compare three different stochastic versions of the EM algorithm: The SEM algorithm, the SAEM algorithm and the MCEM algorithm. We suggest that the most relevant contribution of the MCEM methodology is what we call the simulated annealing MCEM algorithm, which turns out to be very close to SAEM. We focus particularly on the mixture of distributions problem. In this context, we review the available theoretical results on the convergence of these algorithms and on the behavior of SEM as the sample size tends to infinity. The second part is devoted to intensive Monte Carlo numerical simulations and a real data study. We show that, for some particular mixture situations, the SEM algorithm is almost always preferable to the EM and simulated annealing versions SAEM and MCEM. For some very intricate mixtures, however, none of these algorithms can be confidently used. Then, SEM can be used as an efficient data exploratory tool for locating significant maxima of the likelihood function. In the real data case, we show that the SEM stationary distribution provides a contrasted view of the loglikelihood by emphasizing sensible maxima.},
  language = {en},
  number = {1},
  journal = {Bulletin of Sociological Methodology/Bulletin de M{\'e}thodologie Sociologique},
  doi = {10.1177/075910639203700105},
  author = {Celeux, Gilles and Chauveau, Didier and Diebolt, Jean},
  month = dec,
  year = {1992},
  pages = {55-57},
  file = {/home/victor/Zotero/storage/BIUEYSS9/1992 - Institut national de recherche en informatique et .pdf}
}

@misc{michael_waskom_mwaskom/seaborn_2017,
  title = {Mwaskom/Seaborn: V0.8.1 ({{September}} 2017)},
  shorttitle = {Mwaskom/Seaborn},
  abstract = {v0.8.1 (September 2017) Added a warning in FacetGrid when passing a categorical plot function without specifying order (or hue\_order when hue is used), which is likely to produce a plot that is incorrect. Improved compatibility between FacetGrid or PairGrid and interactive matplotlib backends so that the legend no longer remains inside the figure when using legend\_out=True. Changed categorical plot functions with small plot elements to use dark\_palette instead of `light\_palette`` when generating a sequential palette from a specified color. Improved robustness of kdeplot and distplot to data with fewer than two observations. Fixed a bug in clustermap when using yticklabels=False. Fixed a bug in pointplot where colors were wrong if exactly three points were being drawn. Fixed a bug inpointplot where legend entries for missing data appeared with empty markers. Fixed a bug in clustermap where an error was raised when annotating the main heatmap and showing category colors. Fixed a bug in clustermap where row labels were not being properly rotated when they overlapped. Fixed a bug in kdeplot where the maximum limit on the density axes was not being updated when multiple densities were drawn. Improved compatibility with future versions of pandas.},
  howpublished = {Zenodo},
  author = {Michael Waskom and Olga Botvinnik and Drew O'Kane and Paul Hobson and Saulius Lukauskas and David C Gemperline and Tom Augspurger and Yaroslav Halchenko and John B. Cole and Jordi Warmenhoven and {Julian de Ruiter} and Cameron Pye and Stephan Hoyer and Jake Vanderplas and Santi Villalba and Gero Kunter and Eric Quintero and Pete Bachant and Marcel Martin and Kyle Meyer and Alistair Miles and Yoav Ram and Tal Yarkoni and Mike Lee Williams and Constantine Evans and Clark Fitzgerald and Brian and Chris Fonnesbeck and Antony Lee and Adel Qalieh},
  month = sep,
  year = {2017},
  keywords = {software},
  file = {/home/victor/Zotero/storage/M3DU4FR3/883859.html},
  doi = {10.5281/zenodo.883859}
}

@article{hoffmann_unified_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.03188},
  primaryClass = {math, stat},
  title = {Unified Treatment of the Asymptotics of Asymmetric Kernel Density Estimators},
  abstract = {We extend balloon and sample-smoothing estimators, two types of variable-bandwidth kernel density estimators, by a shift parameter and derive their asymptotic properties. Our approach facilitates the unified study of a wide range of density estimators which are subsumed under these two general classes of kernel density estimators. We demonstrate our method by deriving the asymptotic bias, variance, and mean (integrated) squared error of density estimators with gamma, log-normal, Birnbaum-Saunders, inverse Gaussian and reciprocal inverse Gaussian kernels. We propose two new density estimators for positive random variables that yield properly-normalised density estimates. Plugin expressions for bandwidth estimation are provided to facilitate easy exploratory data analysis.},
  journal = {arXiv:1512.03188 [math, stat]},
  author = {Hoffmann, Till and Jones, Nick S.},
  month = dec,
  year = {2015},
  keywords = {Statistics - Methodology,Mathematics - Statistics Theory},
  file = {/home/victor/Zotero/storage/LJUJKCBD/Hoffmann et Jones - 2015 - Unified treatment of the asymptotics of asymmetric.pdf;/home/victor/Zotero/storage/84RUCHCQ/1512.html}
}

@article{kingma_adam_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6980},
  primaryClass = {cs},
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  journal = {arXiv:1412.6980 [cs]},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  month = dec,
  year = {2014},
  keywords = {Computer Science - Learning},
  file = {/home/victor/Zotero/storage/3CTW2N4D/Kingma et Ba - 2014 - Adam A Method for Stochastic Optimization.pdf;/home/victor/Zotero/storage/LBC2YKV7/1412.html}
}

@book{mackay_information_2003,
  title = {Information Theory, Inference and Learning Algorithms},
  publisher = {{Cambridge university press}},
  author = {MacKay, David JC},
  year = {2003},
  file = {/home/victor/Zotero/storage/APYP3J87/MacKay - 2003 - Information theory, inference and learning algorit.pdf;/home/victor/Zotero/storage/93KA4AD4/books.html}
}

@article{ibrahimi_robust_2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1111.6214},
  primaryClass = {cs, math},
  title = {Robust {{Max}}-{{Product Belief Propagation}}},
  abstract = {We study the problem of optimizing a graph-structured objective function under \textbackslash{}emph\{adversarial\} uncertainty. This problem can be modeled as a two-persons zero-sum game between an Engineer and Nature. The Engineer controls a subset of the variables (nodes in the graph), and tries to assign their values to maximize an objective function. Nature controls the complementary subset of variables and tries to minimize the same objective. This setting encompasses estimation and optimization problems under model uncertainty, and strategic problems with a graph structure. Von Neumann's minimax theorem guarantees the existence of a (minimax) pair of randomized strategies that provide optimal robustness for each player against its adversary. We prove several structural properties of this strategy pair in the case of graph-structured payoff function. In particular, the randomized minimax strategies (distributions over variable assignments) can be chosen in such a way to satisfy the Markov property with respect to the graph. This significantly reduces the problem dimensionality. Finally we introduce a message passing algorithm to solve this minimax problem. The algorithm generalizes max-product belief propagation to this new domain.},
  journal = {arXiv:1111.6214 [cs, math]},
  author = {Ibrahimi, Morteza and Javanmard, Adel and Kanoria, Yashodhan and Montanari, Andrea},
  month = nov,
  year = {2011},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control,Computer Science - Computational Engineering; Finance; and Science},
  file = {/home/victor/Zotero/storage/JXFMY8DC/Ibrahimi et al. - 2011 - Robust Max-Product Belief Propagation.pdf;/home/victor/Zotero/storage/E6QC2FXD/1111.html}
}

@article{sasaki_clustering_,
  title = {Clustering via {{Mode Seeking}} by {{Direct Estimation}} of the {{Gradient}} of a {{Log}}-{{Density}}},
  abstract = {Mean shift clustering finds the modes of the data probability density by identifying the zero points of the density gradient. Since it does not require to fix the number of clusters in advance, the mean shift has been a popular clustering algorithm in various application fields. A typical implementation of the mean shift is to first estimate the density by kernel density estimation and then compute its gradient. However, since a good density estimation does not necessarily imply an accurate estimation of the density gradient, such an indirect two-step approach is not reliable. In this paper, we propose a method to directly estimate the gradient of the log-density without going through density estimation. The proposed method gives the global solution analytically and thus is computationally efficient. We then develop a mean-shift-like fixed-point algorithm to find the modes of the density for clustering. As in the mean shift, one does not need to set the number of clusters in advance. We experimentally show that the proposed clustering method significantly outperforms the mean shift especially for high-dimensional data.},
  language = {en},
  author = {Sasaki, Hiroaki and Hyvarinen, Aapo and Sugiyama, Masashi},
  pages = {16},
  file = {/home/victor/Zotero/storage/TWV2B3NR/Sasaki et al. - Clustering via Mode Seeking by Direct Estimation o.pdf}
}

@techreport{miller_surprised_2016,
  address = {{Rochester, NY}},
  type = {{{SSRN Scholarly Paper}}},
  title = {Surprised by the {{Gambler}}'s and {{Hot Hand Fallacies}}? {{A Truth}} in the {{Law}} of {{Small Numbers}}},
  shorttitle = {Surprised by the {{Gambler}}'s and {{Hot Hand Fallacies}}?},
  abstract = {We prove that a subtle but substantial bias exists in a standard measure of the conditional dependence of present outcomes on streaks of past outcomes in sequential data. The magnitude of this novel form of selection bias generally decreases as the sequence gets longer, but increases in streak length, and remains substantial for a range of sequence lengths often used in empirical work.  The bias has important implications for the literature that investigates incorrect beliefs in sequential decision making---most notably the Hot Hand Fallacy and the Gambler's Fallacy. Upon correcting for the bias, the conclusions of prominent studies in the hot hand fallacy literature are reversed.   The bias also provides a novel structural explanation for how belief in the law of small numbers can persist in the face of experience.},
  language = {en},
  number = {ID 2627354},
  institution = {{Social Science Research Network}},
  author = {Miller, Joshua B. and Sanjurjo, Adam},
  month = nov,
  year = {2016},
  keywords = {Alternation Bias,Finite Sample Bias,Gambler's Fallacy,Hot Hand Effect,Hot Hand Fallacy,Law of Small Numbers,Negative Recency Bias,Selection Bias,Sequential Data,Sequential Decision Making,Small Sample Bias},
  file = {/home/victor/Zotero/storage/A9S8ISDT/papers.html}
}

@techreport{daks_golden_2017,
  address = {{Rochester, NY}},
  type = {{{SSRN Scholarly Paper}}},
  title = {Do the {{Golden State Warriors Have Hot Hands}}?},
  abstract = {Star Golden State Warriors Steph Curry, Klay Thompson, and Kevin Durant are great shooters but they are not streak shooters. Only rarely do they show signs of a hot hand. This conclusion is based on an empirical analysis of field goal and free throw data from the 82 regular season and 17 postseason games played by the Warriors in 2016-2017. Our analysis is inspired by the iconic 1985 hot-hand study by Thomas Gilovitch, Robert Vallone and Amos Tversky, but uses a permutation test to automatically account for Josh Miller and Adam Sanjurjo's recent small sample correction. In this study we show how long standing problems can be reexamined using nonparametric statistics to avoid faulty hypothesis tests due to misspecified distributions.},
  language = {en},
  number = {ID 2984615},
  institution = {{Social Science Research Network}},
  author = {Daks, Alon and Desai, Nishant and Goldberg, Lisa R.},
  month = nov,
  year = {2017},
  keywords = {conditional probability,hot hand,nonparametric test,permutation test,small sample correction,streak shooting},
  file = {/home/victor/Zotero/storage/VAXQU5TJ/papers.html}
}

@phdthesis{gilquin_echantillonnages_2016,
  title = {{{\'E}chantillonnages Monte Carlo et quasi-Monte Carlo pour l'estimation des indices de Sobol' : application {\`a} un mod{\`e}le transport-urbanisme}},
  shorttitle = {{{\'E}chantillonnages Monte Carlo et quasi-Monte Carlo pour l'estimation des indices de Sobol'}},
  abstract = {Le d{\'e}veloppement et l'utilisation de mod{\`e}les int{\'e}gr{\'e}s transport-urbanisme sont devenus une norme pour repr{\'e}senter les interactions entre l'usage des sols et le transport de biens et d'individus sur un territoire. Ces mod{\`e}les sont souvent utilis{\'e}s comme outils d'aide {\`a} la d{\'e}cision pour des politiques de planification urbaine.Les mod{\`e}les transport-urbanisme, et plus g{\'e}n{\'e}ralement les mod{\`e}les math{\'e}matiques, sont pour la majorit{\'e} con{\c c}us {\`a} partir de codes num{\'e}riques complexes. Ces codes impliquent tr{\`e}s souvent des param{\`e}tres dont l'incertitude est peu connue et peut potentiellement avoir un impact important sur les variables de sortie du mod{\`e}le.Les m{\'e}thodes d'analyse de sensibilit{\'e} globales sont des outils performants permettant d'{\'e}tudier l'influence des param{\`e}tres d'un mod{\`e}le sur ses sorties. En particulier, les m{\'e}thodes bas{\'e}es sur le calcul des indices de sensibilit{\'e} de Sobol' fournissent la possibilit{\'e} de quantifier l'influence de chaque param{\`e}tre mais {\'e}galement d'identifier l'existence d'interactions entre ces param{\`e}tres.Dans cette th{\`e}se, nous privil{\'e}gions la m{\'e}thode dite {\`a} base de plans d'exp{\'e}riences r{\'e}pliqu{\'e}s encore appel{\'e}e m{\'e}thode r{\'e}pliqu{\'e}e. Cette m{\'e}thode a l'avantage de ne requ{\'e}rir qu'un nombre relativement faible d'{\'e}valuations du mod{\`e}le pour calculer les indices de Sobol' d'ordre un et deux.Cette th{\`e}se se focalise sur des extensions de la m{\'e}thode r{\'e}pliqu{\'e}e pour faire face {\`a} des contraintes issues de notre application sur le mod{\`e}le transport-urbanisme Tranus, comme la pr{\'e}sence de corr{\'e}lation entre param{\`e}tres et la prise en compte de sorties multivari{\'e}es.Nos travaux proposent {\'e}galement une approche r{\'e}cursive pour l'estimation s{\'e}quentielle des indices de Sobol'. L'approche r{\'e}cursive repose {\`a} la fois sur la construction it{\'e}rative d'hypercubes latins et de tableaux orthogonaux stratifi{\'e}s et sur la d{\'e}finition d'un nouveau crit{\`e}re d'arr{\^e}t. Cette approche offre une meilleure pr{\'e}cision sur l'estimation des indices tout en permettant de recycler des premiers jeux d'{\'e}valuations du mod{\`e}le. Nous proposons aussi de combiner une telle approche avec un {\'e}chantillonnage quasi-Monte Carlo.Nous pr{\'e}sentons {\'e}galement une application de nos contributions pour le calage du mod{\`e}le de transport-urbanisme Tranus.},
  language = {fr},
  school = {Universit{\'e} Grenoble Alpes},
  author = {Gilquin, Laurent},
  month = oct,
  year = {2016},
  file = {/home/victor/Zotero/storage/W44NI63K/Gilquin - 2016 - Échantillonnages Monte Carlo et quasi-Monte Carlo .pdf;/home/victor/Zotero/storage/RK4DE9LZ/tel-01680304.html}
}

@article{zhao_same_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.5402},
  primaryClass = {cs, stat},
  title = {{{SAME}} but {{Different}}: {{Fast}} and {{High}}-{{Quality Gibbs Parameter Estimation}}},
  shorttitle = {{{SAME}} but {{Different}}},
  abstract = {Gibbs sampling is a workhorse for Bayesian inference but has several limitations when used for parameter estimation, and is often much slower than non-sampling inference methods. SAME (State Augmentation for Marginal Estimation) \textbackslash{}cite\{Doucet99,Doucet02\} is an approach to MAP parameter estimation which gives improved parameter estimates over direct Gibbs sampling. SAME can be viewed as cooling the posterior parameter distribution and allows annealed search for the MAP parameters, often yielding very high quality (lower loss) estimates. But it does so at the expense of additional samples per iteration and generally slower performance. On the other hand, SAME dramatically increases the parallelism in the sampling schedule, and is an excellent match for modern (SIMD) hardware. In this paper we explore the application of SAME to graphical model inference on modern hardware. We show that combining SAME with factored sample representation (or approximation) gives throughput competitive with the fastest symbolic methods, but with potentially better quality. We describe experiments on Latent Dirichlet Allocation, achieving speeds similar to the fastest reported methods (online Variational Bayes) and lower cross-validated loss than other LDA implementations. The method is simple to implement and should be applicable to many other models.},
  journal = {arXiv:1409.5402 [cs, stat]},
  author = {Zhao, Huasha and Jiang, Biye and Canny, John},
  month = sep,
  year = {2014},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,D.1.3,K.3.2},
  file = {/home/victor/Zotero/storage/3FZ6AFKT/Zhao et al. - 2014 - SAME but Different Fast and High-Quality Gibbs Pa.pdf;/home/victor/Zotero/storage/7W43GP2H/Zhao et al. - 2014 - SAME but Different Fast and High-Quality Gibbs Pa.pdf;/home/victor/Zotero/storage/ZPDSAHMX/1409.html}
}

@article{dempster_maximum_1977,
  title = {Maximum Likelihood from Incomplete Data via the {{EM}} Algorithm},
  journal = {Journal of the royal statistical society. Series B (methodological)},
  author = {Dempster, Arthur P. and Laird, Nan M. and Rubin, Donald B.},
  year = {1977},
  pages = {1--38},
  file = {/home/victor/Zotero/storage/773M4BD9/Dempster et al. - Maximum Likelihood from Incomplete Data via the EM.pdf;/home/victor/Zotero/storage/CSLEN38G/Dempster et al. - 1977 - Maximum likelihood from incomplete data via the EM.pdf;/home/victor/Zotero/storage/7H9ZL662/2984875.html}
}

@book{mclachlan_mixture_1988,
  title = {Mixture Models: {{Inference}} and Applications to Clustering},
  volume = {84},
  shorttitle = {Mixture Models},
  publisher = {{Marcel Dekker}},
  author = {McLachlan, Geoffrey J. and Basford, Kaye E.},
  year = {1988},
  file = {/home/victor/Zotero/storage/N6ARLX48/UQ308790.html}
}

@book{andrieu_introduction_2003,
  title = {An {{Introduction}} to {{MCMC}} for {{Machine Learning}}},
  abstract = {This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.},
  author = {Andrieu, Christophe and Freitas, Nando De and {al}, et},
  year = {2003},
  file = {/home/victor/Zotero/storage/WJ26GD3I/Andrieu et al. - 2003 - An Introduction to MCMC for Machine Learning.pdf;/home/victor/Zotero/storage/FVZ5A9HC/summary.html}
}

@article{lan_wormhole_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1306.0063},
  primaryClass = {stat},
  title = {Wormhole {{Hamiltonian Monte Carlo}}},
  abstract = {In machine learning and statistics, probabilistic inference involving multimodal distributions is quite difficult. This is especially true in high dimensional problems, where most existing algorithms cannot easily move from one mode to another. To address this issue, we propose a novel Bayesian inference approach based on Markov Chain Monte Carlo. Our method can effectively sample from multimodal distributions, especially when the dimension is high and the modes are isolated. To this end, it exploits and modifies the Riemannian geometric properties of the target distribution to create \textbackslash{}emph\{wormholes\} connecting modes in order to facilitate moving between them. Further, our proposed method uses the regeneration technique in order to adapt the algorithm by identifying new modes and updating the network of wormholes without affecting the stationary distribution. To find new modes, as opposed to rediscovering those previously identified, we employ a novel mode searching algorithm that explores a \textbackslash{}emph\{residual energy\} function obtained by subtracting an approximate Gaussian mixture density (based on previously discovered modes) from the target density function.},
  journal = {arXiv:1306.0063 [stat]},
  author = {Lan, Shiwei and Streets, Jeffrey and Shahbaba, Babak},
  month = may,
  year = {2013},
  keywords = {Statistics - Computation},
  file = {/home/victor/Zotero/storage/SQAV645H/Lan et al. - 2013 - Wormhole Hamiltonian Monte Carlo.pdf;/home/victor/Zotero/storage/UCNIUAYF/1306.html}
}

@incollection{chen_estimating_2000,
  series = {Springer {{Series}} in {{Statistics}}},
  title = {Estimating {{Marginal Posterior Densities}}},
  isbn = {978-1-4612-7074-4 978-1-4612-1276-8},
  abstract = {In Bayesian inference, a joint posterior distribution is available through the likelihood function and a prior distribution. One purpose of Bayesian inference is to calculate and display marginal posterior densities because the marginal posterior densities provide complete information about parameters of interest. As shown in Chapter 2, a Markov chain Monte Carlo (MCMC) sampling algorithm, such as the Gibbs sampler or a Metropolis-Hastings algorithm, can be used to draw MCMC samples from the posterior distribution. Chapter 3 also demonstrates how we can easily obtain posterior quantities such as posterior means, posterior standard deviations, and other posterior quantities from MCMC samples. However, when a Bayesian model becomes complicated, it may be difficult to obtain a reliable estimator of a marginal posterior density based on the MCMC sample. A traditional method for estimating marginal posterior densities is kernel density estimation. Since the kernel density estimator is nonparametric, it may not be efficient. On the other hand, the kernel density estimator may not be applicable for some complicated Bayesian models. In the context of Bayesian inference, the joint posterior density is typically known up to a normalizing constant. Using the structure of a posterior density, a number of authors (e.g., Gelfand, Smith, and Lee 1992; Johnson 1992; Chen 1993 and 1994; Chen and Shao 1997c; Chib 1995; Verdinelli and Wasserman 1995) propose parametric marginal posterior density estimators based on the MCMC sample. In this chapter, we present several available Monte Carlo (MC) methods for computing marginal posterior density estimators, and we also discuss how well marginal posterior density estimation works using the Kullback\textemdash{}Leibler (K\textemdash{}L) divergence as a performance measure.},
  language = {en},
  booktitle = {Monte {{Carlo Methods}} in {{Bayesian Computation}}},
  publisher = {{Springer, New York, NY}},
  author = {Chen, Ming-Hui and Shao, Qi-Man and Ibrahim, Joseph G.},
  year = {2000},
  pages = {94-123},
  file = {/home/victor/Zotero/storage/56KWF5P8/Chen et al. - 2000 - Estimating Marginal Posterior Densities.pdf;/home/victor/Zotero/storage/YRAALCCA/978-1-4612-1276-8_4.html},
  doi = {10.1007/978-1-4612-1276-8_4}
}

@article{kuczera_there_2010,
  title = {There Are No Hydrological Monsters, Just Models and Observations with Large Uncertainties!},
  volume = {55},
  issn = {0262-6667},
  abstract = {Catchments that do not behave in the way the hydrologist expects, expose the frailties of hydrological science, particularly its unduly simplistic treatment of input and model uncertainty. A conceptual rainfall\textendash{}runoff model represents a highly simplified hypothesis of the transformation of rainfall into runoff. Sub-grid variability and mis-specification of processes introduce an irreducible model error, about which little is currently known. In addition, hydrological observation systems are far from perfect, with the principal catchment forcing (rainfall) often subject to large sampling errors. When ignored or treated simplistically, these errors develop into monsters that destroy our ability to model certain catchments. In this paper, these monsters are tackled using Bayesian Total Error Analysis, a framework that accounts for user-specified sources of error and yields quantitative insights into how prior knowledge of these uncertainties affects our ability to infer models and use them for predictive purposes. A case study involving a catchment with an apparent water balance anomaly (a hydrological monstrosity!) illustrates these concepts. It is found that, in the absence of additional information, the rainfall\textendash{}runoff record is insufficient to explain this anomaly \textendash{} it could be due to a large export of groundwater, systematic overestimation of catchment rainfall of the order of 40\%, or a conspiracy of these factors. There is ``no free lunch'' in hydrology. The rainfall\textendash{}runoff record on its own is insufficient to decompose the different sources of uncertainty affecting calibration, testing and prediction, and hydrological monstrosities will persist until additional independent knowledge of uncertainties is obtained. Citation Kuczera, G., Renard, B., Thyer, M. \& Kavetski, D. (2010) There are no hydrological monsters, just models and observations with large uncertainties! Hydrol. Sci. J. 55(6), 980\textendash{}991.},
  number = {6},
  journal = {Hydrological Sciences Journal},
  doi = {10.1080/02626667.2010.504677},
  author = {Kuczera, George and Renard, Benjamin and Thyer, Mark and Kavetski, Dmitri},
  month = aug,
  year = {2010},
  keywords = {analyse Bayesienne de l'erreur totale,Bayesian total error analysis,data errors,erreur structurelle du modèle,erreurs d'observation,ill-posedness,inférence mal posée,model structural error,modèles pluie–débit,rainfall–runoff models},
  pages = {980-991},
  file = {/home/victor/Zotero/storage/VDZCEVA2/Kuczera et al. - 2010 - There are no hydrological monsters, just models an.pdf;/home/victor/Zotero/storage/82J26CGU/02626667.2010.html}
}

@article{li_mingliang_calibration_2012,
  title = {Calibration of a Distributed Flood Forecasting Model with Input Uncertainty Using a {{Bayesian}} Framework},
  volume = {48},
  issn = {0043-1397},
  abstract = {In the process of calibrating distributed hydrological models, accounting for input uncertainty is important, yet challenging. In this study, we develop a Bayesian model to estimate parameters associated with a geomorphology?based hydrological model (GBHM). The GBHM model uses geomorphic characteristics to simplify model structure and physically based methods to represent hydrological processes. We divide the observed discharge into low? and high?flow data, and use the first?order autoregressive model to describe their temporal dependence. We consider relative errors in rainfall as spatially distributed variables and estimate them jointly with the GBHM parameters. The joint posterior probability distribution is explored using Markov chain Monte Carlo methods, which include Metropolis?Hastings, delay rejection adaptive Metropolis, and Gibbs sampling methods. We evaluate the Bayesian model using both synthetic and field data sets. The synthetic case study demonstrates that the developed method generally is effective in calibrating GBHM parameters and in estimating their associated uncertainty. The calibration ignoring input errors has lower accuracy and lower reliability compared to the calibration that includes estimation of the input errors, especially under model structure uncertainty. The field case study shows that calibration of GBHM parameters under complex field conditions remains a challenge. Although jointly estimating input errors and GBHM parameters improves the continuous ranked probability score and the consistency of the predictive distribution with the observed data, the improvement is incremental. To better calibrate parameters in a distributed model, such as GBHM here, we need to develop a more complex model and incorporate much more information.},
  number = {8},
  journal = {Water Resources Research},
  doi = {10.1029/2010WR010062},
  author = {{Li Mingliang} and {Yang Dawen} and {Chen Jinsong} and {Hubbard Susan S.}},
  month = aug,
  year = {2012},
  keywords = {MCMC,Bayesian,calibration,distributed hydrological model,flood forecasting,input uncertainty},
  file = {/home/victor/Zotero/storage/6FLWZTMQ/Li Mingliang et al. - 2012 - Calibration of a distributed flood forecasting mod.pdf;/home/victor/Zotero/storage/P4PLU5MD/2010WR010062.html}
}

@article{gong_multi-objective_2015,
  title = {Multi-Objective Parameter Optimization of Common Land Model Using Adaptive Surrogate Modeling},
  volume = {19},
  issn = {1607-7938},
  abstract = {Parameter specification usually has significant influence on the performance of land surface models (LSMs). However, estimating the parameters properly is a challenging task due to the following reasons: (1) LSMs usually have too many adjustable parameters (20 to 100 or even more), leading to the curse of dimensionality in the parameter input space; (2) LSMs usually have many output variables involving water/energy/carbon cycles, so that calibrating LSMs is actually a multi-objective optimization problem; (3) Regional LSMs are expensive to run, while conventional multi-objective optimization methods need a large number of model runs (typically {$\sim$} 105\textendash{}106). It makes parameter optimization computationally prohibitive. An uncertainty quantification framework was developed to meet the aforementioned challenges, which include the following steps: (1) using parameter screening to reduce the number of adjustable parameters, (2) using surrogate models to emulate the responses of dynamic models to the variation of adjustable parameters, (3) using an adaptive strategy to improve the efficiency of surrogate modeling-based optimization; (4) using a weighting function to transfer multi-objective optimization to single-objective optimization. In this study, we demonstrate the uncertainty quantification framework on a single column application of a LSM \textendash{} the Common Land Model (CoLM), and evaluate the effectiveness and efficiency of the proposed framework. The result indicate that this framework can efficiently achieve optimal parameters in a more effective way. Moreover, this result implies the possibility of calibrating other large complex dynamic models, such as regionalscale LSMs, atmospheric models and climate models.},
  language = {en},
  number = {5},
  journal = {Hydrology and Earth System Sciences},
  doi = {10.5194/hess-19-2409-2015},
  author = {Gong, W. and Duan, Q. and Li, J. and Wang, C. and Di, Z. and Dai, Y. and Ye, A. and Miao, C.},
  month = may,
  year = {2015},
  pages = {2409-2425},
  file = {/home/victor/Zotero/storage/YHYSTU8F/Gong et al. - 2015 - Multi-objective parameter optimization of common l.pdf}
}

@article{gong_wei_multiobjective_2015,
  title = {Multiobjective Adaptive Surrogate Modeling-based Optimization for Parameter Estimation of Large, Complex Geophysical Models},
  volume = {52},
  issn = {0043-1397},
  abstract = {Abstract Parameter specification is an important source of uncertainty in large, complex geophysical models. These models generally have multiple model outputs that require multiobjective optimization algorithms. Although such algorithms have long been available, they usually require a large number of model runs and are therefore computationally expensive for large, complex dynamic models. In this paper, a multiobjective adaptive surrogate modeling?based optimization (MO?ASMO) algorithm is introduced that aims to reduce computational cost while maintaining optimization effectiveness. Geophysical dynamic models usually have a prior parameterization scheme derived from the physical processes involved, and our goal is to improve all of the objectives by parameter calibration. In this study, we developed a method for directing the search processes toward the region that can improve all of the objectives simultaneously. We tested the MO?ASMO algorithm against NSGA?II and SUMO with 13 test functions and a land surface model ? the Common Land Model (CoLM). The results demonstrated the effectiveness and efficiency of MO?ASMO.},
  number = {3},
  journal = {Water Resources Research},
  doi = {10.1002/2015WR018230},
  author = {{Gong Wei} and {Duan Qingyun} and {Li Jianduo} and {Wang Chen} and {Di Zhenhua} and {Ye Aizhong} and {Miao Chiyuan} and {Dai Yongjiu}},
  month = dec,
  year = {2015},
  keywords = {Gaussian processes regression,adaptive sampling,multiobjective optimization,surrogate model},
  pages = {1984-2008},
  file = {/home/victor/Zotero/storage/G5IIESLL/Gong Wei et al. - 2015 - Multiobjective adaptive surrogate modeling‐based o.pdf;/home/victor/Zotero/storage/RBW94KNS/2015WR018230.html}
}

@article{wang_evaluation_2014,
  title = {An Evaluation of Adaptive Surrogate Modeling Based Optimization with Two Benchmark Problems},
  volume = {60},
  issn = {1364-8152},
  abstract = {Surrogate modeling uses cheap ``surrogates'' to represent the response surface of simulation models. It involves several steps, including initial sampling, regression and adaptive sampling. This study evaluates an adaptive surrogate modeling based optimization (ASMO) method on two benchmark problems: the Hartman function and calibration of the SAC-SMA hydrologic model. Our results show that: 1) Gaussian Processes are the best surrogate model construction method. A minimum Interpolation Surface method is the best adaptive sampling method. Low discrepancy Quasi Monte Carlo methods are the most suitable initial sampling designs. Some 15\textendash{}20 times the dimension of the problem may be the proper initial sample size; 2) The ASMO method is much more efficient than the widely used Shuffled Complex Evolution global optimization method. However, ASMO can provide only approximate optimal solutions, whose precision is limited by surrogate modeling methods and problem-specific features; and 3) The identifiability of model parameters is correlated with parameter sensitivity.},
  journal = {Environmental Modelling \& Software},
  doi = {10.1016/j.envsoft.2014.05.026},
  author = {Wang, Chen and Duan, Qingyun and Gong, Wei and Ye, Aizhong and Di, Zhenhua and Miao, Chiyuan},
  month = oct,
  year = {2014},
  keywords = {Adaptive sampling,Adaptive surrogate modeling based optimization,Computationally intensive computer models,Design of experiment,Global sensitivity analysis},
  pages = {167-179},
  file = {/home/victor/Zotero/storage/I4NUQW6Q/Wang et al. - 2014 - An evaluation of adaptive surrogate modeling based.pdf;/home/victor/Zotero/storage/DB6ATLC3/S1364815214001698.html}
}

@article{jones_taxonomy_,
  title = {A {{Taxonomy}} of {{Global Optimization Methods Based}} on {{Response Surfaces}}},
  abstract = {This paper presents a taxonomy of existing approaches for using response surfaces for global optimization. Each method is illustrated with a simple numerical example that brings out its advantages and disadvantages. The central theme is that methods that seem quite reasonable often have non-obvious failure modes. Understanding these failure modes is essential for the development of practical algorithms that fulfill the intuitive promise of the response surface approach.},
  language = {en},
  author = {JONES, DONALD R},
  pages = {39},
  file = {/home/victor/Zotero/storage/KNZITF9Q/JONES - A Taxonomy of Global Optimization Methods Based on.pdf}
}

@article{hernandez-lobato_predictive_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.2541},
  primaryClass = {cs, stat},
  title = {Predictive {{Entropy Search}} for {{Efficient Global Optimization}} of {{Black}}-Box {{Functions}}},
  abstract = {We propose a novel information-theoretic approach for Bayesian optimization called Predictive Entropy Search (PES). At each iteration, PES selects the next evaluation point that maximizes the expected information gained with respect to the global maximum. PES codifies this intractable acquisition function in terms of the expected reduction in the differential entropy of the predictive distribution. This reformulation allows PES to obtain approximations that are both more accurate and efficient than other alternatives such as Entropy Search (ES). Furthermore, PES can easily perform a fully Bayesian treatment of the model hyperparameters while ES cannot. We evaluate PES in both synthetic and realworld applications, including optimization problems in machine learning, finance, biotechnology, and robotics. We show that the increased accuracy of PES leads to significant gains in optimization performance.},
  language = {en},
  journal = {arXiv:1406.2541 [cs, stat]},
  author = {{Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel and Hoffman, Matthew W. and Ghahramani, Zoubin},
  month = jun,
  year = {2014},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {/home/victor/Zotero/storage/FF7YVTEY/Hernández-Lobato et al. - 2014 - Predictive Entropy Search for Efficient Global Opt.pdf}
}

@article{wang_max-value_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.01968},
  primaryClass = {stat},
  title = {Max-Value {{Entropy Search}} for {{Efficient Bayesian Optimization}}},
  abstract = {Entropy Search (ES) and Predictive Entropy Search (PES) are popular and empirically successful Bayesian Optimization techniques. Both rely on a compelling information-theoretic motivation, and maximize the information gained about the \$\textbackslash{}arg\textbackslash{}max\$ of the unknown function; yet, both are plagued by the expensive computation for estimating entropies. We propose a new criterion, Max-value Entropy Search (MES), that instead uses the information about the maximum function value. We show relations of MES to other Bayesian optimization methods, and establish a regret bound. We observe that MES maintains or improves the good empirical performance of ES/PES, while tremendously lightening the computational burden. In particular, MES is much more robust to the number of samples used for computing the entropy, and hence more efficient for higher dimensional problems.},
  journal = {arXiv:1703.01968 [stat]},
  author = {Wang, Zi and Jegelka, Stefanie},
  month = mar,
  year = {2017},
  keywords = {Statistics - Machine Learning},
  file = {/home/victor/Zotero/storage/NMAKJPDF/Wang et Jegelka - 2017 - Max-value Entropy Search for Efficient Bayesian Op.pdf;/home/victor/Zotero/storage/TU5YA9D2/1703.html}
}

@article{pedregosa_scikit-learn_2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  volume = {12},
  issn = {1533-7928},
  shorttitle = {Scikit-Learn},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language.  Emphasis is put on ease of use, performance, documentation, and API consistency.  It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings.  Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  journal = {Journal of Machine Learning Research},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  month = oct,
  year = {2011},
  pages = {2825-2830},
  file = {/home/victor/Zotero/storage/XZ34DBFU/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf}
}

@article{mackay_information-based_1992,
  title = {Information-{{Based Objective Functions}} for {{Active Data Selection}}},
  volume = {4},
  issn = {0899-7667, 1530-888X},
  language = {en},
  number = {4},
  journal = {Neural Computation},
  doi = {10.1162/neco.1992.4.4.590},
  author = {MacKay, David J. C.},
  month = jul,
  year = {1992},
  pages = {590-604},
  file = {/home/victor/Zotero/storage/XHG73IPQ/MacKay - 1992 - Information-Based Objective Functions for Active D.pdf}
}

@article{chapelle_empirical_,
  title = {An {{Empirical Evaluation}} of {{Thompson Sampling}}},
  abstract = {Thompson sampling is one of oldest heuristic to address the exploration / exploitation trade-off, but it is surprisingly unpopular in the literature. We present here some empirical results using Thompson sampling on simulated and real data, and show that it is highly competitive. And since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against.},
  language = {en},
  author = {Chapelle, Olivier and Li, Lihong},
  pages = {9},
  file = {/home/victor/Zotero/storage/TVUBSS3S/Chapelle et Li - An Empirical Evaluation of Thompson Sampling.pdf}
}

@article{hennig_fast_,
  title = {Fast {{Probabilistic Optimization}} from {{Noisy Gradients}}},
  language = {en},
  author = {Hennig, Philipp},
  pages = {9},
  file = {/home/victor/Zotero/storage/IZHCITWD/Hennig - Fast Probabilistic Optimization from Noisy Gradien.pdf}
}

@article{villemonteix_informational_2006,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {cs/0611143},
  title = {An Informational Approach to the Global Optimization of Expensive-to-Evaluate Functions},
  abstract = {In many global optimization problems motivated by engineering applications, the number of function evaluations is severely limited by time or cost. To ensure that each evaluation contributes to the localization of good candidates for the role of global minimizer, a sequential choice of evaluation points is usually carried out. In particular, when Kriging is used to interpolate past evaluations, the uncertainty associated with the lack of information on the function can be expressed and used to compute a number of criteria accounting for the interest of an additional evaluation at any given point. This paper introduces minimizer entropy as a new Kriging-based criterion for the sequential choice of points at which the function should be evaluated. Based on stepwise uncertainty reduction, it accounts for the informational gain on the minimizer expected from a new evaluation. The criterion is approximated using conditional simulations of the Gaussian process model behind Kriging, and then inserted into an algorithm similar in spirit to the Efficient Global Optimization (EGO) algorithm. An empirical comparison is carried out between our criterion and expected improvement, one of the reference criteria in the literature. Experimental results indicate major evaluation savings over EGO. Finally, the method, which we call IAGO (for Informational Approach to Global Optimization) is extended to robust optimization problems, where both the factors to be tuned and the function evaluations are corrupted by noise.},
  language = {en},
  journal = {arXiv:cs/0611143},
  author = {Villemonteix, Julien and Vazquez, Emmanuel and Walter, Eric},
  month = nov,
  year = {2006},
  keywords = {Computer Science - Numerical Analysis,G.1.1,G.1.6},
  file = {/home/victor/Zotero/storage/A85EICRS/Villemonteix et al. - 2006 - An informational approach to the global optimizati.pdf}
}

@article{_efficient_,
  title = {Efficient {{Global Optimization}} of {{Expensive Black}}-{{Box Functions}}},
  abstract = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
  language = {en},
  pages = {38},
  file = {/home/victor/Zotero/storage/JSEY6UYJ/Efficient Global Optimization of Expensive Black-B.pdf}
}

@book{matheron_traite_1962,
  title = {Trait{\'e} de G{\'e}ostatistique Appliqu{\'e}e. 1 (1962)},
  volume = {1},
  publisher = {{Editions Technip}},
  author = {Matheron, Georges},
  year = {1962}
}

@article{krige_statistical_1951,
  title = {A Statistical Approach to Some Basic Mine Valuation Problems on the {{Witwatersrand}}},
  volume = {52},
  number = {6},
  journal = {Journal of the Southern African Institute of Mining and Metallurgy},
  author = {Krige, Daniel G.},
  year = {1951},
  pages = {119--139},
  file = {/home/victor/Zotero/storage/JZM426MV/Krige - 1951 - A statistical approach to some basic mine valuatio.pdf;/home/victor/Zotero/storage/ELTXPCEN/AJA0038223X_4792.html}
}

@article{sudret_polynomial_nodate,
  title = {Polynomial Chaos Expansions and Stochastic Finite Element Methods},
  language = {en},
  author = {Sudret, Bruno},
  pages = {43},
  file = {/home/victor/Zotero/storage/WLS85HD6/Sudret - Polynomial chaos expansions and stochastic finite .pdf}
}

@article{wang_optimization_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.08586},
  primaryClass = {cs, math, stat},
  title = {Optimization of {{Smooth Functions}} with {{Noisy Observations}}: {{Local Minimax Rates}}},
  shorttitle = {Optimization of {{Smooth Functions}} with {{Noisy Observations}}},
  abstract = {We consider the problem of global optimization of an unknown non-convex smooth function with zeroth-order feedback. In this setup, an algorithm is allowed to adaptively query the underlying function at different locations and receives noisy evaluations of function values at the queried points (i.e. the algorithm has access to zeroth-order information). Optimization performance is evaluated by the expected difference of function values at the estimated optimum and the true optimum. In contrast to the classical optimization setup, first-order information like gradients are not directly accessible to the optimization algorithm. We show that the classical minimax framework of analysis, which roughly characterizes the worst-case query complexity of an optimization algorithm in this setting, leads to excessively pessimistic results. We propose a local minimax framework to study the fundamental difficulty of optimizing smooth functions with adaptive function evaluations, which provides a refined picture of the intrinsic difficulty of zeroth-order optimization. We show that for functions with fast level set growth around the global minimum, carefully designed optimization algorithms can identify a near global minimizer with many fewer queries. For the special case of strongly convex and smooth functions, our implied convergence rates match the ones developed for zeroth-order convex optimization problems [1, 22]. At the other end of the spectrum, for worst-case smooth functions no algorithm can converge faster than the minimax rate of estimating the entire unknown function in the {$\mathscr{l}$}8-norm. We provide an intuitive and efficient algorithm that attains the derived upper error bounds. Finally, using the local minimax framework we are able to clearly dichotomize adaptive and non-adaptive algorithms by showing that non-adaptive algorithms, although optimal in a global minimax sense, do not attain the optimal local minimax rate.},
  language = {en},
  journal = {arXiv:1803.08586 [cs, math, stat]},
  author = {Wang, Yining and Balakrishnan, Sivaraman and Singh, Aarti},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Mathematics - Statistics Theory},
  file = {/home/victor/Zotero/storage/EBLYPEB9/Wang et al. - 2018 - Optimization of Smooth Functions with Noisy Observ.pdf}
}

@article{ginsbourger_bayesian_2014,
  title = {Bayesian {{Adaptive Reconstruction}} of {{Profile Optima}} and {{Optimizers}}},
  volume = {2},
  issn = {2166-2525},
  abstract = {Given a function depending both on decision parameters and nuisance variables, we consider the issue of estimating and quantifying uncertainty on profile optima and/or optimal points as functions of the nuisance variables. The proposed methods base on interpolations of the objective function constructed from a finite set of evaluations. Here the functions of interest are reconstructed relying on a kriging model, but also using Gaussian field conditional simulations, that allow a quantification of uncertainties in the Bayesian framework. Besides, we elaborate a variant of the Expected Improvement criterion, that proves efficient for adaptively learning the set of profile optima and optimizers. The results are illustrated on a toy example and through a physics case study on the optimal packing of polydisperse frictionless spheres.},
  language = {en},
  number = {1},
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  doi = {10.1137/130949555},
  author = {Ginsbourger, David and Baccou, Jean and Chevalier, Cl{\'e}ment and Perales, Fr{\'e}d{\'e}ric and Garland, Nicolas and Monerie, Yann},
  month = jan,
  year = {2014},
  pages = {490-510},
  file = {/home/victor/Zotero/storage/FBVA9HE4/Ginsbourger et al. - 2014 - Bayesian Adaptive Reconstruction of Profile Optima.pdf}
}

@article{chevalier_fast_,
  title = {Fast Uncertainty Reduction Strategies Relying on {{Gaussian}} Process Models},
  language = {en},
  author = {Chevalier, Cl{\'e}ment},
  pages = {196},
  file = {/home/victor/Zotero/storage/9U4ENIG7/Chevalier - Fast uncertainty reduction strategies relying on G.pdf}
}

@book{azais_level_2009,
  title = {Level Sets and Extrema of Random Processes and Fields},
  publisher = {{Wiley \& Sons}},
  author = {Aza{\"i}s, Jean-Marc and Wschebor, Mario},
  year = {2009},
  file = {/home/victor/Zotero/storage/XMVNYBKM/hal-00982795.html},
  doi = {10.1002/9780470434642}
}

@article{forouzandeh_shahraki_reliability-based_2014,
  title = {Reliability-Based Robust Design Optimization: {{A}} General Methodology Using Genetic Algorithm},
  volume = {74},
  issn = {0360-8352},
  shorttitle = {Reliability-Based Robust Design Optimization},
  abstract = {In this paper, we present an improved general methodology including four stages to design robust and reliable products under uncertainties. First, as the formulation stage, we consider reliability and robustness simultaneously to propose the new formulation of reliability-based robust design optimization (RBRDO) problems. In order to generate reliable and robust Pareto-optimal solutions, the combination of genetic algorithm with reliability assessment loop based on the performance measure approach is applied as the second stage. Next, we develop two criteria to select a solution from obtained Pareto-optimal set to achieve the best possible implementation. Finally, the result verification is performed with Monte Carlo Simulations and also the quality improvement during manufacturing process is considered by identifying and controlling the critical variables. The effectiveness and applicability of this new proposed methodology is demonstrated through a case study.},
  journal = {Computers \& Industrial Engineering},
  doi = {10.1016/j.cie.2014.05.013},
  author = {Forouzandeh Shahraki, Ameneh and Noorossana, Rassoul},
  month = aug,
  year = {2014},
  keywords = {Genetic algorithm,Most probable point,Multi-objective optimization,Process capability index,Reliability-based robust design optimization},
  pages = {199-207},
  file = {/home/victor/Zotero/storage/3SXRCR3K/Forouzandeh Shahraki et Noorossana - 2014 - Reliability-based robust design optimization A ge.pdf;/home/victor/Zotero/storage/PFKHCTX8/S0360835214001582.html}
}

@inproceedings{cheng_robust_2009,
  title = {Robust Reliability Optimization of Mechanical Components Using Non-Probabilistic Interval Model},
  abstract = {Mechanical components design is subjected to uncertainties in material and geometrical properties, loads and other variables. For reliability optimization design with uncertain parameters, based on the non-probabilistic reliability theory, robust design and optimal design method, the uncertain parameters of mechanical components are expressed by non-probabilistic interval variables, and a non-probabilistic measure and procedure for robust reliability computation is presented. Compared with the conventional probabilistic reliability optimization approach, the proposed method does not require a presumed probability distribution of the uncertain parameters and only the bounds or ranges of their variations are required. The optimal design for non-probabilistic robust reliability is formulated as a two level optimization problem, in which the first level minimizes the original robust optimal objective with the constraints of non-probabilistic reliability index, and the secondary level is used to identify the reliability index. The purpose of it is to get a tradeoff between the design objective and the robustness to uncertainties and satisfy the requirements for reliability. For instance, the robust reliability optimization of unidirectional wedge-typed overrunning clutch is proposed, and the results show that the proposed method is useful for mechanical components design and quality improvement.},
  booktitle = {2009 {{IEEE}} 10th {{International Conference}} on {{Computer}}-{{Aided Industrial Design Conceptual Design}}},
  doi = {10.1109/CAIDCD.2009.5375184},
  author = {Cheng, X.},
  month = nov,
  year = {2009},
  keywords = {probability,Constraint optimization,design engineering,Design methodology,Design optimization,geometrical properties,mechanical components design,Mechanical Components Design,Mechanical factors,mechanical products,Mechanical variables measurement,Non-probabilistic Interval Model,nonprobabilistic interval model,nonprobabilistic interval variables,nonprobabilistic measure,nonprobabilistic reliability index,nonprobabilistic reliability theory,nonprobabilistic robust reliability,optimal design,optimisation,Optimization methods,probabilistic reliability optimization,probability distribution,Probability distribution,reliability,Reliability Optimization,reliability optimization design,Reliability theory,robust design,Robust Design,robust reliability optimization,Robustness,two level optimization problem,uncertain parameters,Uncertainty,unidirectional wedge-typed overrunning clutch},
  pages = {821-825},
  file = {/home/victor/Zotero/storage/H9PVKPM9/5375184.html}
}

@article{wald_statistical_1945,
  title = {Statistical {{Decision Functions Which Minimize}} the {{Maximum Risk}}},
  volume = {46},
  issn = {0003-486X},
  number = {2},
  journal = {Annals of Mathematics},
  doi = {10.2307/1969022},
  author = {Wald, Abraham},
  year = {1945},
  pages = {265-280}
}

@article{zhuang_enhancing_2015,
  title = {Enhancing Product Robustness in Reliability-Based Design Optimization},
  volume = {138},
  issn = {0951-8320},
  abstract = {Different types of uncertainties need to be addressed in a product design optimization process. In this paper, the uncertainties in both product design variables and environmental noise variables are considered. The reliability-based design optimization (RBDO) is integrated with robust product design (RPD) to concurrently reduce the production cost and the long-term operation cost, including quality loss, in the process of product design. This problem leads to a multi-objective optimization with probabilistic constraints. In addition, the model uncertainties associated with a surrogate model that is derived from numerical computation methods, such as finite element analysis, is addressed. A hierarchical experimental design approach, augmented by a sequential sampling strategy, is proposed to construct the response surface of product performance function for finding optimal design solutions. The proposed method is demonstrated through an engineering example.},
  journal = {Reliability Engineering \& System Safety},
  doi = {10.1016/j.ress.2015.01.026},
  author = {Zhuang, Xiaotian and Pan, Rong and Du, Xiaoping},
  month = jun,
  year = {2015},
  keywords = {Kriging metamodel,Robust design optimization,Sequential optimization and reliability assessment,Sequential sampling},
  pages = {145-153},
  file = {/home/victor/Zotero/storage/VZ5TD4AD/Zhuang et al. - 2015 - Enhancing product robustness in reliability-based .pdf;/home/victor/Zotero/storage/NUX5FFBN/S0951832015000368.html}
}

@article{zaman_robustness-based_2011,
  title = {Robustness-Based Design Optimization under Data Uncertainty},
  volume = {44},
  issn = {1615-147X, 1615-1488},
  abstract = {This paper proposes formulations and algorithms for design optimization under both aleatory (i.e., natural or physical variability) and epistemic uncertainty (i.e., imprecise probabilistic information), from the perspective of system robustness. The proposed formulations deal with epistemic uncertainty arising from both sparse and interval data without any assumption about the probability distributions of the random variables. A decoupled approach is proposed in this paper to un-nest the robustness-based design from the analysis of non-design epistemic variables to achieve computational efficiency. The proposed methods are illustrated for the upper stage design problem of a two-stage-toorbit (TSTO) vehicle, where the information on the random design inputs are only available as sparse point and/or interval data. As collecting more data reduces uncertainty but increases cost, the effect of sample size on the optimality and robustness of the solution is also studied. A method is developed to determine the optimal sample size for sparse point data that leads to the solutions of the design problem that are least sensitive to variations in the input random variables.},
  language = {en},
  number = {2},
  journal = {Structural and Multidisciplinary Optimization},
  doi = {10.1007/s00158-011-0622-2},
  author = {Zaman, Kais and McDonald, Mark and Mahadevan, Sankaran and Green, Lawrence},
  month = aug,
  year = {2011},
  pages = {183-197},
  file = {/home/victor/Zotero/storage/TDFMEENU/Zaman et al. - 2011 - Robustness-based design optimization under data un.pdf}
}

@article{beyer_robust_2007,
  title = {Robust Optimization \textendash{} {{A}} Comprehensive Survey},
  volume = {196},
  issn = {00457825},
  abstract = {This paper reviews the state-of-the-art in robust design optimization \textendash{} the search for designs and solutions which are immune with respect to production tolerances, parameter drifts during operation time, model sensitivities and others. Starting with a short glimps of Taguchi's robust design methodology, a detailed survey of approaches to robust optimization is presented. This includes a detailed discussion on how to account for design uncertainties and how to measure robustness (i.e., how to evaluate robustness). The main focus will be on the different approaches to perform robust optimization in practice including the methods of mathematical programming, deterministic nonlinear optimization, and direct search methods such as stochastic approximation and evolutionary computation. It discusses the strengths and weaknesses of the different methods, thus, providing a basis for guiding the engineer to the most appropriate techniques. It also addresses performance aspects and test scenarios for direct robust optimization techniques.},
  language = {en},
  number = {33-34},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  doi = {10.1016/j.cma.2007.03.003},
  author = {Beyer, Hans-Georg and Sendhoff, Bernhard},
  month = jul,
  year = {2007},
  pages = {3190-3218},
  file = {/home/victor/Zotero/storage/CRUQZVV2/Beyer et Sendhoff - 2007 - Robust optimization – A comprehensive survey.pdf}
}

@article{au_estimation_2001,
  title = {Estimation of Small Failure Probabilities in High Dimensions by Subset Simulation},
  volume = {16},
  issn = {02668920},
  abstract = {A new simulation approach, called `subset simulation', is proposed to compute small failure probabilities encountered in reliability analysis of engineering systems. The basic idea is to express the failure probability as a product of larger conditional failure probabilities by introducing intermediate failure events. With a proper choice of the conditional events, the conditional failure probabilities can be made suf\textregistered{}ciently large so that they can be estimated by means of simulation with a small number of samples. The original problem of calculating a small failure probability, which is computationally demanding, is reduced to calculating a sequence of conditional probabilities, which can be readily and ef\textregistered{}ciently estimated by means of simulation. The conditional probabilities cannot be estimated ef\textregistered{}ciently by a standard Monte Carlo procedure, however, and so a Markov chain Monte Carlo simulation (MCS) technique based on the Metropolis algorithm is presented for their estimation. The proposed method is robust to the number of uncertain parameters and ef\textregistered{}cient in computing small probabilities. The ef\textregistered{}ciency of the method is demonstrated by calculating the \textregistered{}rst-excursion probabilities for a linear oscillator subjected to white noise excitation and for a \textregistered{}ve-story nonlinear hysteretic shear building under uncertain seismic excitation. q 2001 Elsevier Science Ltd. All rights reserved.},
  language = {en},
  number = {4},
  journal = {Probabilistic Engineering Mechanics},
  doi = {10.1016/S0266-8920(01)00019-4},
  author = {Au, Siu-Kui and Beck, James L.},
  month = oct,
  year = {2001},
  pages = {263-277},
  file = {/home/victor/Zotero/storage/9JP6H7FP/Au et Beck - 2001 - Estimation of small failure probabilities in high .pdf}
}

@article{bect_sequential_2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1009.5177},
  title = {Sequential Design of Computer Experiments for the Estimation of a Probability of Failure},
  volume = {22},
  issn = {0960-3174, 1573-1375},
  abstract = {This paper deals with the problem of estimating the volume of the excursion set of a function f : Rd \textrightarrow R above a given threshold, under a probability measure on Rd that is assumed to be known. In the industrial world, this corresponds to the problem of estimating a probability of failure of a system. When only an expensive-to-simulate model of the system is available, the budget for simulations is usually severely limited and therefore classical Monte Carlo methods ought to be avoided. One of the main contributions of this article is to derive SUR (stepwise uncertainty reduction) strategies from a Bayesian-theoretic formulation of the problem of estimating a probability of failure. These sequential strategies use a Gaussian process model of f and aim at performing evaluations of f as efficiently as possible to infer the value of the probability of failure. We compare these strategies to other strategies also based on a Gaussian process model for estimating a probability of failure.},
  language = {en},
  number = {3},
  journal = {Statistics and Computing},
  doi = {10.1007/s11222-011-9241-4},
  author = {Bect, Julien and Ginsbourger, David and Li, Ling and Picheny, Victor and Vazquez, Emmanuel},
  month = may,
  year = {2012},
  keywords = {Statistics - Computation,62L05; 62C10; 62P30,Statistics - Applications},
  pages = {773-793},
  file = {/home/victor/Zotero/storage/KMNZSABA/Bect et al. - 2012 - Sequential design of computer experiments for the .pdf}
}

@book{boyd_convex_2004,
  address = {{Cambridge, UK ; New York}},
  title = {Convex Optimization},
  isbn = {978-0-521-83378-3},
  lccn = {QA402.5 .B69 2004},
  language = {en},
  publisher = {{Cambridge University Press}},
  author = {Boyd, Stephen P. and Vandenberghe, Lieven},
  year = {2004},
  keywords = {Convex functions,Mathematical optimization},
  file = {/home/victor/Zotero/storage/YZQWMY7F/Boyd et Vandenberghe - 2004 - Convex optimization.pdf}
}

@article{jones_efficient_1998,
  title = {Efficient Global Optimization of Expensive Black-Box Functions},
  volume = {13},
  number = {4},
  journal = {Journal of Global optimization},
  author = {Jones, Donald R. and Schonlau, Matthias and Welch, William J.},
  year = {1998},
  pages = {455--492},
  file = {/home/victor/Zotero/storage/8QF4EUL8/Jones et al. - 1998 - Efficient global optimization of expensive black-b.pdf;/home/victor/Zotero/storage/DSISWVYM/A1008306431147.html}
}

@article{dubourg_reliability-based_2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1104.3667},
  title = {Reliability-Based Design Optimization Using Kriging Surrogates and Subset Simulation},
  volume = {44},
  issn = {1615-147X, 1615-1488},
  abstract = {The aim of the present paper is to develop a strategy for solving reliability-based design optimization (RBDO) problems that remains applicable when the performance models are expensive to evaluate. Starting with the premise that simulation-based approaches are not affordable for such problems, and that the most-probablefailure-point-based approaches do not permit to quantify the error on the estimation of the failure probability, an approach based on both metamodels and advanced simulation techniques is explored. The kriging metamodeling technique is chosen in order to surrogate the performance functions because it allows one to genuinely quantify the surrogate error. The surrogate error onto the limit-state surfaces is propagated to the failure probabilities estimates in order to provide an empirical error measure. This error is then sequentially reduced by means of a populationbased adaptive refinement technique until the kriging surrogates are accurate enough for reliability analysis. This original refinement strategy makes it possible to add several observations in the design of experiments at the same time. Reliability and reliability sensitivity analyses are performed by means of the subset simulation technique for the sake of numerical efficiency. The adaptive surrogate-based strategy for reliability estimation is finally involved into a classical gradient-based optimization algorithm in order to solve the RBDO problem. The kriging surrogates are built in a so-called augmented reliability space thus making them reusable from one nested RBDO iteration to the other. The strategy is compared to other approaches available in the literature on three academic examples in the field of structural mechanics.},
  language = {en},
  number = {5},
  journal = {Structural and Multidisciplinary Optimization},
  doi = {10.1007/s00158-011-0653-8},
  author = {Dubourg, V. and Sudret, B. and Bourinet, J.-M.},
  month = nov,
  year = {2011},
  keywords = {Statistics - Machine Learning,Statistics - Methodology},
  pages = {673-690},
  file = {/home/victor/Zotero/storage/L9JA7B9K/Dubourg et al. - 2011 - Reliability-based design optimization using krigin.pdf}
}

@article{qin_improving_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.10033},
  primaryClass = {cs, stat},
  title = {Improving the {{Expected Improvement Algorithm}}},
  abstract = {The expected improvement (EI) algorithm is a popular strategy for information collection in optimization under uncertainty. The algorithm is widely known to be too greedy, but nevertheless enjoys wide use due to its simplicity and ability to handle uncertainty and noise in a coherent decision theoretic framework. To provide rigorous insight into EI, we study its properties in a simple setting of Bayesian optimization where the domain consists of a finite grid of points. This is the so-called best-arm identification problem, where the goal is to allocate measurement effort wisely to confidently identify the best arm using a small number of measurements. In this framework, one can show formally that EI is far from optimal. To overcome this shortcoming, we introduce a simple modification of the expected improvement algorithm. Surprisingly, this simple change results in an algorithm that is asymptotically optimal for Gaussian best-arm identification problems, and provably outperforms standard EI by an order of magnitude.},
  journal = {arXiv:1705.10033 [cs, stat]},
  author = {Qin, Chao and Klabjan, Diego and Russo, Daniel},
  month = may,
  year = {2017},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {/home/victor/Zotero/storage/V2QHCQYZ/Qin et al. - 2017 - Improving the Expected Improvement Algorithm.pdf;/home/victor/Zotero/storage/NBKTIDCQ/1705.html}
}

@incollection{mockus_bayesian_1974,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {On Bayesian Methods for Seeking the Extremum},
  isbn = {978-3-540-07165-5 978-3-540-37497-8},
  language = {en},
  booktitle = {Optimization {{Techniques IFIP Technical Conference Novosibirsk}}, {{July}} 1\textendash{}7, 1974},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Mo{\v c}kus, J.},
  month = jul,
  year = {1974},
  pages = {400-404},
  file = {/home/victor/Zotero/storage/UG95UKH3/Močkus - 1974 - On bayesian methods for seeking the extremum.pdf;/home/victor/Zotero/storage/MCDFG96M/3-540-07165-2_55.html},
  doi = {10.1007/3-540-07165-2_55}
}

@article{azais_distribution_2005,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {math/0503475},
  title = {On the Distribution of the Maximum of a Gaussian Field with d Parameters},
  volume = {15},
  issn = {1050-5164},
  abstract = {Let I be a compact d-dimensional manifold, let X:I\textbackslash{}to R be a Gaussian process with regular paths and let F\_I(u), u\textbackslash{}in R, be the probability distribution function of sup\_\{t\textbackslash{}in I\}X(t). We prove that under certain regularity and nondegeneracy conditions, F\_I is a C\^1-function and satisfies a certain implicit equation that permits to give bounds for its values and to compute its asymptotic behavior as u\textbackslash{}to +\textbackslash{}infty. This is a partial extension of previous results by the authors in the case d=1. Our methods use strongly the so-called Rice formulae for the moments of the number of roots of an equation of the form Z(t)=x, where Z:I\textbackslash{}to R\^d is a random field and x is a fixed point in R\^d. We also give proofs for this kind of formulae, which have their own interest beyond the present application.},
  language = {en},
  number = {1A},
  journal = {The Annals of Applied Probability},
  doi = {10.1214/105051604000000602},
  author = {Azais, Jean-Marc and Wschebor, Mario},
  month = feb,
  year = {2005},
  keywords = {Mathematics - Probability,60G15; 60G70. (Primary)},
  pages = {254-278},
  file = {/home/victor/Zotero/storage/AMLGSN9K/Azais et Wschebor - 2005 - On the distribution of the maximum of a gaussian f.pdf}
}

@article{rychlik_reliability_nodate,
  title = {On {{Some Reliability Applications}} of {{Rice}}'s {{Formula}} for the {{Intensity}} of {{Level Crossings}}},
  abstract = {Let X be a stationary process with absolutely continuous sample paths. If EjX\_0j is \textregistered{}nite and if the distribution of X0 is absolutely continuous, then, for almost all u, the crossing intensity mu of the level u by Xt is given by the generalized Rice's formula muR EjX\_0jjX0  u fX0u. The classical Rice's formula for mu, which is valid for a \textregistered{}xed level u, mu  jzj fX\_0;X0z; udz, holds under more restrictive technical conditions that can be dif\textregistered{}cult to check in applications. In this paper it is shown that often in practice the weaker form of Rice's formula (valid for almost all u) is suf\textregistered{}cient. Three engineering problems are discussed; prediction of fatigue life time; computing the average stress at slams and analysis of crest height of sea waves.},
  language = {en},
  author = {RYCHLIK, IGOR},
  pages = {19},
  file = {/home/victor/Zotero/storage/YEIM26HS/RYCHLIK - On Some Reliability Applications of Rice's Formula.pdf}
}

@article{gorissen_practical_2015,
  title = {A Practical Guide to Robust Optimization},
  volume = {53},
  issn = {03050483},
  abstract = {Robust optimization is a young and active research field that has been mainly developed in the last 15 years. Robust optimization is very useful for practice, since it is tailored to the information at hand, and it leads to computationally tractable formulations. It is therefore remarkable that real-life applications of robust optimization are still lagging behind; there is much more potential for real-life applications than has been exploited hitherto. The aim of this paper is to help practitioners to understand robust optimization and to successfully apply it in practice. We provide a brief introduction to robust optimization, and also describe important do's and don'ts for using it in practice. We use many small examples to illustrate our discussions.},
  language = {en},
  journal = {Omega},
  doi = {10.1016/j.omega.2014.12.006},
  author = {Gorissen, Bram L. and Yan\i{}ko{\u g}lu, {\.I}hsan and {den Hertog}, Dick},
  month = jun,
  year = {2015},
  pages = {124-137},
  file = {/home/victor/Zotero/storage/7UD53J55/Gorissen et al. - 2015 - A practical guide to robust optimization.pdf}
}

@article{lee_robust_2010,
  title = {A Robust Structural Design Method Using the {{Kriging}} Model to Define the Probability of Design Success},
  volume = {224},
  issn = {0954-4062},
  abstract = {Abstract, In this study, a robust optimization method is proposed by introducing the Kriging approximation model and defining the probability of design-success. A key problem in robust optimization is that the mean and the variation of a response cannot be calculated easily. This research presents an implementation of the approximate statistical moment method based on the Kriging metamodel. Furthermore, the statistics using the second-order statistical approximation method are adopted to avoid the local robust optimum. Thus, the probability of design-success, which is defined as the probability of satisfying the imposed design requirements, is represented as a function of approximate mean and variance. The formulation for the robust optimization can be defined as the probability of design-success of each response. The mathematical problem and the design problems of a two-bar structure and microgyroscope are investigated for the validation of the proposed method.},
  language = {en},
  number = {2},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
  doi = {10.1243/09544062JMES1736},
  author = {Lee, K-H},
  month = feb,
  year = {2010},
  pages = {379-388}
}

@inproceedings{vazquez_estimating_2005,
  title = {Estimating Derivatives and Integrals with {{Kriging}}},
  isbn = {978-0-7803-9567-1},
  abstract = {This paper formalizes a methodology based on Kriging, a technique developped by geostatisticians, for estimating derivatives and integrals of signals that are only known via possibly irregularly spaced and noisy observations. This finds direct applications, e.g., in system identification when differential algebra is used to express parameters as nonlinear functions of the inputs and outputs and their derivatives. The procedure is quite simple to implement, and allows confidence intervals on the predicted values to be derived.},
  language = {en},
  publisher = {{IEEE}},
  doi = {10.1109/CDC.2005.1583482},
  author = {Vazquez, E. and Walter, E.},
  year = {2005},
  pages = {8156-8161},
  file = {/home/victor/Zotero/storage/4BSCESRW/Vazquez et Walter - 2005 - Estimating derivatives and integrals with Kriging.pdf}
}

@inproceedings{liang_single-loop_2004,
  title = {A {{Single}}-{{Loop Method}} for {{Reliability}}-{{Based Design Optimization}}},
  volume = {2004},
  isbn = {978-0-7918-4694-0},
  abstract = {Reliability-Based Design Optimization (RBDO) can provide optimum designs in the presence of uncertainty. It can therefore, be a powerful tool for design under uncertainty. The traditional, double-loop RBDO algorithm requires nested optimization loops, where the design optimization (outer) loop, repeatedly calls a series of reliability (inner) loops. Due to the nested optimization loops, the computational effort can be prohibitive for practical problems. A single-loop RBDO algorithm is proposed in this paper for both normal and nonnormal random variables. Its accuracy is the same with the double-loop approach and its efficiency is almost equivalent to deterministic optimization. It collapses the nested optimization loops into an equivalent single-loop optimization process by imposing the Karush-Kuhn-Tucker optimality conditions of the reliability loops as equivalent deterministic equality constraints of the design optimization loop. It therefore, converts the probabilistic optimization problem into an equivalent deterministic optimization problem, eliminating the need for calculating the Most Probable Point (MPP) in repeated reliability assessments. Several numerical applications including an automotive vehicle side impact example, demonstrate the accuracy and superior efficiency of the proposed single-loop RBDO algorithm.},
  language = {en},
  publisher = {{ASME}},
  doi = {10.1115/DETC2004-57255},
  author = {Liang, Jinghong and Mourelatos, Zissimos P. and Tu, Jian},
  year = {2004},
  pages = {419-430},
  file = {/home/victor/Zotero/storage/XNMVG9MM/Liang et al. - 2004 - A Single-Loop Method for Reliability-Based Design .pdf}
}

@article{pujol_minimisation_nodate,
  title = {{Minimisation de quantiles \textendash{} application en m{\'e}canique}},
  language = {fr},
  author = {Pujol, Gilles and Riche, Rodolphe Le and Bay, Xavier and Roustant, Olivier},
  pages = {7},
  file = {/home/victor/Zotero/storage/GSYTV4DK/Pujol et al. - Minimisation de quantiles – application en mécaniq.pdf}
}

@article{bect_bayesian_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1601.02557},
  title = {Bayesian Subset Simulation},
  volume = {5},
  issn = {2166-2525},
  abstract = {We consider the problem of estimating a probability of failure {$\alpha$}, defined as the volume of the excursion set of a function f : X {$\subseteq$} Rd \textrightarrow R above a given threshold, under a given probability measure on X. In this article, we combine the popular subset simulation algorithm (Au and Beck, Probab. Eng. Mech. 2001) and our sequential Bayesian approach for the estimation of a probability of failure (Bect, Ginsbourger, Li, Picheny and Vazquez, Stat. Comput. 2012). This makes it possible to estimate {$\alpha$} when the number of evaluations of f is very limited and {$\alpha$} is very small. The resulting algorithm is called Bayesian subset simulation (BSS). A key idea, as in the subset simulation algorithm, is to estimate the probabilities of a sequence of excursion sets of f above intermediate thresholds, using a sequential Monte Carlo (SMC) approach. A Gaussian process prior on f is used to define the sequence of densities targeted by the SMC algorithm, and drive the selection of evaluation points of f to estimate the intermediate probabilities. Adaptive procedures are proposed to determine the intermediate thresholds and the number of evaluations to be carried out at each stage of the algorithm. Numerical experiments illustrate that BSS achieves significant savings in the number of function evaluations with respect to other Monte Carlo approaches.},
  language = {en},
  number = {1},
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  doi = {10.1137/16M1078276},
  author = {Bect, Julien and Li, Ling and Vazquez, Emmanuel},
  month = jan,
  year = {2017},
  keywords = {Statistics - Computation},
  pages = {762-786},
  file = {/home/victor/Zotero/storage/CRIYDVDY/Bect et al. - 2017 - Bayesian subset simulation.pdf}
}

@article{schobi_combining_nodate,
  title = {Combining Polynomial Chaos Expansions and {{Kriging}} for Solving Structural Reliability Problems},
  abstract = {Nowadays advanced simulation models such as finite element models are used in every domain of science and engineering in order to predict the behaviour of systems and, in case of engineering applications, to optimize them and assess their performance. In parallel, engineers are all the more concerned with structural reliability and robust design, meaning that the quantification of uncertainties has become a key challenge.},
  language = {en},
  author = {Schobi, Roland and Sudret, Bruno},
  pages = {11},
  file = {/home/victor/Zotero/storage/UWZH64MW/Schobi et Sudret - Combining polynomial chaos expansions and Kriging .pdf}
}

@article{robbins_measure_1944,
  title = {On the {{Measure}} of a {{Random Set}}},
  volume = {15},
  issn = {0003-4851, 2168-8990},
  abstract = {Project Euclid - mathematics and statistics online},
  language = {EN},
  number = {1},
  journal = {The Annals of Mathematical Statistics},
  doi = {10.1214/aoms/1177731315},
  author = {Robbins, H. E.},
  month = mar,
  year = {1944},
  pages = {70-74},
  file = {/home/victor/Zotero/storage/JMFXEH7K/Robbins - 1944 - On the Measure of a Random Set.pdf;/home/victor/Zotero/storage/XU8WEAC9/1177731315.html}
}

@misc{noauthor_virtual_nodate,
  title = {Virtual {{Library}} of {{Simulation Experiments}}: {{Test Functions}} and {{Datasets}}},
  howpublished = {https://www.sfu.ca/\textasciitilde{}ssurjano/index.html},
  file = {/home/victor/Zotero/storage/EN5EEF58/index.html}
}

@article{molchanov_lectures_nodate,
  title = {Lectures on Random Sets and Their Applications in Economics and finance},
  abstract = {This course introduces main concepts from the theory of random sets with emphasis on applications in economics and finance: most importantly inference for partially identified models and transaction costs modelling.},
  language = {en},
  author = {Molchanov, Ilya},
  pages = {32},
  file = {/home/victor/Zotero/storage/DNHLWW2L/Molchanov - Lectures on random sets and their applications in .pdf}
}

@article{wu_parallel_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.04414},
  primaryClass = {cs, stat},
  title = {The {{Parallel Knowledge Gradient Method}} for {{Batch Bayesian Optimization}}},
  abstract = {In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g. when evaluating the performances of several different neural network architectures in a parallel computing environment. In this paper, we develop a novel batch Bayesian optimization algorithm --- the parallel knowledge gradient method. By construction, this method provides the one-step Bayes-optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy.},
  journal = {arXiv:1606.04414 [cs, stat]},
  author = {Wu, Jian and Frazier, Peter I.},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Statistics - Machine Learning,Computer Science - Machine Learning},
  file = {/home/victor/Zotero/storage/7MDQJAV2/Wu et Frazier - 2016 - The Parallel Knowledge Gradient Method for Batch B.pdf;/home/victor/Zotero/storage/4C3GXJWK/1606.html}
}

@incollection{nicosia_fast_2013,
  address = {{Berlin, Heidelberg}},
  title = {Fast {{Computation}} of the {{Multi}}-{{Points Expected Improvement}} with {{Applications}} in {{Batch Selection}}},
  volume = {7997},
  isbn = {978-3-642-44972-7 978-3-642-44973-4},
  abstract = {The Multi-points Expected Improvement criterion (or q-EI) has recently been studied in batch-sequential Bayesian Optimization. This paper deals with a new way of computing q-EI, without using Monte-Carlo simulations, through a closed-form formula. The latter allows a very fast computation of q-EI for reasonably low values of q (typically, less than 10). New parallel kriging-based optimization strategies, tested on different toy examples, show promising results.},
  language = {en},
  booktitle = {Learning and {{Intelligent Optimization}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Chevalier, Cl{\'e}ment and Ginsbourger, David},
  editor = {Nicosia, Giuseppe and Pardalos, Panos},
  year = {2013},
  pages = {59-69},
  file = {/home/victor/Zotero/storage/WQFDMN8K/Chevalier et Ginsbourger - 2013 - Fast Computation of the Multi-Points Expected Impr.pdf},
  doi = {10.1007/978-3-642-44973-4_7}
}

@unpublished{zahm_certified_2018,
  title = {Certified Dimension Reduction in Nonlinear {{Bayesian}} Inverse Problems},
  abstract = {We propose a dimension reduction technique for Bayesian inverse problems with nonlinear forward operators, non-Gaussian priors, and non-Gaussian observation noise. The likelihood function is approximated by a ridge function, i.e., a map which depends non-trivially only on a few linear combinations of the parameters. We build this ridge approximation by minimizing an upper bound on the Kullback-Leibler divergence between the posterior distribution and its approximation. This bound, obtained via logarithmic Sobolev inequalities, allows one to certify the error of the posterior approximation. Computing the bound requires computing the second moment matrix of the gradient of the log-likelihood function. In practice, a sample-based approximation of the upper bound is then required. We provide an analysis that enables control of the posterior approximation error due to this sampling. Numerical and theoretical comparisons with existing methods illustrate the benefits of the proposed methodology.},
  author = {Zahm, Olivier and Cui, Tiangang and Law, Kody and Spantini, Alessio and Marzouk, Youssef},
  month = jul,
  year = {2018},
  file = {/home/victor/Zotero/storage/PLPEVGG7/Zahm et al. - 2018 - Certified dimension reduction in nonlinear Bayesia.pdf}
}

@article{wu_bayesian_nodate,
  title = {Bayesian {{Optimization}} with {{Gradients}}},
  abstract = {Bayesian optimization has been successful at global optimization of expensiveto-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to find good solutions with fewer objective function evaluations. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledgegradient (d-KG), which is one-step Bayes-optimal, asymptotically consistent, and provides greater one-step value of information than in the derivative-free setting. d-KG accommodates noisy and incomplete derivative information, comes in both sequential and batch forms, and can optionally reduce the computational cost of inference through automatically selected retention of a single directional derivative. We also compute the d-KG acquisition function and its gradient using a novel fast discretization-free technique. We show d-KG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, deep learning, kernel learning, and k-nearest neighbors.},
  language = {en},
  author = {Wu, Jian and Poloczek, Matthias and Wilson, Andrew G and Frazier, Peter},
  pages = {12},
  file = {/home/victor/Zotero/storage/IW7GFGC8/Wu et al. - Bayesian Optimization with Gradients.pdf}
}

@incollection{sudret_polynomial_2015,
  title = {Polynomial Chaos Expansions and Stochastic Finite Element Methods},
  abstract = {This paper is a state-of-the art review on sparse polynomial chaos expansions (PCE) for engineering applications. It contains a step-by-step presentation of PCEs and their use in moment-, sensitivity- and reliability analysis.  Error estimators and sparse expansions for addressing high-dimensional problems are discussed. Applications in geotechnical engineering showcase the efficiency of sparse PCEs.},
  booktitle = {Risk and {{Reliability}} in {{Geotechnical Engineering}}},
  publisher = {{CRC Press}},
  author = {Sudret, Bruno},
  editor = {{Kok-Kwang Phoon}, Jianye Ching},
  year = {2015},
  keywords = {geotechnics,global sensitivity analysis,polynomial chaos expansions,risk analysis},
  pages = {265-300},
  file = {/home/victor/Zotero/storage/VMUHQL7E/Sudret - 2015 - Polynomial chaos expansions and stochastic finite .pdf}
}

@article{wilson_maximizing_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.10196},
  primaryClass = {cs, stat},
  title = {Maximizing Acquisition Functions for {{Bayesian}} Optimization},
  abstract = {Bayesian optimization is a sample-efficient approach to global optimization that relies on theoretically motivated value heuristics (acquisition functions) to guide the search process. Fully maximizing acquisition functions produces the Bayes' decision rule, but this ideal is difficult to achieve since these functions are frequently non-trivial to optimize. This statement is especially true when evaluating queries in parallel, where acquisition functions are routinely non-convex, highdimensional, and intractable. We present two modern approaches for maximizing acquisition functions that exploit key properties thereof, namely the differentiability of Monte Carlo integration and the submodularity of parallel querying.},
  language = {en},
  journal = {arXiv:1805.10196 [cs, stat]},
  author = {Wilson, James T. and Hutter, Frank and Deisenroth, Marc Peter},
  month = may,
  year = {2018},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  file = {/home/victor/Zotero/storage/Z5DW775A/Wilson et al. - 2018 - Maximizing acquisition functions for Bayesian opti.pdf}
}

@inproceedings{wang_new_2017,
  title = {A New Acquisition Function for {{Bayesian}} Optimization Based on the Moment-Generating Function},
  isbn = {978-1-5386-1645-1},
  abstract = {Bayesian Optimization or Efficient Global Optimization (EGO) is a global search strategy that is designed for expensive black-box functions. In this algorithm, a statistical model (usually the Gaussian process model) is constructed on some initial data samples. The global optimum is approached by iteratively maximizing a so-called acquisition function, that balances the exploration and exploitation effect of the search. The performance of such an algorithm is largely affected by the choice of the acquisition function. Inspired by the usage of higher moments from the Gaussian process model, it is proposed to construct a novel acquisition function based on the moment-generating function (MGF) of the improvement, which is the stochastic gain over the current best fitness value by sampling at an unknown point. This MGF-based acquisition function takes all the higher moments into account and introduces an additional real-valued parameter to control the trade-off between exploration and exploitation. The motivation, rationale and closed-form expression of the proposed function are discussed in detail. In addition, we also illustrate its advantage over other acquisition functions, especially the so-called generalized expected improvement.},
  language = {en},
  publisher = {{IEEE}},
  doi = {10.1109/SMC.2017.8122656},
  author = {Wang, Hao and {van Stein}, Bas and Emmerich, Michael and Back, Thomas},
  month = oct,
  year = {2017},
  pages = {507-512},
  file = {/home/victor/Zotero/storage/H8NR48J2/Wang et al. - 2017 - A new acquisition function for Bayesian optimizati.pdf}
}

@article{rontsis_distributionally_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.04191},
  primaryClass = {stat},
  title = {Distributionally {{Ambiguous Optimization Techniques}} for {{Batch Bayesian Optimization}}},
  abstract = {We propose a novel, theoretically-grounded, acquisition function for Batch Bayesian optimization informed by insights from distributionally ambiguous optimization. Our acquisition function is a lower bound on the well-known Expected Improvement function, which requires evaluation of a Gaussian Expectation over a multivariate piecewise affine function. Our bound is computed instead by evaluating the best-case expectation over all probability distributions consistent with the same mean and variance as the original Gaussian distribution. Unlike alternative approaches, including Expected Improvement, our proposed acquisition function avoids multi-dimensional integrations entirely, and can be computed exactly - even on large batch sizes - as the solution of a tractable convex optimization problem. Our suggested acquisition function can also be optimized efficiently, since first and second derivative information can be calculated inexpensively as by-products of the acquisition function calculation itself. We derive various novel theorems that ground our work theoretically and we demonstrate superior performance via simple motivating examples, benchmark functions and real-world problems.},
  journal = {arXiv:1707.04191 [stat]},
  author = {Rontsis, Nikitas and Osborne, Michael A. and Goulart, Paul J.},
  month = jul,
  year = {2017},
  keywords = {Statistics - Machine Learning},
  file = {/home/victor/Zotero/storage/VASPF8V9/Rontsis et al. - 2017 - Distributionally Ambiguous Optimization Techniques.pdf;/home/victor/Zotero/storage/RKIUBHSG/1707.html}
}

@incollection{pardo-iguzquiza_corrected_2014,
  address = {{Berlin, Heidelberg}},
  title = {Corrected {{Kriging Update Formulae}} for {{Batch}}-{{Sequential Data Assimilation}}},
  isbn = {978-3-642-32407-9 978-3-642-32408-6},
  language = {en},
  booktitle = {Mathematics of {{Planet Earth}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Chevalier, Cl{\'e}ment and Ginsbourger, David and Emery, Xavier},
  editor = {{Pardo-Ig{\'u}zquiza}, Eulogio and {Guardiola-Albert}, Carolina and Heredia, Javier and {Moreno-Merino}, Luis and Dur{\'a}n, Juan Jos{\'e} and {Vargas-Guzm{\'a}n}, Jose Antonio},
  year = {2014},
  pages = {119-122},
  file = {/home/victor/Zotero/storage/LYXH8TW3/Chevalier et al. - 2014 - Corrected Kriging Update Formulae for Batch-Sequen.pdf},
  doi = {10.1007/978-3-642-32408-6_29}
}

@article{pronzato_minimax_2017,
  title = {Minimax and Maximin Space-Filling Designs: Some Properties and Methods for Construction},
  abstract = {A few properties of minimax and maximin optimal designs in a compact subset of Rd are presented, and connections with other space-filling constructions are indicated. Several methods are given for the evaluation of the minimax-distance (or dispersion) criterion for a given n-point design. Various optimisation methods are proposed and their limitations, in particular in terms of dimension d, are indicated. A large majority of the results presented are not new, but their collection in a single document containing a respectable bibliography will hopefully be useful to the reader.},
  language = {en},
  author = {Pronzato, Luc},
  year = {2017},
  pages = {31},
  file = {/home/victor/Zotero/storage/7APNG8N5/Pronzato - 2017 - Minimax and maximin space-filling designs some pr.pdf}
}

@article{russo_tutorial_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.02038},
  primaryClass = {cs},
  title = {A {{Tutorial}} on {{Thompson Sampling}}},
  abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, dynamic pricing, recommendation, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms.},
  journal = {arXiv:1707.02038 [cs]},
  author = {Russo, Daniel and Van Roy, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
  month = jul,
  year = {2017},
  keywords = {Computer Science - Machine Learning},
  file = {/home/victor/Zotero/storage/5YEIEJDI/Russo et al. - 2017 - A Tutorial on Thompson Sampling.pdf;/home/victor/Zotero/storage/FNS6338N/1707.html}
}

@article{srinivas_gaussian_2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0912.3995},
  title = {Gaussian {{Process Optimization}} in the {{Bandit Setting}}: {{No Regret}} and {{Experimental Design}}},
  volume = {58},
  issn = {0018-9448, 1557-9654},
  shorttitle = {Gaussian {{Process Optimization}} in the {{Bandit Setting}}},
  abstract = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
  language = {en},
  number = {5},
  journal = {IEEE Transactions on Information Theory},
  doi = {10.1109/TIT.2011.2182033},
  author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M. and Seeger, Matthias},
  month = may,
  year = {2012},
  keywords = {Computer Science - Machine Learning},
  pages = {3250-3265},
  file = {/home/victor/Zotero/storage/5S6ZESFE/Srinivas et al. - 2012 - Gaussian Process Optimization in the Bandit Settin.pdf}
}

@article{honnorat_identification_2010,
  title = {Identification of Equivalent Topography in an Open Channel Flow Using {{Lagrangian}} Data Assimilation},
  volume = {13},
  issn = {1432-9360, 1433-0369},
  abstract = {We present a Lagrangian data assimilation experiment in an open channel flow above a broad-crested weir. The observations consist of trajectories of particles transported by the flow and extracted from a video film, in addition to classical water level measurements. However, the presence of vertical recirculations on both sides of the weir actually conducts to the identification of an equivalent topography corresponding to the lower limit of a surface jet. In addition, results on the identification of the Manning coefficient may allow to detect the presence of bottom reciruclations.},
  language = {en},
  number = {3},
  journal = {Computing and Visualization in Science},
  doi = {10.1007/s00791-009-0130-8},
  author = {Honnorat, Marc and Monnier, J{\'e}r{\^o}me and Rivi{\`e}re, Nicolas and Huot, {\'E}tienne and Le Dimet, Fran{\c c}ois-Xavier},
  month = mar,
  year = {2010},
  pages = {111-119},
  file = {/home/victor/Zotero/storage/QBA5CTZ4/Honnorat et al. - 2010 - Identification of equivalent topography in an open.pdf}
}

@article{cooper_improving_nodate,
  title = {Improving {{Inundation Forecasting}} Using {{Data Assimilation}}},
  abstract = {Fluvial flooding is a costly problem in the UK and worldwide. Real-time accurate inundation forecasting can help to reduce the damage caused by inundation events by alerting people to take necessary mitigating actions. This work is part of an effort to improve inundation forecasting using data assimilation (DA). DA is a method for combining a numerical model of a system with observations in order to best estimate the current state of the system. A river inundation model has been developed using numerical implementation of the shallow water equations with an inflow source term added; the model and implementation of the source term are described here. The model has then been used with idealised river valley topographies in order to investigate the sensitivities of the system to model parameters describing the effect of friction between water and the river channel, and the effect of the topography slope at the downstream boundary. Initial DA experiments using an ensemble Kalman Filter are also described. Identical twin experiments show that the DA as implemented in this domain can correct the water levels at the time of the observations, and that more observations lead to a better correction. However, by the time of the next observations very similar water levels are predicted, regardless of the number of observations used in the assimilation. This implies that the effective time for observations in this system is small compared to the time between observations.},
  language = {en},
  author = {Cooper, E S and Dance, S L and Nichols, N K and Smith, P J and {Garcia-Pintado}, J},
  pages = {20},
  file = {/home/victor/Zotero/storage/5UJ8F2JJ/Cooper et al. - Improving Inundation Forecasting using Data Assimi.pdf}
}

@incollection{d._bates_uncertainty_2014,
  title = {Uncertainty in {{Flood Inundation Modelling}}},
  isbn = {978-1-84816-270-9},
  author = {D. Bates, Paul and Pappenberger, Florian and Romanowicz, Renata},
  month = mar,
  year = {2014},
  pages = {232-269},
  file = {/home/victor/Zotero/storage/BQGH65W8/D. Bates et al. - 2014 - Uncertainty in Flood Inundation Modelling.pdf},
  doi = {10.1142/9781848162716_0010}
}

@misc{noauthor_airsea_nodate,
  title = {{{AIRSEA}} \textendash{} {{Mathematics}} and Computing Applied to Oceanic and Atmospheric Flows},
  language = {en-US},
  file = {/home/victor/Zotero/storage/MC3GRPCQ/en.html}
}

@incollection{rasmussen_gaussian_2004,
  address = {{Berlin, Heidelberg}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Gaussian {{Processes}} in {{Machine Learning}}},
  isbn = {978-3-540-28650-9},
  abstract = {We give a basic introduction to Gaussian Process regression models. We focus on understanding the role of the stochastic process and how it is used to define a distribution over functions. We present the simple equations for incorporating training data and examine how to learn the hyperparameters using the marginal likelihood. We explain the practical advantages of Gaussian Process and end with conclusions and a look at the current trends in GP work.},
  language = {en},
  booktitle = {Advanced {{Lectures}} on {{Machine Learning}}: {{ML Summer Schools}} 2003, {{Canberra}}, {{Australia}}, {{February}} 2 - 14, 2003, {{T{\"u}bingen}}, {{Germany}}, {{August}} 4 - 16, 2003, {{Revised Lectures}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Rasmussen, Carl Edward},
  editor = {Bousquet, Olivier and {von Luxburg}, Ulrike and R{\"a}tsch, Gunnar},
  year = {2004},
  keywords = {Covariance Function,Gaussian Process,Joint Gaussian Distribution,Marginal Likelihood,Posterior Variance},
  pages = {63-71},
  doi = {10.1007/978-3-540-28650-9_4}
}

@book{rasmussen_gaussian_2006,
  address = {{Cambridge, Mass}},
  series = {Adaptive Computation and Machine Learning},
  title = {Gaussian Processes for Machine Learning},
  isbn = {978-0-262-18253-9},
  lccn = {QA274.4 .R37 2006},
  language = {en},
  publisher = {{MIT Press}},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  keywords = {Gaussian processes,Data processing,Machine learning,Mathematical models},
  file = {/home/victor/Zotero/storage/NP79YRUY/Rasmussen et Williams - 2006 - Gaussian processes for machine learning.pdf},
  note = {OCLC: ocm61285753}
}

@article{beland_bayesian_nodate,
  title = {Bayesian {{Optimization Under Uncertainty}}},
  abstract = {We consider the problem of robust optimization, where it is sought to design a system such that it sustains a specified measure of performance under uncertainty. This problem is challenging since modeling a complex system under uncertainty can be expensive and for most real-world problems robust optimization will not be computationally viable. In this paper, we propose a Bayesian methodology to efficiently solve a class of robust optimization problems that arise in engineering design under uncertainty. The central idea is to use Gaussian process models of loss functions (or robustness metrics) together with appropriate acquisition functions to guide the search for a robust optimal solution. Numerical studies on a test problem are presented to demonstrate the efficacy of the proposed approach.},
  language = {en},
  author = {Beland, Justin J and Nair, Prasanth B},
  pages = {5},
  file = {/home/victor/Zotero/storage/VJ4T9MEE/Beland et Nair - Bayesian Optimization Under Uncertainty.pdf}
}

@article{vazquez_new_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1408.4622},
  primaryClass = {cs, math, stat},
  title = {A New Integral Loss Function for {{Bayesian}} Optimization},
  abstract = {We consider the problem of maximizing a real-valued continuous function f using a Bayesian approach. Since the early work of Jonas Mockus and Antanas Z\textasciicaron{} ilinskas in the 70's, the problem of optimization is usually formulated by considering the loss function max f - Mn (where Mn denotes the best function value observed after n evaluations of f ). This loss function puts emphasis on the value of the maximum, at the expense of the location of the maximizer. In the special case of a one-step Bayes-optimal strategy, it leads to the classical Expected Improvement (EI) sampling criterion. This is a special case of a Stepwise Uncertainty Reduction (SUR) strategy, where the risk associated to a certain uncertainty measure (here, the expected loss) on the quantity of interest is minimized at each step of the algorithm. In this article, assuming that f is defined over a measure space (X, {$\lambda$}), we propose to consider instead the integral loss function X( f - Mn)+ d{$\lambda$}, and we show that this leads, in the case of a Gaussian process prior, to a new numerically tractable sampling criterion that we call EI2 (for Expected Integrated Expected Improvement). A numerical experiment illustrates that a SUR strategy based on this new sampling criterion reduces the error on both the value and the location of the maximizer faster than the EI-based strategy.},
  language = {en},
  journal = {arXiv:1408.4622 [cs, math, stat]},
  author = {Vazquez, Emmanuel and Bect, Julien},
  month = aug,
  year = {2014},
  keywords = {Statistics - Machine Learning,Statistics - Computation,Mathematics - Optimization and Control,Computer Science - Machine Learning},
  file = {/home/victor/Zotero/storage/TJXDPGCX/Vazquez et Bect - 2014 - A new integral loss function for Bayesian optimiza.pdf}
}

@book{dimet_variational_1988,
  title = {Variational and {{Optimization Methods}} in {{Meteorology}}: {{A Review}}},
  shorttitle = {Variational and {{Optimization Methods}} in {{Meteorology}}},
  abstract = {Recent advances in variational and optimization methods applied to increasingly complex numerical weather prediction models with larger numbers of degrees of freedom mandate to take a perspective view of past and recent developments in this field, and present a view of the state of art in the field. Variational methods attempt to achieve a best fit between data and model subject to some `a priori' criteria \textendash{} in view of resolving the undeterminancy problem between the size of the model and the respective number of data required for its satisfactory solution. This review paper presents in a synthesized way the combined views of the authors as to the state of the art of variational and optimization methods in meteorology. Issues discussed include topics of variational analysis, variational initialization, optimal control techniques, variational methods applied for numerical purposes and constrained adjustment, and finally how some of the variational and optimization methods discussed in the review relate to each other.},
  author = {Dimet, F. X. Le and Navon, I. M.},
  year = {1988},
  file = {/home/victor/Zotero/storage/W6FNUMJV/Dimet et Navon - 1988 - Variational and Optimization Methods in Meteorolog.pdf;/home/victor/Zotero/storage/TW3ZC5FN/summary.html}
}

@article{basu_analysis_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.06808},
  primaryClass = {stat},
  title = {Analysis of {{Thompson Sampling}} for {{Gaussian Process Optimization}} in the {{Bandit Setting}}},
  abstract = {We consider the global optimization of a function over a continuous domain. At every evaluation attempt, we can observe the function at a chosen point in the domain and we reap the reward of the value observed. We assume that drawing these observations are expensive and noisy. We frame it as a continuum-armed bandit problem with a Gaussian Process prior on the function. In this regime, most algorithms have been developed to minimize some form of regret. Contrary to this popular norm, in this paper, we study the convergence of the sequential point \$\textbackslash{}boldsymbol\{x\}\^t\$ to the global optimizer \$\textbackslash{}boldsymbol\{x\}\^*\$ for the Thompson Sampling approach. Under some assumptions and regularity conditions, we show an exponential rate of convergence to the true optimal.},
  language = {en},
  journal = {arXiv:1705.06808 [stat]},
  author = {Basu, Kinjal and Ghosh, Souvik},
  month = may,
  year = {2017},
  keywords = {Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/victor/Zotero/storage/VCV7B7YA/Basu et Ghosh - 2017 - Analysis of Thompson Sampling for Gaussian Process.pdf}
}

@misc{hartnett_what_nodate,
  title = {What {{Makes}} the {{Hardest Equations}} in {{Physics So Difficult}}?},
  abstract = {The Navier-Stokes equations describe simple, everyday phenomena, like water flowing from a garden hose, yet they provide a million-dollar mathematical challenge},
  journal = {Quanta Magazine},
  howpublished = {https://www.quantamagazine.org/what-makes-the-hardest-equations-in-physics-so-difficult-20180116/},
  author = {Hartnett, ByKevin},
  file = {/home/victor/Zotero/storage/XJ89CPQF/what-makes-the-hardest-equations-in-physics-so-difficult-20180116.html}
}

@book{booker_rigorous_1998,
  title = {A {{Rigorous Framework}} for {{Optimization}} of {{Expensive Functions}} by {{Surrogates}}},
  abstract = {The goal of the research reported here is to develop rigorous optimization algorithms to apply to some engineering design problems for which direct application of traditional optimization approaches is not practical. This paper presents and analyzes a framework for generating a sequence of approximations to the objective function and managing the use of these approximations as surrogates for optimization. The result is to obtain convergence to a minimizer of an expensive objective function subject to simple constraints. The approach is widely applicable because it does not require, or even explicitly approximate, derivatives of the objective. Numerical results are presented for a 31-variable helicopter rotor blade design example and for a standard optimization test example.},
  author = {Booker, Andrew J. and Jr, J. E. Dennis and Frank, Paul D. and Serafini, David B. and Torczon, Virginia and Trosset, Michael W.},
  year = {1998},
  file = {/home/victor/Zotero/storage/A23KJ6LH/Booker et al. - 1998 - A Rigorous Framework for Optimization of Expensive.pdf;/home/victor/Zotero/storage/YPKX7WD3/summary.html}
}

@article{malherbe_global_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.02628},
  primaryClass = {stat},
  title = {Global Optimization of {{Lipschitz}} Functions},
  abstract = {The goal of the paper is to design sequential strategies which lead to efficient optimization of an unknown function under the only assumption that it has a finite Lipschitz constant. We first identify sufficient conditions for the consistency of generic sequential algorithms and formulate the expected minimax rate for their performance. We introduce and analyze a first algorithm called LIPO which assumes the Lipschitz constant to be known. Consistency, minimax rates for LIPO are proved, as well as fast rates under an additional H\textasciidieresis{}older like condition. An adaptive version of LIPO is also introduced for the more realistic setup where the Lipschitz constant is unknown and has to be estimated along with the optimization. Similar theoretical guarantees are shown to hold for the adaptive LIPO algorithm and a numerical assessment is provided at the end of the paper to illustrate the potential of this strategy with respect to state-of-the-art methods over typical benchmark problems for global optimization.},
  language = {en},
  journal = {arXiv:1703.02628 [stat]},
  author = {Malherbe, C{\'e}dric and Vayatis, Nicolas},
  month = mar,
  year = {2017},
  keywords = {Statistics - Machine Learning},
  file = {/home/victor/Zotero/storage/R7XXF9DC/Malherbe et Vayatis - 2017 - Global optimization of Lipschitz functions.pdf}
}

@article{raynaud_vacumm_2014,
  title = {{{VACUMM}} - {{A Python}} Library for Ocean Science},
  abstract = {VACUMM is an open-source Python library for processing data from observations and numerical models. The library is now used for several years in research and operational contexts, for instance for producing figures and reports, validating models, converting data, or making simple or advanced diagnostics. In this paper, we introduce how the library is built, and we present two applications of its use: one in an perational context and one in a research context.},
  journal = {Mercator-Ocean Newsletter},
  author = {Raynaud, Stephane and Charria, Guillaume and Wilkins, Jonathan and Garnier, Val{\'e}rie and Garreau, P and Theetten, S{\'e}bastien},
  month = apr,
  year = {2014},
  pages = {99},
  file = {/home/victor/Zotero/storage/CNG8K37B/Raynaud et al. - 2014 - VACUMM - A Python library for ocean science.pdf}
}

@article{hogan_point--set_1973,
  title = {Point-to-{{Set Maps}} in {{Mathematical Programming}}},
  volume = {15},
  issn = {0036-1445},
  abstract = {Properties of point-to-set maps are studied from an elementary viewpoint oriented toward applications in mathematical programming. A number of different definitions and results are compared and integrated. Conditions establishing continuity of extremal value functions and properties of maps determined by inequalities are included.},
  number = {3},
  journal = {SIAM Review},
  author = {Hogan, William W.},
  year = {1973},
  pages = {591-603}
}

@article{mayer_diversity_2016,
  title = {Diversity of Immune Strategies Explained by Adaptation to Pathogen Statistics},
  copyright = {\textcopyright{}  . Freely available online through the PNAS open access option.},
  issn = {0027-8424, 1091-6490},
  abstract = {Biological organisms have evolved a wide range of immune mechanisms to defend themselves against pathogens. Beyond molecular details, these mechanisms differ in how protection is acquired, processed, and passed on to subsequent generations\textemdash{}differences that may be essential to long-term survival. Here, we introduce a mathematical framework to compare the long-term adaptation of populations as a function of the pathogen dynamics that they experience and of the immune strategy that they adopt. We find that the two key determinants of an optimal immune strategy are the frequency and the characteristic timescale of the pathogens. Depending on these two parameters, our framework identifies distinct modes of immunity, including adaptive, innate, bet-hedging, and CRISPR-like immunities, which recapitulate the diversity of natural immune systems.},
  language = {en},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1600663113},
  author = {Mayer, Andreas and Mora, Thierry and Rivoire, Olivier and Walczak, Aleksandra M.},
  month = jul,
  year = {2016},
  keywords = {adaptive immunity,bet hedging,CRISPR immunity,evolution of immunity,immune systems},
  pages = {201600663},
  file = {/home/victor/Zotero/storage/STEVNHSJ/Mayer et al. - 2016 - Diversity of immune strategies explained by adapta.pdf;/home/victor/Zotero/storage/Q467KF27/1600663113.html},
  pmid = {27432970}
}

@article{erickson_comparison_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.03157},
  primaryClass = {stat},
  title = {Comparison of {{Gaussian}} Process Modeling Software},
  abstract = {Gaussian process fitting, or kriging, is often used to create a model from a set of data. Many available software packages do this, but we show that very different results can be obtained from different packages even when using the same data and model. We describe the parameterization, features, and optimization used by eight different fitting packages that run on four different platforms. We then compare these eight packages using various data functions and data sets, revealing that there are stark differences between the packages. In addition to comparing the prediction accuracy, the predictive variance\textemdash{}which is important for evaluating precision of predictions and is often used in stopping criteria\textemdash{}is also evaluated.},
  language = {en},
  journal = {arXiv:1710.03157 [stat]},
  author = {Erickson, Collin B. and Ankenman, Bruce E. and Sanchez, Susan M.},
  month = oct,
  year = {2017},
  keywords = {Statistics - Computation},
  file = {/home/victor/Zotero/storage/H8N6PEJV/Erickson et al. - 2017 - Comparison of Gaussian process modeling software.pdf}
}

@article{picheny_benchmark_2013,
  title = {A Benchmark of Kriging-Based Infill Criteria for Noisy Optimization},
  volume = {48},
  issn = {1615-147X, 1615-1488},
  abstract = {Responses of many real-world problems can only be evaluated perturbed by noise. In order to make an efficient optimization of these problems possible, intelligent optimization strategies successfully coping with noisy evaluations are required. In this article, a comprehensive comparison of existing kriging-based methods for the optimization of noisy functions is provided. Ten methods are described using a unified formalism, and compared on analytical benchmark problems with different configurations (noise level, maximum number of observations, initial number of observations). It is found that the optimal method depends on the optimization problem, even though some criteria are consistently more efficient than others.},
  language = {en},
  number = {3},
  journal = {Structural and Multidisciplinary Optimization},
  doi = {10.1007/s00158-013-0919-4},
  author = {Picheny, Victor and Wagner, Tobias and Ginsbourger, David},
  month = sep,
  year = {2013},
  pages = {607-626},
  file = {/home/victor/Zotero/storage/B65I8QBM/Picheny et al. - 2013 - A benchmark of kriging-based infill criteria for n.pdf}
}

@article{feinberg_chaospy_2015,
  title = {Chaospy: {{An}} Open Source Tool for Designing Methods of Uncertainty Quantification},
  volume = {11},
  issn = {1877-7503},
  shorttitle = {Chaospy},
  abstract = {The paper describes the philosophy, design, functionality, and usage of the Python software toolbox Chaospy for performing uncertainty quantification via polynomial chaos expansions and Monte Carlo simulation. The paper compares Chaospy to similar packages and demonstrates a stronger focus on defining reusable software building blocks that can easily be assembled to construct new, tailored algorithms for uncertainty quantification. For example, a Chaospy user can in a few lines of high-level computer code define custom distributions, polynomials, integration rules, sampling schemes, and statistical metrics for uncertainty analysis. In addition, the software introduces some novel methodological advances, like a framework for computing Rosenblatt transformations and a new approach for creating polynomial chaos expansions with dependent stochastic variables.},
  journal = {Journal of Computational Science},
  doi = {10.1016/j.jocs.2015.08.008},
  author = {Feinberg, Jonathan and Langtangen, Hans Petter},
  month = nov,
  year = {2015},
  keywords = {Monte Carlo simulation,Polynomial chaos expansions,Python package,Rosenblatt transformations,Uncertainty quantification},
  pages = {46-57},
  file = {/home/victor/Zotero/storage/F5UT4FSX/Feinberg et Langtangen - 2015 - Chaospy An open source tool for designing methods.pdf;/home/victor/Zotero/storage/KZ9YB9SP/Feinberg et Langtangen - 2015 - Chaospy An open source tool for designing methods.pdf;/home/victor/Zotero/storage/CFEA9WQX/S1877750315300119.html;/home/victor/Zotero/storage/EYK3LJHA/S1877750315300119.html}
}

@article{homan_output-space_nodate,
  title = {Output-{{Space Predictive Entropy Search}} for {{Flexible Global Optimization}}},
  language = {en},
  author = {Hoffman, Matthew W and Ghahramani, Zoubin},
  pages = {5},
  file = {/home/victor/Zotero/storage/IGLU6SSE/Hoﬀman et Ghahramani - Output-Space Predictive Entropy Search for Flexibl.pdf}
}

@inproceedings{eldred_formulations_2002,
  address = {{Atlanta, Georgia}},
  title = {Formulations for {{Surrogate}}-{{Based Optimization Under Uncertainty}}},
  isbn = {978-1-62410-120-5},
  abstract = {In this paper, several formulations for optimization under uncertainty are presented. In addition to the direct nesting of uncertainty quantification within optimization, formulations are presented for surrogate-based optimization under uncertainty in which the surrogate model appears at the optimization level, at the uncertainty quantification level, or at both levels. These surrogate models encompass both data fit and hierarchical surrogates. The DAKOTA software framework is used to provide the foundation for prototyping and initial benchmarking of these formulations. A critical component is the extension of algorithmic techniques for deterministic surrogate-based optimization to these surrogate-based optimization under uncertainty formulations. This involves the use of sequential trust regionbased approaches to manage the extent of the approximations and verify the approximate optima. Two analytic test problems and one engineering problem are solved using the different methodologies in order to compare their relative merits. Results show that surrogate-based optimization under uncertainty formulations show promise both in reducing the number of function evaluations required and in mitigating the effects of nonsmooth response variations.},
  language = {en},
  booktitle = {9th {{AIAA}}/{{ISSMO Symposium}} on {{Multidisciplinary Analysis}} and {{Optimization}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/6.2002-5585},
  author = {Eldred, Michael and Giunta, Anthony and Wojtkiewicz, Steven and Trucano, Timothy},
  month = sep,
  year = {2002},
  file = {/home/victor/Zotero/storage/5KF3T7LA/Eldred et al. - 2002 - Formulations for Surrogate-Based Optimization Unde.pdf}
}

@misc{noauthor_[1209.2736]_nodate,
  title = {[1209.2736] {{The Ensemble Kalman Filter}} for {{Inverse Problems}}},
  howpublished = {https://arxiv.org/abs/1209.2736},
  file = {/home/victor/Zotero/storage/YUSX97KI/1209.html}
}

@inproceedings{miranda_adjoint-based_2016,
  address = {{Crete Island, Greece}},
  title = {Adjoint-Based {{Robust Optimization}} Using {{Polynomial Chaos Expansions}}},
  isbn = {978-618-82844-0-1},
  abstract = {Adjoint methods are nowadays widely used to efficiently perform optimization for problems with a large number of design variables. However, in reality, the problem at hand might be subjected to uncertainties in the operational conditions or, in case of optimizing geometries, the design variables itself might be uncertain due to manufacturing tolerances. For such applications, the optimum obtained using deterministic methods might be very sensitive to small variations in the uncertainties, i.e. it lacks robustness. In a robust optimization, the uncertainties are taken directly into account during the optimization process by introducing, next to the mean objective, its variance as a second objective. This implies that, when using gradient based optimization methods, the gradients of both objectives (mean and variance) must be known. In this work the Polynomial Chaos Expansion (PCE) is used in combination with adjoint methods to efficiently obtain both gradients. A non intrusive, regression based PCE is used, requiring a new adjoint solution for each sampling point in order to build the PCE of the gradient. A PCE for the objective is also built (at no extra cost) in order to compute the gradient of the variance.},
  language = {en},
  booktitle = {Proceedings of the {{VII European Congress}} on {{Computational Methods}} in {{Applied Sciences}} and {{Engineering}} ({{ECCOMAS Congress}} 2016)},
  publisher = {{Institute of Structural Analysis and Antiseismic Research School of Civil Engineering National Technical University of Athens (NTUA) Greece}},
  doi = {10.7712/100016.2418.10873},
  author = {Miranda, Joao and Kumar, Dinesh and Lacor, Chris},
  year = {2016},
  pages = {8351-8364},
  file = {/home/victor/Zotero/storage/7SRD672D/Miranda et al. - 2016 - ADJOINT-BASED ROBUST OPTIMIZATION USING POLYNOMIAL.pdf}
}

@inproceedings{gu_multi-parametric_2015,
  title = {Multi-Parametric Uncertainty Quantification with a Hybrid {{Monte}}-{{Carlo}} / Polynomial Chaos Expansion {{FDTD}} Method},
  abstract = {The increasing complexity of microwave structures necessitates the quantification and analysis of fabrication process uncertainties that directly impact their performance, as a means of ensuring performance robustness. A particular technique that has emerged as a promising alternative to the time-consuming Monte-Carlo method for such analyses, is the polynomial chaos expansion (PCE). However, PCE-based methods suffer from a rapid increase in their computational cost with the number of random variables included in the analysis. This paper demonstrates that a hybrid Monte-Carlo/PCE technique can lead to the dramatic acceleration of its constituent methods, in the context of FDTD, effectively mitigating the ``curse of dimensionality'' and facilitating the multi-parametric uncertainty analysis of electromagnetic geometries.},
  booktitle = {2015 {{IEEE MTT}}-{{S International Microwave Symposium}}},
  doi = {10.1109/MWSYM.2015.7166881},
  author = {Gu, Zixi and Sarris, C. D.},
  month = may,
  year = {2015},
  keywords = {chaos,Computational electromagnetics,electromagnetic geometries,FDTD,FDTD method,finite difference time-domain analysis,finite difference time-domain method,hybrid Monte Carlo-PCE technique,hybrid Monte Carlo-polynomial chaos expansion,microwave materials,microwave structures,Monte Carlo method,Monte Carlo methods,multiparametric uncertainty analysis,multiparametric uncertainty quantification,PCE-based methods,polynomials,statistical analysis},
  pages = {1-3},
  file = {/home/victor/Zotero/storage/JWLRK5J5/7166881.html}
}

@misc{noauthor_trappler_nodate,
  title = {{{TRAPPLER Victor}} / {{These}}},
  abstract = {Gitlab at INRIA},
  language = {en},
  journal = {GitLab},
  howpublished = {https://gitlab.inria.fr/vtrapple/These},
  file = {/home/victor/Zotero/storage/MHBSY49A/These.html}
}

@misc{noauthor_optimization_nodate,
  title = {Optimization under Uncertainty: State-of-the-Art and Opportunities - {{ScienceDirect}}},
  howpublished = {https://www.sciencedirect.com/science/article/pii/S0098135403002369},
  file = {/home/victor/Zotero/storage/RKSNRNZE/S0098135403002369.html}
}

@incollection{gupta_unified_2016,
  title = {A {{Unified Framework}} for {{Optimization Under Uncertainty}}},
  isbn = {978-0-9843378-9-7},
  language = {en},
  booktitle = {Optimization {{Challenges}} in {{Complex}}, {{Networked}} and {{Risky Systems}}},
  publisher = {{INFORMS}},
  author = {Powell, Warren B.},
  editor = {Gupta, Aparna and Capponi, Agostino and Smith, J. Cole and Greenberg, Harvey J.},
  month = oct,
  year = {2016},
  pages = {45-83},
  file = {/home/victor/Zotero/storage/B72JCMMX/Powell - 2016 - A Unified Framework for Optimization Under Uncerta.pdf},
  doi = {10.1287/educ.2016.0149}
}

@article{das_estimation_1991,
  title = {On the Estimation of Parameters of Hydraulic Models by Assimilation of Periodic Tidal Data},
  volume = {96},
  issn = {0148-0227},
  language = {en},
  number = {C8},
  journal = {Journal of Geophysical Research},
  doi = {10.1029/91JC01318},
  author = {Das, S. K. and Lardner, R. W.},
  year = {1991},
  pages = {15187},
  file = {/home/victor/Zotero/storage/GP9KM35M/Das et Lardner - 1991 - On the estimation of parameters of hydraulic model.pdf}
}

@article{das_variational_1992,
  title = {Variational Parameter Estimation for a Two-Dimensional Numerical Tidal Model},
  volume = {15},
  copyright = {Copyright \textcopyright{} 1992 John Wiley \& Sons, Ltd},
  issn = {1097-0363},
  abstract = {It is shown that the parameters in a two-dimensional (depth-averaged) numerical tidal model can be estimated accurately by assimilation of data from tide gauges. The tidal model considered is a semi-linearized one in which kinematical non-linearities are neglected but non-linear bottom friction is included. The parameters to be estimated (bottom friction coefficient and water depth) are assumed to be position-dependent and are approximated by piecewise linear interpolations between certain nodal values. The numerical scheme consists of a two-level leapfrog method. The adjoint scheme is constructed on the assumption that a certain norm of the difference between computed and observed elevations at the tide gauges should be minimized. It is shown that a satisfactory numerical minimization can be completed using either the Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton algorithm or Nash's truncated Newton algorithm. On the basis of a number of test problems, it is shown that very effective estimation of the nodal values of the parameters can be achieved provided the number of data stations is sufficiently large in relation to the number of nodes.},
  language = {en},
  number = {3},
  journal = {International Journal for Numerical Methods in Fluids},
  doi = {10.1002/fld.1650150305},
  author = {Das, S. K. and Lardner, R. W.},
  month = aug,
  year = {1992},
  keywords = {Data assimilation,Numerical tidal model,Optimal control,Parameter estimation},
  pages = {313-327},
  file = {/home/victor/Zotero/storage/IUZLL882/Das et Lardner - 1992 - Variational parameter estimation for a two-dimensi.pdf;/home/victor/Zotero/storage/DICBZ35B/fld.html}
}

@article{ibrahim_power_2015,
  title = {The {{Power Prior}}: {{Theory}} and {{Applications}}},
  volume = {34},
  issn = {0277-6715},
  shorttitle = {The {{Power Prior}}},
  abstract = {The power prior has been widely used in many applications covering a large number of disciplines. The power prior is intended to be an informative prior constructed from historical data. It has been used in clinical trials, genetics, health care, psychology, environmental health, engineering, economics, and business. It has also been applied for a wide variety of models and settings, both in the experimental design and analysis contexts. In this review article, we give an A to Z exposition of the power prior and its applications to date. We review its theoretical properties, variations in its formulation, statistical contexts for which it has been used, applications, and its advantages over other informative priors. We review models for which it has been used, including generalized linear models, survival models, and random effects models. Statistical areas where the power prior has been used include model selection, experimental design, hierarchical modeling, and conjugate priors. Prequentist properties of power priors in posterior inference are established and a simulation study is conducted to further examine the empirical performance of the posterior estimates with power priors. Real data analyses are given illustrating the power prior as well as the use of the power prior in the Bayesian design of clinical trials.},
  number = {28},
  journal = {Statistics in medicine},
  doi = {10.1002/sim.6728},
  author = {Ibrahim, Joseph G. and Chen, Ming-Hui and Gwon, Yeongjin and Chen, Fang},
  month = dec,
  year = {2015},
  pages = {3724-3749},
  file = {/home/victor/Zotero/storage/7ST5NWJ8/Ibrahim et al. - 2015 - The Power Prior Theory and Applications.pdf},
  pmid = {26346180},
  pmcid = {PMC4626399}
}

@article{robbins_stochastic_1951,
  title = {A {{Stochastic Approximation Method}}},
  volume = {22},
  issn = {0003-4851, 2168-8990},
  abstract = {Let M(x)M(x)M(x) denote the expected value at level xxx of the response to a certain experiment. M(x)M(x)M(x) is assumed to be a monotone function of xxx but is unknown to the experimenter, and it is desired to find the solution x=\texttheta{}x=\texttheta{}x = \textbackslash{}theta of the equation M(x)={$\alpha$}M(x)={$\alpha$}M(x) = \textbackslash{}alpha, where {$\alpha\alpha\backslash$}alpha is a given constant. We give a method for making successive experiments at levels x1,x2,{$\cdots$}x1,x2,{$\cdots$}x\_1,x\_2,\textbackslash{}cdots in such a way that xnxnx\_n will tend to \texttheta\texttheta\textbackslash{}theta in probability.},
  language = {EN},
  number = {3},
  journal = {The Annals of Mathematical Statistics},
  doi = {10.1214/aoms/1177729586},
  author = {Robbins, Herbert and Monro, Sutton},
  month = sep,
  year = {1951},
  pages = {400-407},
  file = {/home/victor/Zotero/storage/B6H3EKLA/Robbins et Monro - 1951 - A Stochastic Approximation Method.pdf;/home/victor/Zotero/storage/XLRD9927/1177729586.html}
}

@article{sinha_principal_1997,
  title = {The Principal Lunar Semidiurnal Tide and Its Harmonics: Baseline Solutions for {{M2}} and {{M4}} Constituents on the {{North}}-{{West European Continental Shelf}}},
  volume = {17},
  issn = {0278-4343},
  shorttitle = {The Principal Lunar Semidiurnal Tide and Its Harmonics},
  abstract = {A 2D numerical model of the shelf seas around the U.K. is used to derive the M2 tide and the M4 and M6 tidal harmonics. The accuracy of the tidal harmonics is shown to be related to the form of the dissipation. In particular a quadratic form of the bottom friction results in unrealistically high amplitudes for the M6 tide. It is shown that this effect can be reduced by use of a combination of a linearised friction tensor and quadratic friction without significantly affecting the M2 constituent. The role of diffusion in controlling the amplitude of the M4 tide is demonstrated. The 2D results provide a baseline or reference solution for further development using 3D and/or multiconstituent models. Amplitude and phase diagrams derived from the model show a progression with frequency in the North Sea and the English Channel suggestive of a transition from Kelvin wave to Poincar{\'e} wave propagation. It is shown that both the North Sea and the English Channel can be considered from the point of view of waves propagating in a closed channel in the presence of friction. For the inviscid case, the critical frequency above which Poincar{\'e} waves are able to propagate is calculated to be between the M2 and M4 frequencies for the North Sea and between the M6 and M8 frequencies for the English Channel. A dispersion relation is derived for Poincar{\'e} and Kelvin waves in the presence of friction. Both types of wave possess decaying and propagating characteristics when friction is present with wavelength and decay scale dependent on frequency and the coefficient of friction. There is no longer a sharp critical frequency, but instead a transition region where wavelength and decay scales vary relatively rapidly with frequency. The response of a semi-infinite rectangular channel to Kelvin and Poincar{\'e} wave propagation is calculated providing a partial explanation of the numerical model results. \textcopyright{} 1997 Elsevier Science Ltd},
  number = {11},
  journal = {Continental Shelf Research},
  doi = {10.1016/S0278-4343(97)00007-1},
  author = {Sinha, B. and Pingree, R. D.},
  month = sep,
  year = {1997},
  pages = {1321-1365},
  file = {/home/victor/Zotero/storage/5AIPIYRK/Sinha et Pingree - 1997 - The principal lunar semidiurnal tide and its harmo.pdf;/home/victor/Zotero/storage/J2WHXNT7/S0278434397000071.html}
}

@inproceedings{healy_retrospective_1991,
  address = {{Phoenix, AZ, USA}},
  title = {Retrospective Simulation Response Optimization},
  isbn = {978-0-7803-0181-8},
  abstract = {An approach to simulation response optimization is presented where a simulation experiment is run in such a manner as to generate optimal solutions. Once the stochastic sample path of a simulation run has been generated, it is sometimes possible to retrospectively solve a deterministic optimization problem or a closely related problem. As demonstrated with some examples from communications and manufacturing, this approach can greatly simplify both the simulation experiment and the simulation model.},
  language = {en},
  booktitle = {1991 {{Winter Simulation Conference Proceedings}}.},
  publisher = {{IEEE}},
  doi = {10.1109/WSC.1991.185703},
  author = {Healy, K. and Schruben, L.W.},
  year = {1991},
  pages = {901-906},
  file = {/home/victor/Zotero/storage/P97JS8Y7/Healy et Schruben - 1991 - Retrospective simulation response optimization.pdf}
}

@article{shapiro_asymptotic_1991,
  title = {Asymptotic Analysis of Stochastic Programs},
  volume = {30},
  issn = {0254-5330, 1572-9338},
  abstract = {In this paper we discuss a general approach to studying asymptotic properties of statistical estimators in stochastic programming. The approach is based on an extended delta method and appears to be particularly suitable for deriving asymptotics of the optimal value of stochastic programs. Asymptotic analysis of the optimal value will be presented in detail. Asymptotic properties of the corresponding optimal solutions are briefly discussed.},
  language = {en},
  number = {1},
  journal = {Annals of Operations Research},
  doi = {10.1007/BF02204815},
  author = {Shapiro, Alexander},
  month = dec,
  year = {1991},
  pages = {169-186},
  file = {/home/victor/Zotero/storage/797X5VRH/Shapiro - 1991 - Asymptotic analysis of stochastic programs.pdf}
}

@article{bertsimas_robust_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1408.4445},
  primaryClass = {math},
  title = {Robust {{Sample Average Approximation}}},
  abstract = {Sample average approximation (SAA) is a widely popular approach to data-driven decisionmaking under uncertainty. Under mild assumptions, SAA is both tractable and enjoys strong asymptotic performance guarantees. Similar guarantees, however, do not typically hold in finite samples. In this paper, we propose a modification of SAA, which we term Robust SAA, which retains SAA's tractability and asymptotic properties and, additionally, enjoys strong finite-sample performance guarantees. The key to our method is linking SAA, distributionally robust optimization, and hypothesis testing of goodness-of-fit. Beyond Robust SAA, this connection provides a unified perspective enabling us to characterize the finite sample and asymptotic guarantees of various other data-driven procedures that are based upon distributionally robust optimization. This analysis provides insight into the practical performance of these various methods in real applications. We present examples from inventory management and portfolio allocation, and demonstrate numerically that our approach outperforms other data-driven approaches in these applications.},
  language = {en},
  journal = {arXiv:1408.4445 [math]},
  author = {Bertsimas, Dimitris and Gupta, Vishal and Kallus, Nathan},
  month = aug,
  year = {2014},
  keywords = {Mathematics - Optimization and Control},
  file = {/home/victor/Zotero/storage/FUIBHXQN/Bertsimas et al. - 2014 - Robust Sample Average Approximation.pdf}
}

@article{nielsen_cramer-rao_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1301.3578},
  primaryClass = {cs, math},
  title = {Cramer-{{Rao Lower Bound}} and {{Information Geometry}}},
  abstract = {This article focuses on an important piece of work of the world renowned Indian statistician, Calyampudi Radhakrishna Rao. In 1945, C. R. Rao (25 years old then) published a pathbreaking paper, which had a profound impact on subsequent statistical research.},
  language = {en},
  journal = {arXiv:1301.3578 [cs, math]},
  author = {Nielsen, Frank},
  month = jan,
  year = {2013},
  keywords = {Computer Science - Information Theory},
  file = {/home/victor/Zotero/storage/IBXL2CD4/Nielsen - 2013 - Cramer-Rao Lower Bound and Information Geometry.pdf}
}

@article{ide_unified_1997,
  title = {Unified {{Notation}} for {{Data Assimilation}} : {{Operational}}, {{Sequential}} and {{Variational}} ({{gtSpecial IssueltData Assimilation}} in {{Meteology}} and {{Oceanography}}: {{Theory}} and {{Practice}})},
  volume = {75},
  issn = {0026-1165, 2186-9057},
  shorttitle = {Unified {{Notation}} for {{Data Assimilation}}},
  abstract = {Japan's largest platform for academic e-journals: J-STAGE is a full text database for reviewed academic papers published by Japanese societies},
  language = {en},
  number = {1B},
  journal = {Journal of the Meteorological Society of Japan. Ser. II},
  doi = {10.2151/jmsj1965.75.1B_181},
  author = {Ide, Kayo and Courtier, Philippe and Ghil, Michael and Lorenc, Andrew C.},
  month = mar,
  year = {1997},
  pages = {181-189},
  file = {/home/victor/Zotero/storage/STH2ECYW/Ide et al. - 1997 - Unified Notation for Data Assimilation  Operation.pdf;/home/victor/Zotero/storage/GD2IBDM9/_article.html}
}

@article{parzen_estimation_1962,
  title = {On Estimation of a Probability Density Function and Mode},
  volume = {33},
  number = {3},
  journal = {The annals of mathematical statistics},
  author = {Parzen, Emanuel},
  year = {1962},
  pages = {1065--1076},
  file = {/home/victor/Zotero/storage/LPM2WCGR/Parzen - 1962 - On estimation of a probability density function an.pdf;/home/victor/Zotero/storage/JAE2F2MP/2237880.html}
}

@inproceedings{juditsky_stochastic_2009,
  title = {Stochastic {{Approximation Approach}} to {{Stochastic Programming}}},
  abstract = {A basic difficulty with solving stochastic programming problems is that it requires computation of expectations given by multidimensional integrals. One approach, based on Monte Carlo sampling techniques, is to generate a reasonably large random sample and consequently to solve the constructed so-called Sample Average Approximation (SAA) problem. The other classical approach is based on Stochastic Approximation (SA) techniques. In this talk we discuss some recent advances in development of SA type numerical algorithms for solving convex stochastic programming problems. Numerical experiments show that for some classes of problems the so-called Mirror Descent SA Method can significantly outperform the SAA approach.},
  language = {en},
  booktitle = {{{ISMP}} 2009 - 20th {{International Symposium}} of {{Mathematical Programming}}},
  author = {Juditsky, Anatoli and Nemirovski, Arkadii S. and Lan, Guanghui and Shapiro, Alexander},
  month = aug,
  year = {2009},
  file = {/home/victor/Zotero/storage/D3M8UC7U/Juditsky et al. - 2009 - Stochastic Approximation Approach to Stochastic Pr.pdf;/home/victor/Zotero/storage/MAKLLQFI/hal-00981931.html}
}

@article{santner_design_nodate,
  title = {The {{Design}} and {{Analysis}} of {{Computer Experiments}}},
  language = {en},
  author = {Santner, Thomas J and Williams, Brian J and Notz, William I},
  pages = {236},
  file = {/home/victor/Zotero/storage/FM6QXR9F/Santner et al. - The Design and Analysis of Computer Experiments.pdf}
}

@article{williams_sequential_2000,
  title = {Sequential Design of Computer Experiments to Minimize Integrated Response Functions},
  journal = {Statistica Sinica},
  author = {Williams, Brian J. and Santner, Thomas J. and Notz, William I.},
  year = {2000},
  pages = {1133--1152},
  file = {/home/victor/Zotero/storage/RM4Z5QR3/Williams et al. - 2000 - Sequential design of computer experiments to minim.pdf;/home/victor/Zotero/storage/YGB5AB9Z/24306770.html}
}

@article{picheny_adaptive_2010,
  title = {Adaptive {{Designs}} of {{Experiments}} for {{Accurate Approximation}} of a {{Target Region}}},
  volume = {132},
  issn = {10500472},
  language = {en},
  number = {7},
  journal = {Journal of Mechanical Design},
  doi = {10.1115/1.4001873},
  author = {Picheny, Victor and Ginsbourger, David and Roustant, Olivier and Haftka, Raphael T. and Kim, Nam-Ho},
  year = {2010},
  pages = {071008},
  file = {/home/victor/Zotero/storage/YLSGBQYE/Picheny et al. - 2010 - Adaptive Designs of Experiments for Accurate Appro.pdf}
}

@article{vazquez_convergence_2010,
  title = {Convergence Properties of the Expected Improvement Algorithm with Fixed Mean and Covariance Functions},
  volume = {140},
  issn = {03783758},
  abstract = {This paper deals with the convergence of the expected improvement algorithm, a popular global optimization algorithm based on a Gaussian process model of the function to be optimized. The first result is that under some mild hypotheses on the covariance function k of the Gaussian process, the expected improvement algorithm produces a dense sequence of evaluation points in the search domain, when the function to be optimized is in the reproducing kernel Hilbert space generated by k. The second result states that the density property also holds for P-almost all continuous functions, where P is the (prior) probability distribution induced by the Gaussian process.},
  language = {en},
  number = {11},
  journal = {Journal of Statistical Planning and Inference},
  doi = {10.1016/j.jspi.2010.04.018},
  author = {Vazquez, Emmanuel and Bect, Julien},
  month = nov,
  year = {2010},
  pages = {3088-3095},
  file = {/home/victor/Zotero/storage/HYPX6PGX/Vazquez et Bect - 2010 - Convergence properties of the expected improvement.pdf}
}

@article{scott_optimal_1979,
  title = {On {{Optimal}} and {{Data}}-{{Based Histograms}}},
  volume = {66},
  issn = {00063444},
  abstract = {In thispaper the formulaforthe optimalhistogrambin widthis derivedwhichasymptotically minimizesthe integratedmean squared error.Monte Carlo methodsare used to verify the usefulnessofthisformulaforsmall samples. A data-based procedureforchoosingthe bin widthparameteris proposed,whichassumes a Gaussian referencestandard and requiresonly the sample size and an estimateofthe standard deviation. The sensitivityofthe procedureis investigatedusingseveral probabilitymodelswhichviolate the Gaussian assumption.},
  language = {en},
  number = {3},
  journal = {Biometrika},
  doi = {10.2307/2335182},
  author = {Scott, David W.},
  month = dec,
  year = {1979},
  pages = {605},
  file = {/home/victor/Zotero/storage/7EJWJC8N/Scott - 1979 - On Optimal and Data-Based Histograms.pdf}
}

@article{beirlant_nonparametric_1997,
  title = {Nonparametric Entropy Estimation: {{An}} Overview},
  volume = {6},
  shorttitle = {Nonparametric Entropy Estimation},
  number = {1},
  journal = {International Journal of Mathematical and Statistical Sciences},
  author = {Beirlant, Jan and Dudewicz, Edward J. and Gy{\"o}rfi, L{\'a}szl{\'o} and {Van der Meulen}, Edward C.},
  year = {1997},
  pages = {17--39}
}

@book{bucholtz_handbook_2013,
  title = {Handbook of {{Differential Entropy}}},
  isbn = {978-1-4665-8316-0 978-1-4665-8317-7},
  language = {en},
  publisher = {{Chapman and Hall/CRC}},
  author = {Bucholtz, Frank},
  month = nov,
  year = {2013},
  file = {/home/victor/Zotero/storage/6UFL6ZZI/Bucholtz - 2013 - Handbook of Differential Entropy.pdf},
  doi = {10.1201/b15991}
}

@misc{noauthor_wilks_nodate,
  title = {Wilks Estimate Quantile - {{Recherche Google}}},
  howpublished = {https://www.google.com/search?client=ubuntu\&hs=2sf\&channel=fs\&biw=1867\&bih=982\&ei=yF3hW-f3DszMgAaf5YqABA\&q=wilks+estimate+quantile\&oq=wilks+estimate+quantile\&gs\_l=psy-ab.3..33i21k1l2.193064.194150.0.194190.9.8.0.0.0.0.165.665.6j1.7.0....0...1c.1.64.psy-ab..2.7.664...33i160k1.0.x0NctscuA9I},
  file = {/home/victor/Zotero/storage/3JTKPD57/search.html}
}

@article{robert_monte_nodate,
  title = {Monte {{Carlo Statistical Methods}}},
  language = {en},
  author = {Robert, Christian P and Casella, George},
  pages = {85},
  file = {/home/victor/Zotero/storage/TQ3MFXTJ/Robert et Casella - Monte Carlo Statistical Methods.pdf}
}

@article{iooss_evaluation_nodate,
  title = {{Evaluation de quantiles de codes de calcul}},
  language = {fr},
  author = {Iooss, B and Devictor, N},
  pages = {19},
  file = {/home/victor/Zotero/storage/Y8GS47EU/Iooss et Devictor - Evaluation de quantiles de codes de calcul.pdf}
}

@article{hennig_entropy_2011,
  title = {Entropy {{Search}} for {{Information}}-{{Efficient Global Optimization}}},
  language = {en},
  author = {Hennig, Philipp and Schuler, Christian J.},
  month = dec,
  year = {2011},
  file = {/home/victor/Zotero/storage/W9KFZ9I4/Hennig et Schuler - 2011 - Entropy Search for Information-Efficient Global Op.pdf;/home/victor/Zotero/storage/XTZ68DPE/1112.html}
}

@article{vazquez_3._nodate,
  title = {3. {{Some}} Tools for the Analysis of Sequential Strategies Based on a {{Gaussian}} Process Prior},
  language = {en},
  author = {Vazquez, E},
  pages = {14},
  file = {/home/victor/Zotero/storage/FSRSMT7Y/Vazquez - 3. Some tools for the analysis of sequential strat.pdf}
}

@article{rosasco_reproducing_nodate,
  title = {Reproducing {{Kernel Hilbert Spaces}}},
  language = {en},
  author = {Rosasco, Lorenzo and Durrett, Greg},
  pages = {8},
  file = {/home/victor/Zotero/storage/Y2LFAF22/Rosasco et Durrett - Reproducing Kernel Hilbert Spaces.pdf}
}

@misc{noauthor_pdf_nodate,
  title = {({{PDF}}) {{Learning Feature}}-{{Parameter Mappings}} for {{Parameter Tuning}} via the {{Profile Expected Improvement}}},
  abstract = {PDF | The majority of algorithms can be controlled or adjusted by parameters. Their values can substantially affect the algorithms' performance. Since the manual exploration of the parameter space is tedious -- even for few parameters -- several automatic procedures for parameter...},
  language = {en},
  journal = {ResearchGate},
  howpublished = {https://www.researchgate.net/publication/280076463\_Learning\_Feature-Parameter\_Mappings\_for\_Parameter\_Tuning\_via\_the\_Profile\_Expected\_Improvement},
  file = {/home/victor/Zotero/storage/WYC2QZY3/280076463_Learning_Feature-Parameter_Mappings_for_Parameter_Tuning_via_the_Profile_Expected_Imp.html},
  doi = {http://dx.doi.org/10.1145/2739480.2754673}
}

@inproceedings{bossek_learning_2015,
  address = {{New York, NY, USA}},
  series = {{{GECCO}} '15},
  title = {Learning {{Feature}}-{{Parameter Mappings}} for {{Parameter Tuning}} via the {{Profile Expected Improvement}}},
  isbn = {978-1-4503-3472-3},
  abstract = {The majority of algorithms can be controlled or adjusted by parameters. Their values can substantially affect the algorithms' performance. Since the manual exploration of the parameter space is tedious -- even for few parameters -- several automatic procedures for parameter tuning have been proposed. Recent approaches also take into account some characteristic properties of the problem instances, frequently termed instance features. Our contribution is the proposal of a novel concept for feature-based algorithm parameter tuning, which applies an approximating surrogate model for learning the continuous feature-parameter mapping. To accomplish this, we learn a joint model of the algorithm performance based on both the algorithm parameters and the instance features. The required data is gathered using a recently proposed acquisition function for model refinement in surrogate-based optimization: the profile expected improvement. This function provides an avenue for maximizing the information required for the feature-parameter mapping, i.e., the mapping from instance features to the corresponding optimal algorithm parameters. The approach is validated by applying the tuner to exemplary evolutionary algorithms and problems, for which theoretically grounded or heuristically determined feature-parameter mappings are available.},
  booktitle = {Proceedings of the 2015 {{Annual Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  publisher = {{ACM}},
  doi = {10.1145/2739480.2754673},
  author = {Bossek, Jakob and Bischl, Bernd and Wagner, Tobias and Rudolph, G{\"u}nter},
  year = {2015},
  keywords = {evolutionary algorithms,model-based optimization,parameter tuning},
  pages = {1319--1326},
  file = {/home/victor/Zotero/storage/CSL4V53D/Bossek et al. - 2015 - Learning Feature-Parameter Mappings for Parameter .pdf}
}

@article{wiener_homogeneous_1938,
  title = {The {{Homogeneous Chaos}}},
  volume = {60},
  issn = {0002-9327},
  number = {4},
  journal = {American Journal of Mathematics},
  doi = {10.2307/2371268},
  author = {Wiener, Norbert},
  year = {1938},
  pages = {897-936}
}

@article{gotoh_robust_2018,
  title = {Robust Empirical Optimization Is Almost the Same as Mean\textendash{}Variance Optimization},
  volume = {46},
  issn = {0167-6377},
  abstract = {We formulate a distributionally robust optimization problem where the deviation of the alternative distribution is controlled by a {$\phi$}-divergence penalty in the objective, and show that a large class of these problems are essentially equivalent to a mean\textendash{}variance problem. We also show that while a ``small amount of robustness'' always reduces the in-sample expected reward, the reduction in the variance, which is a measure of sensitivity to model misspecification, is an order of magnitude larger.},
  number = {4},
  journal = {Operations Research Letters},
  doi = {10.1016/j.orl.2018.05.005},
  author = {Gotoh, Jun-ya and Kim, Michael Jong and Lim, Andrew E. B.},
  month = jul,
  year = {2018},
  keywords = {Regularization,-divergence,Bias–variance trade-off,Data-driven optimization,Mean–variance optimization,Robust empirical optimization},
  pages = {448-452},
  file = {/home/victor/Zotero/storage/57TMEQYI/Gotoh et al. - 2018 - Robust empirical optimization is almost the same a.pdf;/home/victor/Zotero/storage/RHUWPUAR/S016763771730514X.html}
}

@article{natarajan_constructing_2009,
  title = {Constructing {{Risk Measures}} from {{Uncertainty Sets}}},
  volume = {57},
  issn = {0030-364X, 1526-5463},
  abstract = {We propose a unified theory that links uncertainty sets in robust optimization to risk measures in portfolio optimization. We illustrate the correspondence between uncertainty sets and some popular risk measures in finance, and show how robust optimization can be used to generalize the concepts of these measures. We also show that by using properly defined uncertainty sets in robust optimization models, one can in fact construct coherent risk measures. Our approach to creating coherent risk measures is easy to apply in practice, and computational experiments suggest that it may lead to superior portfolio performance. Our results have implications for efficient portfolio optimization under different measures of risk.},
  language = {en},
  number = {5},
  journal = {Operations Research},
  doi = {10.1287/opre.1080.0683},
  author = {Natarajan, Karthik and Pachamanova, Dessislava and Sim, Melvyn},
  month = oct,
  year = {2009},
  pages = {1129-1141},
  file = {/home/victor/Zotero/storage/7ASYSSKY/Natarajan et al. - 2009 - Constructing Risk Measures from Uncertainty Sets.pdf}
}

@article{grodzevich_normalization_2006,
  title = {Normalization and Other Topics in Multi-Objective Optimization},
  author = {Grodzevich, Oleg and Romanko, Oleksandr},
  year = {2006},
  file = {/home/victor/Zotero/storage/FHRIYBL5/Grodzevich et Romanko - 2006 - Normalization and other topics in multi-objective .pdf}
}

@article{sunar_comparative_nodate,
  title = {A {{Comparative Study}} of {{Multiobjective Optimization Methods}} in {{Structural Design}}},
  abstract = {The computational algorithms of different multiobjective optimization techniques and their applications to structural systems are presented. The weighting, -constraint, goal programming and modified game theory methods are described along with a comparative study of the results. The conflicting nature of the objective functions is studied through two multiobjective optimization problems. Specifically, the design of a 25-bar space truss and that of a satellite with flexible appendages are considered in numerical studies. The results from the multiobjective optimization methods are evaluated in terms of a supercriterion. It is concluded that the results obtained using the goal programming and modified game theory/goal programming approaches are properly balanced yielding the best compromise in the presence of conflicting objectives.},
  language = {en},
  author = {Sunar, Mehmet and Kahraman, Ramazan},
  pages = {10},
  file = {/home/victor/Zotero/storage/CBH38ZKP/Sunar et Kahraman - A Comparative Study of Multiobjective Optimization.pdf}
}

@book{boucheron_concentration_2013,
  title = {Concentration {{Inequalities}}: {{A Nonasymptotic Theory}} of {{Independence}}},
  isbn = {978-0-19-953525-5},
  shorttitle = {Concentration {{Inequalities}}},
  language = {en},
  publisher = {{Oxford University Press}},
  author = {Boucheron, St{\'e}phane and Lugosi, G{\'a}bor and Massart, Pascal},
  month = feb,
  year = {2013},
  file = {/home/victor/Zotero/storage/7CQ8ZP24/Boucheron et al. - 2013 - Concentration Inequalities A Nonasymptotic Theory.pdf},
  doi = {10.1093/acprof:oso/9780199535255.001.0001}
}

@inproceedings{arnaud_evaluation_2010,
  title = {{{\'E}valuation d'un risque d'inondation fluviale par planification s{\'e}quentielle d'exp{\'e}riences}},
  abstract = {Nous nous int{\'e}ressons au risque d'inondation d'une zone habitable ou industrielle, situ{\'e}e {\`a} proximit{\'e} d'un fleuve. Le risque est {\'e}valu{\'e} {\`a} partir d'un mod{\`e}le de la ligne d'eau du fleuve en pr{\'e}sence d'incertitudes sur le d{\'e}bit et les caract{\'e}ristiques du lit fluvial. Comme l'{\'e}valuation du mod{\`e}le de la hauteur d'eau, pour un d{\'e}bit et des caract{\'e}ristiques du lit fix{\'e}s, est potentiellement co{\^u}teux en temps de calcul, l'estimation d'une probabilit{\'e} de d{\'e}passement de seuil ou d'un quantile de la hauteur d'eau doit en pratique {\^e}tre conduite avec un budget r{\'e}duit de simulations. Dans cet article, nous nous int{\'e}ressons sp{\'e}cifiquement {\`a} l'estimation d'un quantile et nous proposons une m{\'e}thode de planification d'exp{\'e}riences s{\'e}quentielle qui construit une approximation du mod{\`e}le par krigeage en choisissant les points d'{\'e}valuation du mod{\`e}le de mani{\`e}re {\`a} r{\'e}duire la variance d'estimation du quantile.},
  language = {fr},
  booktitle = {{42{\`e}mes Journ{\'e}es de Statistique}},
  author = {Arnaud, Aur{\'e}lie and Bect, Julien and Couplet, Mathieu and Pasanisi, Alberto and Vazquez, Emmanuel},
  year = {2010},
  file = {/home/victor/Zotero/storage/6LTDBWZX/Arnaud et al. - 2010 - Évaluation d'un risque d'inondation fluviale par p.pdf;/home/victor/Zotero/storage/BNT2E33P/inria-00494767.html}
}

@article{van_barel_robust_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.02574},
  primaryClass = {cs, math},
  title = {Robust {{Optimization}} of {{PDEs}} with {{Random Coefficients Using}} a {{Multilevel Monte Carlo Method}}},
  abstract = {This paper addresses optimization problems constrained by partial differential equations with uncertain coefficients. In particular, the robust control problem and the average control problem are considered for a tracking type cost functional with an additional penalty on the variance of the state. The expressions for the gradient and Hessian corresponding to either problem contain expected value operators. Due to the large number of uncertainties considered in our model, we suggest to evaluate these expectations using a multilevel Monte Carlo (MLMC) method. Under mild assumptions, it is shown that this results in the gradient and Hessian corresponding to the MLMC estimator of the original cost functional. Furthermore, we show that the use of certain correlated samples yields a reduction in the total number of samples required. Two optimization methods are investigated: the nonlinear conjugate gradient method and the Newton method. For both, a specific algorithm is provided that dynamically decides which and how many samples should be taken in each iteration. The cost of the optimization up to some specified tolerance \$\textbackslash{}tau\$ is shown to be proportional to the cost of a gradient evaluation with requested root mean square error \$\textbackslash{}tau\$. The algorithms are tested on a model elliptic diffusion problem with lognormal diffusion coefficient. An additional nonlinear term is also considered.},
  journal = {arXiv:1711.02574 [cs, math]},
  author = {Van Barel, Andreas and Vandewalle, Stefan},
  month = nov,
  year = {2017},
  keywords = {Mathematics - Probability,Mathematics - Optimization and Control,Computer Science - Numerical Analysis},
  file = {/home/victor/Zotero/storage/ZHZ5Z22V/Van Barel et Vandewalle - 2017 - Robust Optimization of PDEs with Random Coefficien.pdf;/home/victor/Zotero/storage/BDBZFZNI/1711.html}
}

@article{bolton_applications_2019,
  title = {Applications of {{Deep Learning}} to {{Ocean Data Inference}} and {{Sub}}-{{Grid Parameterisation}}},
  volume = {0},
  issn = {1942-2466},
  abstract = {Abstract Oceanographic observations are limited by sampling rates, while ocean models are limited by finite resolution and high viscosity and diffusion coefficients. Therefore both data from observations and ocean models lack information at small- and fast-scales. Methods are needed to either extract information, extrapolate, or up-scale existing oceanographic datasets, to account for or represent unresolved physical processes. Here we use machine learning to leverage observations and model data by predicting unresolved turbulent processes and sub-surface flow fields. As a proof-of-concept, we train convolutional neural networks on degraded-data from a high-resolution quasi-geostrophic ocean model. We demonstrate that convolutional neural networks successfully replicate the spatio-temporal variability of the sub-grid eddy momentum forcing, are capable of generalising to a range of dynamical behaviours, and can be forced to respect global momentum conservation. The training data of our convolutional neural networks can be sub-sampled to 10-20\% of the original size without a significant decrease in accuracy. We also show that the sub-surface flow field can be predicted using only information at the surface (e.g., using only satellite altimetry data). Our study indicates that data-driven approaches can be exploited to predict both sub-grid and large-scale processes, while respecting physical principles, even when data is limited to a particular region or external forcing. Our in-depth study presents evidence for the successful design of ocean eddy parameterisations for implementation in coarse-resolution climate models.},
  number = {ja},
  journal = {Journal of Advances in Modeling Earth Systems},
  doi = {10.1029/2018MS001472},
  author = {Bolton, Thomas and Zanna, Laure},
  month = jan,
  year = {2019},
  keywords = {Machine learning,Data Inference,Eddies,Oceanography,Turbulence},
  file = {/home/victor/Zotero/storage/D36QTPGZ/Bolton et Zanna - 2019 - Applications of Deep Learning to Ocean Data Infere.pdf;/home/victor/Zotero/storage/2EJ763U5/2018MS001472.html}
}

@article{augustin_trust-region_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.04156},
  primaryClass = {math},
  title = {A Trust-Region Method for Derivative-Free Nonlinear Constrained Stochastic Optimization},
  abstract = {In this work we introduce the algorithm (S)NOWPAC (Stochastic Nonlinear Optimization With Path-Augmented Constraints) for stochastic nonlinear constrained derivative-free optimization. The algorithm extends the derivative-free optimizer NOWPAC to be applicable to nonlinear stochastic programming. It is based on a trust region framework, utilizing local fully linear surrogate models combined with Gaussian process surrogates to mitigate the noise in the objective function and constraint evaluations. We show several benchmark results that demonstrate (S)NOWPAC's efficiency and highlight the accuracy of the optimal solutions found.},
  journal = {arXiv:1703.04156 [math]},
  author = {Augustin, F. and Marzouk, Y. M.},
  month = mar,
  year = {2017},
  keywords = {Mathematics - Optimization and Control,G.1.6,9080; 90C15; 90C30; 90C56; 65K05; 60G15},
  file = {/home/victor/Zotero/storage/LXXZ4ZT5/Augustin et Marzouk - 2017 - A trust-region method for derivative-free nonlinea.pdf;/home/victor/Zotero/storage/PVWW83A3/Augustin et Marzouk - 2017 - A trust-region method for derivative-free nonlinea.pdf;/home/victor/Zotero/storage/WYTGF3KZ/Augustin et Marzouk - 2017 - A trust-region method for derivative-free nonlinea.pdf;/home/victor/Zotero/storage/5W93B5GQ/1703.html}
}

@article{bertsimas_theory_2010,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1010.5445},
  primaryClass = {cs, math},
  title = {Theory and {{Applications}} of {{Robust Optimization}}},
  abstract = {In this paper we survey the primary research, both theoretical and applied, in the area of Robust Optimization (RO). Our focus is on the computational attractiveness of RO approaches, as well as the modeling power and broad applicability of the methodology. In addition to surveying prominent theoretical results of RO, we also present some recent results linking RO to adaptable models for multi-stage decision-making problems. Finally, we highlight applications of RO across a wide spectrum of domains, including finance, statistics, learning, and various areas of engineering.},
  journal = {arXiv:1010.5445 [cs, math]},
  author = {Bertsimas, Dimitris and Brown, David B. and Caramanis, Constantine},
  month = oct,
  year = {2010},
  keywords = {Mathematics - Optimization and Control,Computer Science - Computational Engineering; Finance; and Science,90C25},
  file = {/home/victor/Zotero/storage/T6QT64CY/Bertsimas et al. - 2010 - Theory and Applications of Robust Optimization.pdf;/home/victor/Zotero/storage/6DFX49PU/1010.html}
}

@article{lorenz_deterministic_1963,
  title = {Deterministic Nonperiodic Flow},
  volume = {20},
  number = {2},
  journal = {Journal of the atmospheric sciences},
  author = {Lorenz, Edward N.},
  year = {1963},
  pages = {130--141},
  file = {/home/victor/Zotero/storage/E5EJ45IY/Lorenz - 1963 - Deterministic nonperiodic flow.pdf;/home/victor/Zotero/storage/IWFXT9YF/1520-0469(1963)0200130DNF2.0.html}
}

@article{artzner_coherent_1999,
  title = {Coherent {{Measures}} of {{Risk}}},
  volume = {9},
  issn = {0960-1627, 1467-9965},
  abstract = {In this paper we study both market risks and nonmarket risks, without complete markets assumption, and discuss methods of measurement of these risks. We present and justify a set of four desirable properties for measures of risk, and call the measures satisfying these properties ``coherent.'' We examine the measures of risk provided and the related actions required by SPAN, by the SEC/NASD rules, and by quantile-based methods. We demonstrate the universality of scenario-based methods for providing coherent measures. We offer suggestions concerning the SEC method. We also suggest a method to repair the failure of subadditivity of quantile-based methods.},
  language = {en},
  number = {3},
  journal = {Mathematical Finance},
  doi = {10.1111/1467-9965.00068},
  author = {Artzner, Philippe and Delbaen, Freddy and Eber, Jean-Marc and Heath, David},
  month = jul,
  year = {1999},
  pages = {203-228},
  file = {/home/victor/Zotero/storage/2U9VWDRP/Artzner et al. - 1999 - Coherent Measures of Risk.pdf}
}

@article{kisiala_conditional_nodate,
  title = {Conditional {{Value}}-at-{{Risk}}: {{Theory}} and {{Applications}}},
  language = {en},
  author = {Kisiala, Jakob},
  pages = {96},
  file = {/home/victor/Zotero/storage/U6CSBR4I/Kisiala - Conditional Value-at-Risk Theory and Applications.pdf}
}

@techreport{rockafellar_deviation_2002,
  address = {{Rochester, NY}},
  type = {{{SSRN Scholarly Paper}}},
  title = {Deviation {{Measures}} in {{Risk Analysis}} and {{Optimization}}},
  abstract = {General deviation measures, which include standard deviation as a special case but need not be symmetric with respect to ups and downs, are defined and shown to correspond to risk measures in the sense of Artzner, Delbaen, Eber and Heath when those are applied to the difference between a random variable and its expectation, instead of to the random variable itself.   A property called expectation-boundedness of the risk measure is uncovered as essential for this correspondence.  It is shown to be satisfied by conditional value-at-risk and by worst-case risk, as well as various mixtures, although not by ordinary value-at-risk.},
  language = {en},
  number = {ID 365640},
  institution = {{Social Science Research Network}},
  author = {Rockafellar, R. Tyrrell and Uryasev, Stanislav P. and Zabarankin, Michael},
  month = dec,
  year = {2002},
  keywords = {coherent risk,deviation measures,risk management},
  file = {/home/victor/Zotero/storage/3RZUKHRL/Rockafellar et al. - 2002 - Deviation Measures in Risk Analysis and Optimizati.pdf;/home/victor/Zotero/storage/AACZGBUN/papers.html}
}

@article{noauthor_post-modern_2018,
  title = {Post-Modern Portfolio Theory},
  copyright = {Creative Commons Attribution-ShareAlike License},
  abstract = {Post-modern portfolio theory (or PMPT) is an extension of the traditional modern portfolio theory (MPT, which is an application of mean-variance analysis or MVA). Both theories propose how rational investors should use diversification to optimize their portfolios, and how a risky asset should be priced.},
  language = {en},
  journal = {Wikipedia},
  month = sep,
  year = {2018},
  file = {/home/victor/Zotero/storage/6SXFXTKC/index.html},
  note = {Page Version ID: 860848261}
}

@article{menetrier_oceanographie_2014,
  title = {{Oc{\'e}anographie Mar{\'e}es Support de cours}},
  language = {fr},
  author = {M{\'e}n{\'e}trier, Benjamin},
  year = {2014},
  pages = {38},
  file = {/home/victor/Zotero/storage/NXVGNP9L/Ménétrier - Océanographie Marées Support de cours.pdf}
}

@inproceedings{giunta_perspectives_2004,
  title = {Perspectives in {{Optimization Under Uncertainty}}: {{Algorithms}} and {{Applications}}},
  shorttitle = {Perspectives in {{Optimization Under Uncertainty}}},
  booktitle = {10th {{AIAA}}/{{ISSMO Multidisciplinary Analysis}} and {{Optimization Conference}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {Giunta, Anthony and Eldred, Michael and Swiler, Laura and Trucano, Timothy and Wojtkiewicz, Steven},
  year = {2004},
  file = {/home/victor/Zotero/storage/CJN956LS/Giunta et al. - Perspectives in Optimization Under Uncertainty Al.pdf;/home/victor/Zotero/storage/IW3DSWB8/6.html},
  doi = {10.2514/6.2004-4451}
}

@article{cook_horsetail_2018,
  title = {Horsetail Matching: A Flexible Approach to Optimization under Uncertainty},
  volume = {50},
  issn = {0305-215X, 1029-0273},
  shorttitle = {Horsetail Matching},
  abstract = {It is important to design engineering systems to be robust with respect to uncertainties in the design process. Often, this is done by considering statistical moments, but over-reliance on statistical moments when formulating a robust optimization can produce designs that are stochastically dominated by other feasible designs. This article instead proposes a formulation for optimization under uncertainty that minimizes the difference between a design's cumulative distribution function and a target. A standard target is proposed that produces stochastically non-dominated designs, but the formulation also offers enough flexibility to recover existing approaches for robust optimization. A numerical implementation is developed that employs kernels to give a differentiable objective function. The method is applied to algebraic test problems and a robust transonic airfoil design problem where it is compared to multi-objective, weighted-sum and density matching approaches to robust optimization; several advantages over these existing methods are demonstrated.},
  language = {en},
  number = {4},
  journal = {Engineering Optimization},
  doi = {10.1080/0305215X.2017.1327581},
  author = {Cook, L. W. and Jarrett, J. P.},
  month = apr,
  year = {2018},
  pages = {549-567},
  file = {/home/victor/Zotero/storage/QZQCNVQ7/Cook et Jarrett - 2018 - Horsetail matching a flexible approach to optimiz.pdf}
}

@article{cook_extending_2017,
  title = {Extending {{Horsetail Matching}} for {{Optimization Under Probabilistic}}, {{Interval}}, and {{Mixed Uncertainties}}},
  volume = {56},
  issn = {0001-1452},
  abstract = {This paper presents a new approach for optimization under uncertainty in the presence of probabilistic, interval, and mixed uncertainties, avoiding the need to specify probability distributions on uncertain parameters when such information is not readily available. Existing approaches for optimization under these types of uncertainty mostly rely on treating combinations of statistical moments as separate objectives, but this can give rise to stochastically dominated designs. Here, horsetail matching is extended for use with these types of uncertainties to overcome some of the limitations of existing approaches. The formulation delivers a single, differentiable metric as the objective function for optimization. It is demonstrated on algebraic test problems, the design of a wing using a low-fidelity coupled aerostructural code, and the aerodynamic shape optimization of a wing using computational fluid dynamics analysis.},
  number = {2},
  journal = {AIAA Journal},
  doi = {10.2514/1.J056371},
  author = {Cook, Laurence W. and Jarrett, Jerome P. and Willcox, Karen E.},
  month = oct,
  year = {2017},
  pages = {849-861},
  file = {/home/victor/Zotero/storage/ZBT9TGQK/Cook et al. - 2017 - Extending Horsetail Matching for Optimization Unde.pdf;/home/victor/Zotero/storage/SJNAGEIJ/1.html}
}

@article{seshadri_density-matching_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.7089},
  primaryClass = {math, stat},
  title = {A Density-Matching Approach for Optimization under Uncertainty},
  abstract = {Modern computers enable methods for design optimization that account for uncertainty in the system---so-called optimization under uncertainty. We propose a metric for OUU that measures the distance between a designer-specified probability density function of the system response the target and system response's density function at a given design. We study an OUU formulation that minimizes this distance metric over all designs. We discretize the objective function with numerical quadrature and approximate the response density function with a Gaussian kernel density estimate. We offer heuristics for addressing issues that arise in this formulation, and we apply the approach to a CFD-based airfoil shape optimization problem. We qualitatively compare the density-matching approach to a multi-objective robust design optimization to gain insight into the method.},
  journal = {arXiv:1409.7089 [math, stat]},
  author = {Seshadri, Pranay and Constantine, Paul and Iaccarino, Gianluca and Parks, Geoffrey},
  month = sep,
  year = {2014},
  keywords = {Statistics - Computation,Mathematics - Optimization and Control},
  file = {/home/victor/Zotero/storage/KK6NXSQM/Seshadri et al. - 2014 - A density-matching approach for optimization under.pdf;/home/victor/Zotero/storage/XLQ37PBN/1409.html}
}

@article{petrone_robustness_2011,
  title = {Robustness Criteria in Optimization under Uncertainty},
  journal = {Evolutionary and deterministic methods for design, optimization and control (EUROGEN 2011). CIRA, Capua},
  author = {Petrone, Giovanni and Iaccarino, Gianluca and Quagliarella, D.},
  year = {2011},
  pages = {244--252},
  file = {/home/victor/Zotero/storage/BBSY2QRE/Petrone et al. - 2011 - Robustness criteria in optimization under uncertai.pdf}
}

@phdthesis{cook_effective_2018,
  type = {Thesis},
  title = {Effective {{Formulations}} of {{Optimization Under Uncertainty}} for {{Aerospace Design}}},
  copyright = {Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)},
  abstract = {Formulations of optimization under uncertainty (OUU) commonly used in 
aerospace design\textemdash{}those based on treating statistical moments of the quantity 
of interest (QOI) as separate objectives\textemdash{}can result in stochastically dominated 
designs. A stochastically dominated design is undesirable, because it is less likely 
than another design to achieve a QOI at least as good as a given value, for any 
given value. 
 
As a remedy to this limitation for the multi-objective formulation of moments, 
a novel OUU formulation is proposed\textemdash{}dominance optimization. This formulation 
seeks a set of solutions and makes use of global optimizers, so is useful for early 
stages of the design process when exploration of design space is important. 
 
Similarly, to address this limitation for the single-objective formulation of 
moments (combining moments via a weighted sum), a second novel formulation 
is proposed\textemdash{}horsetail matching. This formulation can make use of gradient- 
based local optimizers, so is useful for later stages of the design process when 
exploitation of a region of design space is important. Additionally, horsetail 
matching extends straightforwardly to different representations of uncertainty, 
and is flexible enough to emulate several existing OUU formulations. 
 
Existing multi-fidelity methods for OUU are not compatible with these novel 
formulations, so one such method\textemdash{}information reuse\textemdash{}is generalized to be 
compatible with these and other formulations. 
 
The proposed formulations, along with generalized information reuse, are 
compared to their most comparable equivalent in the current state-of-the-art 
on practical design problems: transonic aerofoil design, coupled aero-structural 
wing design, high-fidelity 3D wing design, and acoustic horn shape design. 
 
Finally, the two novel formulations are combined in a two-step design process, 
which is used to obtain a robust design in a challenging version of the acoustic horn 
design problem. Dominance optimization is given half the computational budget 
for exploration; then horsetail matching is given the other half for exploitation. 
Using exactly the same computational budget as a moment-based approach, the 
design obtained using the novel formulations is 95\% more likely to achieve a 
better QOI than the best value achievable by the moment-based design.},
  language = {en},
  school = {University of Cambridge},
  author = {Cook, Laurence William},
  month = jul,
  year = {2018},
  file = {/home/victor/Zotero/storage/8W7UKHCR/Cook - 2018 - Effective Formulations of Optimization Under Uncer.pdf;/home/victor/Zotero/storage/RWIX948G/276146.html},
  doi = {10.17863/CAM.23427}
}

@article{wolpert_no_1997,
  title = {No Free Lunch Theorems for Optimization},
  volume = {1},
  issn = {1089778X},
  abstract = {A framework is developed to explore the connection between e ective optimization algorithms and the problems they are solving. A number of \textbackslash{}no free lunch" (NFL) theorems are presented that establish that for any algorithm, any elevated performance over one class of problems is exactly paid for in performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed are time-varying optimization problems and a priori \textbackslash{}head-to-head" minimax distinctions between optimization algorithms, distinctions that can obtain despite the NFL theorems' enforcing of a type of uniformity over all algorithms.},
  language = {en},
  number = {1},
  journal = {IEEE Transactions on Evolutionary Computation},
  doi = {10.1109/4235.585893},
  author = {Wolpert, D.H. and Macready, W.G.},
  month = apr,
  year = {1997},
  pages = {67-82},
  file = {/home/victor/Zotero/storage/3WVWVEGC/Wolpert et Macready - 1997 - No free lunch theorems for optimization.pdf}
}

@article{marler_weighted_2010,
  title = {The Weighted Sum Method for Multi-Objective Optimization: New Insights},
  volume = {41},
  issn = {1615-147X, 1615-1488},
  shorttitle = {The Weighted Sum Method for Multi-Objective Optimization},
  abstract = {As a common concept in multi-objective optimization, minimizing a weighted sum constitutes an independent method as well as a component of other methods. Consequently, insight into characteristics of the weighted sum method has far reaching implications. However, despite the many published applications for this method and the literature addressing its pitfalls with respect to depicting the Pareto optimal set, there is little comprehensive discussion concerning the conceptual significance of the weights and techniques for maximizing the effectiveness of the method with respect to a priori articulation of preferences. Thus, in this paper, we investigate the fundamental significance of the weights in terms of preferences, the Pareto optimal set, and objective-function values. We determine the factors that dictate which solution point results from a particular set of weights. Fundamental deficiencies are identified in terms of a priori articulation of preferences, and guidelines are provided to help avoid blind use of the method.},
  language = {en},
  number = {6},
  journal = {Structural and Multidisciplinary Optimization},
  doi = {10.1007/s00158-009-0460-7},
  author = {Marler, R. Timothy and Arora, Jasbir S.},
  month = jun,
  year = {2010},
  pages = {853-862},
  file = {/home/victor/Zotero/storage/EJS8CI6X/Marler et Arora - 2010 - The weighted sum method for multi-objective optimi.pdf}
}

@article{oberkampf_measures_2006,
  title = {Measures of Agreement between Computation and Experiment: {{Validation}} Metrics},
  volume = {217},
  issn = {00219991},
  shorttitle = {Measures of Agreement between Computation and Experiment},
  abstract = {With the increasing role of computational modeling in engineering design, performance estimation, and safety assessment, improved methods are needed for comparing computational results and experimental measurements. Traditional methods of graphically comparing computational and experimental results, though valuable, are essentially qualitative. Computable measures are needed that can quantitatively compare computational and experimental results over a range of input, or control, variables to sharpen assessment of computational accuracy. This type of measure has been recently referred to as a validation metric. We discuss various features that we believe should be incorporated in a validation metric, as well as features that we believe should be excluded. We develop a new validation metric that is based on the statistical concept of confidence intervals. Using this fundamental concept, we construct two specific metrics: one that requires interpolation of experimental data and one that requires regression (curve fitting) of experimental data. We apply the metrics to three example problems: thermal decomposition of a polyurethane foam, a turbulent buoyant plume of helium, and compressibility effects on the growth rate of a turbulent free-shear layer. We discuss how the present metrics are easily interpretable for assessing computational model accuracy, as well as the impact of experimental measurement uncertainty on the accuracy assessment.},
  language = {en},
  number = {1},
  journal = {Journal of Computational Physics},
  doi = {10.1016/j.jcp.2006.03.037},
  author = {Oberkampf, William L. and Barone, Matthew F.},
  month = sep,
  year = {2006},
  pages = {5-36},
  file = {/home/victor/Zotero/storage/LM68SXBZ/Oberkampf et Barone - 2006 - Measures of agreement between computation and expe.pdf}
}

@article{quagliarella_optimization_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1407.4636},
  primaryClass = {cs, math},
  title = {Optimization {{Under Uncertainty Using}} the {{Generalized Inverse Distribution Function}}},
  abstract = {A framework for robust optimization under uncertainty based on the use of the generalized inverse distribution function (GIDF), also called quantile function, is here proposed. Compared to more classical approaches that rely on the usage of statistical moments as deterministic attributes that define the objectives of the optimization process, the inverse cumulative distribution function allows for the use of all the possible information available in the probabilistic domain. Furthermore, the use of a quantile based approach leads naturally to a multi-objective methodology which allows an a-posteriori selection of the candidate design based on risk/opportunity criteria defined by the designer. Finally, the error on the estimation of the objectives due to the resolution of the GIDF will be proven to be quantifiable},
  language = {en},
  journal = {arXiv:1407.4636 [cs, math]},
  author = {Quagliarella, Domenico and Petrone, Giovanni and Iaccarino, Gianluca},
  month = jul,
  year = {2014},
  keywords = {Mathematics - Optimization and Control,Computer Science - Neural and Evolutionary Computing},
  file = {/home/victor/Zotero/storage/IK2F8YTC/Quagliarella et al. - 2014 - Optimization Under Uncertainty Using the Generaliz.pdf}
}

@phdthesis{ribaud_krigeage_2018-1,
  type = {Thesis},
  title = {Krigeage Pour La Conception de Turbomachines : Grande Dimension et Optimisation Multi-Objectif Robuste},
  shorttitle = {Krigeage Pour La Conception de Turbomachines},
  abstract = {Dans le secteur de l'automobile, les turbomachines sont des machines tournantes participant au refroidissement des moteurs des voitures. Leur performance d{\'e}pend de multiples param{\`e}tres g{\'e}om{\'e}triques qui d{\'e}terminent leur forme. Cette th{\`e}se s'inscrit dans le projet ANR PEPITO r{\'e}unissant industriels et acad{\'e}miques autour de l'optimisation de ces turbomachines. L'objectif du projet est de trouver la forme du ventilateur maximisant le rendement en certains points de fonctionnement. Dans ce but, les industriels ont d{\'e}velopp{\'e} des codes CFD (computational fluid dynamics) simulant le fonctionnement de la machine. Ces codes sont tr{\`e}s co{\^u}teux en temps de calcul. Il est donc impossible d'utiliser directement le r{\'e}sultat de ces simulations pour conduire une optimisation.Par ailleurs, lors de la construction des turbomachines, on observe des perturbations sur les param{\`e}tres d'entr{\'e}e. Elles sont le reflet de fluctuations des machines de production. Les {\'e}carts observ{\'e}s sur la forme g{\'e}om{\'e}trique finale de la turbomachine peuvent provoquer une perte de performance cons{\'e}quente. Il est donc n{\'e}cessaire de prendre en compte ces perturbations et de proc{\'e}der {\`a} une optimisation robuste {\`a} ces fluctuations. Dans ce travail de th{\`e}se, nous proposons des m{\'e}thodes bas{\'e}es sur du krigeage r{\'e}pondant aux deux principales probl{\'e}matiques li{\'e}es {\`a} ce contexte de simulations co{\^u}teuses :\textbullet{}	Comment construire une bonne surface de r{\'e}ponse pour le rendement lorsqu'il y a beaucoup de param{\`e}tres g{\'e}om{\'e}triques ?\textbullet{}	Comment proc{\'e}der {\`a} une optimisation du rendement efficace tout en prenant en compte les perturbations des entr{\'e}es ?Nous r{\'e}pondons {\`a} la premi{\`e}re probl{\'e}matique en proposant plusieurs algorithmes permettant de construire un noyau de covariance pour le krigeage adapt{\'e} {\`a} la grande dimension. Ce noyau est un produit tensoriel de noyaux isotropes o{\`u} chacun de ces noyaux est li{\'e} {\`a} un sous groupe de variables d'entr{\'e}e. Ces algorithmes sont test{\'e}s sur des cas simul{\'e}s et sur une fonction r{\'e}elle. Les r{\'e}sultats montrent que l'utilisation de ce noyau permet d'am{\'e}liorer la qualit{\'e} de pr{\'e}diction en grande dimension. Concernant la seconde probl{\'e}matique, nous proposons plusieurs strat{\'e}gies it{\'e}ratives bas{\'e}es sur un co-krigeage avec d{\'e}riv{\'e}es pour conduire l'optimisation robuste. A chaque it{\'e}ration, un front de Pareto est obtenu par la minimisation de deux objectifs calcul{\'e}s {\`a} partir des pr{\'e}dictions de la fonction co{\^u}teuse. Le premier objectif repr{\'e}sente la fonction elle-m{\^e}me et le second la robustesse. Cette robustesse est quantifi{\'e}e par un crit{\`e}re estimant une variance locale et bas{\'e}e sur le d{\'e}veloppement de Taylor. Ces strat{\'e}gies sont compar{\'e}es sur deux cas tests en petite et plus grande dimension. Les r{\'e}sultats montrent que les meilleures strat{\'e}gies permettent bien de trouver l'ensemble des solutions robustes. Enfin, les m{\'e}thodes propos{\'e}es sont appliqu{\'e}es sur les cas industriels propres au projet PEPITO.},
  school = {Lyon},
  author = {Ribaud, M{\'e}lina},
  month = oct,
  year = {2018},
  keywords = {Robust optimization,Kriging,Algorithm,Algorithme,Algorithmes,Covariance kernel,Grande dimension,High dimension,Krigeage,Mathématiques,Noyau de covariance,Optimisation robuste,Turbomachines},
  file = {/home/victor/Zotero/storage/CKTSANXI/Ribaud - 2018 - Krigeage pour la conception de turbomachines  gra.pdf;/home/victor/Zotero/storage/R65EGT37/Ribaud - 2018 - Krigeage pour la conception de turbomachines  gra.pdf;/home/victor/Zotero/storage/7E6NAWNZ/2018LYSEC026.html;/home/victor/Zotero/storage/JRFU2DP6/2018LYSEC026.html}
}

@misc{noauthor_savage_nodate,
  title = {Savage: {{The}} Theory of Statistical Decision - {{Google Scholar}}},
  howpublished = {https://scholar.google.com/scholar\_lookup?hl=en\&volume=46\&publication\_year=1951\&pages=55-67\&journal=Journal+of+the+American+Statistical+Association\&issue=253\&author=L.+J.+Savage\&title=The+theory+of+statistical+decision},
  file = {/home/victor/Zotero/storage/AIZINKIG/scholar_lookup.html}
}

@article{savage_theory_1951,
  title = {The {{Theory}} of {{Statistical Decision}}},
  volume = {46},
  issn = {0162-1459},
  number = {253},
  journal = {Journal of the American Statistical Association},
  doi = {10.1080/01621459.1951.10500768},
  author = {Savage, L. J.},
  month = mar,
  year = {1951},
  pages = {55-67},
  file = {/home/victor/Zotero/storage/NFKIH2DJ/Savage - 1951 - The theory of statistical decision.ps;/home/victor/Zotero/storage/FXV6BBDL/01621459.1951.html;/home/victor/Zotero/storage/J6RUE4WP/01621459.1951.html}
}

@article{mcphail_robustness_2018,
  title = {Robustness {{Metrics}}: {{How Are They Calculated}}, {{When Should They Be Used}} and {{Why Do They Give Different Results}}?},
  volume = {6},
  copyright = {\textcopyright{} 2018 The Authors.},
  issn = {2328-4277},
  shorttitle = {Robustness {{Metrics}}},
  abstract = {Robustness is being used increasingly for decision analysis in relation to deep uncertainty and many metrics have been proposed for its quantification. Recent studies have shown that the application of different robustness metrics can result in different rankings of decision alternatives, but there has been little discussion of what potential causes for this might be. To shed some light on this issue, we present a unifying framework for the calculation of robustness metrics, which assists with understanding how robustness metrics work, when they should be used, and why they sometimes disagree. The framework categorizes the suitability of metrics to a decision-maker based on (1) the decision-context (i.e., the suitability of using absolute performance or regret), (2) the decision-maker's preferred level of risk aversion, and (3) the decision-maker's preference toward maximizing performance, minimizing variance, or some higher-order moment. This article also introduces a conceptual framework describing when relative robustness values of decision alternatives obtained using different metrics are likely to agree and disagree. This is used as a measure of how ``stable'' the ranking of decision alternatives is when determined using different robustness metrics. The framework is tested on three case studies, including water supply augmentation in Adelaide, Australia, the operation of a multipurpose regulated lake in Italy, and flood protection for a hypothetical river based on a reach of the river Rhine in the Netherlands. The proposed conceptual framework is confirmed by the case study results, providing insight into the reasons for disagreements between rankings obtained using different robustness metrics.},
  language = {en},
  number = {2},
  journal = {Earth's Future},
  doi = {10.1002/2017EF000649},
  author = {McPhail, C. and Maier, H. R. and Kwakkel, J. H. and Giuliani, M. and Castelletti, A. and Westra, S.},
  year = {2018},
  keywords = {decision analysis,deep uncertainty,ranking stability,robustness metrics,water systems},
  pages = {169-191},
  file = {/home/victor/Zotero/storage/SUZY5W3T/McPhail et al. - 2018 - Robustness Metrics How Are They Calculated, When .pdf;/home/victor/Zotero/storage/J8QMG2HC/2017EF000649.html}
}

@article{steyerberg_assessing_2010,
  title = {Assessing the Performance of Prediction Models: A Framework for Some Traditional and Novel Measures},
  volume = {21},
  issn = {1044-3983},
  shorttitle = {Assessing the Performance of Prediction Models},
  abstract = {The performance of prediction models can be assessed using a variety of different methods and metrics. Traditional measures for binary and survival outcomes include the Brier score to indicate overall model performance, the concordance (or c) statistic for discriminative ability (or area under the receiver operating characteristic (ROC) curve), and goodness-of-fit statistics for calibration., Several new measures have recently been proposed that can be seen as refinements of discrimination measures, including variants of the c statistic for survival, reclassification tables, net reclassification improvement (NRI), and integrated discrimination improvement (IDI). Moreover, decision\textendash{}analytic measures have been proposed, including decision curves to plot the net benefit achieved by making decisions based on model predictions., We aimed to define the role of these relatively novel approaches in the evaluation of the performance of prediction models. For illustration we present a case study of predicting the presence of residual tumor versus benign tissue in patients with testicular cancer (n=544 for model development, n=273 for external validation)., We suggest that reporting discrimination and calibration will always be important for a prediction model. Decision-analytic measures should be reported if the predictive model is to be used for making clinical decisions. Other measures of performance may be warranted in specific applications, such as reclassification metrics to gain insight into the value of adding a novel predictor to an established model.},
  number = {1},
  journal = {Epidemiology (Cambridge, Mass.)},
  doi = {10.1097/EDE.0b013e3181c30fb2},
  author = {Steyerberg, Ewout W. and Vickers, Andrew J. and Cook, Nancy R. and Gerds, Thomas and Gonen, Mithat and Obuchowski, Nancy and Pencina, Michael J. and Kattan, Michael W.},
  month = jan,
  year = {2010},
  pages = {128-138},
  file = {/home/victor/Zotero/storage/XXH8BP7U/Steyerberg et al. - 2010 - Assessing the performance of prediction models a .pdf},
  pmid = {20010215},
  pmcid = {PMC3575184}
}

@incollection{deodatis_robust_2014,
  title = {Robust Minimax Design from Costly Simulations},
  isbn = {978-1-138-00086-5 978-1-315-88488-2},
  abstract = {Design of complex physical systems most often relies on numerical simulations that may be extremely costly. In this paper, design is formalized as the optimization of a performance index with respect to control variables. Uncertainty is modeled via a vector of environmental variables that can take any value in a known compact set and may have an adverse effect on performance. In this context, the determination of a robust design requires the continuous minimax optimization of black-box functions. An algorithm combining Kriging-based optimization with relaxation is presented, which makes it possible to find approximate solutions to such problems on a limited computational budget. The design of a vibration absorber is presented as an illustrative example.},
  language = {en},
  booktitle = {Safety, {{Reliability}}, {{Risk}} and {{Life}}-{{Cycle Performance}} of {{Structures}} and {{Infrastructures}}},
  publisher = {{CRC Press}},
  author = {{Piet-Lahanier}, H and Marzat, J and Walter, E},
  editor = {Deodatis, George and Ellingwood, Bruce and Frangopol, Dan},
  month = jan,
  year = {2014},
  pages = {3231-3236},
  file = {/home/victor/Zotero/storage/V67NAMCJ/Piet-Lahanier et al. - 2014 - Robust minimax design from costly simulations.pdf},
  doi = {10.1201/b16387-467}
}

@article{calafiore_scenario_2006,
  title = {The {{Scenario Approach}} to {{Robust Control Design}}},
  volume = {51},
  issn = {0018-9286},
  abstract = {We propose a new probabilistic solution framework for robust control analysis and synthesis problems that can be expressed in the form of minimization of a linear objective subject to convex constraints parameterized by uncertainty terms. This includes for instance the wide class of NP-hard control problems representable by means of parameter-dependent linear matrix inequalities (LMIs).},
  language = {en},
  number = {5},
  journal = {IEEE Transactions on Automatic Control},
  doi = {10.1109/TAC.2006.875041},
  author = {Calafiore, G.C. and Campi, M.C.},
  month = may,
  year = {2006},
  pages = {742-753},
  file = {/home/victor/Zotero/storage/UCVEUFN8/Calafiore et Campi - 2006 - The Scenario Approach to Robust Control Design.pdf}
}

@article{e._randall_optimum_1981,
  title = {Optimum {{Vibration Absorbers}} for {{Linear Damped Systems}}},
  volume = {103},
  abstract = {This paper presents computational graphs that determine the optimal linear vibration absorber for linear damped primary systems. Considered as independent parameters are the main system damping ratio and the mass ratio examined over the range 0 to 0. 50 and 0. 01 to 0. 40, respectively. The remaining nondimensional parameters were optimized using numerical methods based on minimum-maximum amplitude criteria. This procedure is illustrated in a design example.},
  journal = {Journal of Mechanical Design - J MECH DESIGN},
  doi = {10.1115/1.3255005},
  author = {E. Randall, S and M. Halsted, D and L. Taylor, D},
  month = oct,
  year = {1981}
}

@article{moritz_gohler_robustness_2016,
  title = {Robustness {{Metrics}}: {{Consolidating}} the {{Multiple Approaches}} to {{Quantify Robustness}}},
  volume = {138},
  issn = {1050-0472},
  shorttitle = {Robustness {{Metrics}}},
  abstract = {The robustness of a design has a major influence on how much the product's performance will vary and is of great concern to design, quality and production engineers. While variability is always central to the definition of robustness, the concept does contain ambiguity and although subtle, this ambiguity can have significant influence on the strategies used to combat variability, the way it is quantified and ultimately, the quality of the final design. In this contribution the literature for robustness metrics was systematically reviewed. From the 108 relevant publications found, 38 metrics were determined to be conceptually different from one another. The metrics were classified by their meaning and interpretation based on the types of information necessary to calculate the metrics. Four different classes were identified: 1) Sensitivity robustness metrics; 2) Size of feasible design space robustness metrics; 3) Functional expectancy and dispersion robustness metrics; and 4) Probability of compliance robustness metrics. The goal was to give a comprehensive overview of robustness metrics and guidance to scholars and practitioners to understand the different types of robustness metrics and to remove the ambiguities of the term robustness. By applying an exemplar metric from each class to a case study, the differences between the classes were further highlighted. These classes form the basis for the definition of four specific sub-definitions of robustness, namely the `robust concept', `robust design', `robust function' and `robust product'.},
  language = {en},
  number = {11},
  journal = {Journal of Mechanical Design},
  doi = {10.1115/1.4034112},
  author = {Moritz G{\"o}hler, Simon and Eifler, Tobias and Howard, Thomas J.},
  month = sep,
  year = {2016},
  pages = {111407},
  file = {/home/victor/Zotero/storage/BFB4RZIJ/Moritz Göhler et al. - 2016 - Robustness Metrics Consolidating the Multiple App.pdf}
}

@article{marzat_worst-case_2013,
  title = {Worst-Case Global Optimization of Black-Box Functions through {{Kriging}} and Relaxation},
  volume = {55},
  issn = {0925-5001, 1573-2916},
  abstract = {A new algorithm is proposed to deal with the worst-case optimization of black-box functions evaluated through costly computer simulations. The input variables of these computer experiments are assumed to be of two types. Control variables must be tuned while environmental variables have an undesirable effect, to which the design of the control variables should be robust. The algorithm to be proposed searches for a minimax solution, i.e., values of the control variables that minimize the maximum of the objective function with respect to the environmental variables. The problem is particularly difficult when the control and environmental variables live in continuous spaces. Combining a relaxation procedure with Krigingbased optimization makes it possible to deal with the continuity of the variables and the fact that no analytical expression of the objective function is available in most real-case problems. Numerical experiments are conducted to assess the accuracy and efficiency of the algorithm, both on analytical test functions with known results and on an engineering application.},
  language = {en},
  number = {4},
  journal = {Journal of Global Optimization},
  doi = {10.1007/s10898-012-9899-y},
  author = {Marzat, Julien and Walter, Eric and {Piet-Lahanier}, H{\'e}l{\`e}ne},
  month = apr,
  year = {2013},
  pages = {707-727},
  file = {/home/victor/Zotero/storage/F6CV6LGA/Marzat et al. - 2013 - Worst-case global optimization of black-box functi.pdf}
}

@article{apley_understanding_2006,
  title = {Understanding the {{Effects}} of {{Model Uncertainty}} in {{Robust Design With Computer Experiments}}},
  volume = {128},
  issn = {10500472},
  language = {en},
  number = {4},
  journal = {Journal of Mechanical Design},
  doi = {10.1115/1.2204974},
  author = {Apley, Daniel W. and Liu, Jun and Chen, Wei},
  year = {2006},
  pages = {945},
  file = {/home/victor/Zotero/storage/B344XSHR/Apley et al. - 2006 - Understanding the Effects of Model Uncertainty in .pdf}
}

@article{lelievre_consideration_2016,
  title = {On the Consideration of Uncertainty in Design: Optimization-Reliability-Robustness},
  volume = {54},
  shorttitle = {On the Consideration of Uncertainty in Design},
  number = {6},
  journal = {Structural and Multidisciplinary Optimization},
  author = {Leli{\`e}vre, Nicolas and Beaurepaire, Pierre and Mattrand, C{\'e}cile and Gayton, Nicolas and Otsmane, Abdelkader},
  year = {2016},
  pages = {1423--1437},
  file = {/home/victor/Zotero/storage/XYKS6BH8/Lelièvre et al. - 2016 - On the consideration of uncertainty in design opt.pdf;/home/victor/Zotero/storage/BKWV7C8V/s00158-016-1556-5.html}
}

@article{gabrel_recent_2014,
  title = {Recent Advances in Robust Optimization: {{An}} Overview},
  volume = {235},
  issn = {03772217},
  shorttitle = {Recent Advances in Robust Optimization},
  abstract = {This paper provides an overview of developments in robust optimization since 2007. It seeks to give a representative picture of the research topics most explored in recent years, highlight common themes in the investigations of independent research teams and highlight the contributions of rising as well as established researchers both to the theory of robust optimization and its practice. With respect to the theory of robust optimization, this paper reviews recent results on the cases without and with recourse, i.e., the static and dynamic settings, as well as the connection with stochastic optimization and risk theory, the concept of distributionally robust optimization, and findings in robust nonlinear optimization. With respect to the practice of robust optimization, we consider a broad spectrum of applications, in particular inventory and logistics, finance, revenue management, but also queueing networks, machine learning, energy systems and the public good. Key developments in the period from 2007 to present include: (i) an extensive body of work on robust decision-making under uncertainty with uncertain distributions, i.e., ``robustifying'' stochastic optimization, (ii) a greater connection with decision sciences by linking uncertainty sets to risk theory, (iii) further results on nonlinear optimization and sequential decision-making and (iv) besides more work on established families of examples such as robust inventory and revenue management, the addition to the robust optimization literature of new application areas, especially energy systems and the public good.},
  language = {en},
  number = {3},
  journal = {European Journal of Operational Research},
  doi = {10.1016/j.ejor.2013.09.036},
  author = {Gabrel, Virginie and Murat, C{\'e}cile and Thiele, Aur{\'e}lie},
  month = jun,
  year = {2014},
  pages = {471-483},
  file = {/home/victor/Zotero/storage/72QGQQ79/Gabrel et al. - 2014 - Recent advances in robust optimization An overvie.pdf}
}

@unpublished{ribaud_robustness_2019,
  title = {Robustness Kriging-Based Optimization},
  abstract = {In the context of robust shape optimization, the estimation cost of some physical models is reduced 10 by the use of a response surface. The multi objective methodology for robust optimization that requires the partitioning of the Pareto front (minimization of the function and the robustness criterion) has already been developed. However, the efficient estimation of the robustness criterion in the context of time-consuming simulation has not been much explored. We propose a robust optimization procedure based on the prediction of the function and its derivatives by a kriging. The 15 usual moment 2 is replaced by an approximated version using Taylor theorem. A Pareto front of the robust solutions is generated by a genetic algorithm named NSGA-II. This algorithm gives a Pareto front in an reasonable time of calculation. We detail seven relevant strategies and compare them for the same budget in two test functions (2D and 6D). In each case, we compare the results when the derivatives are observed and not.},
  author = {Ribaud, M{\'e}lina and {Blanchet-Scalliet}, Christophette and Gillot, Frederic and Helbert, C{\'e}line},
  month = feb,
  year = {2019},
  keywords = {Multi-objective optimization,Expected Improvement 25,Gaussian process modelling,Gaussian process regression,Robust Optimization,robustness criterion,Tay-lor expansion},
  file = {/home/victor/Zotero/storage/22TIVRVK/Ribaud et al. - 2019 - Robustness kriging-based optimization.pdf}
}

@article{freedman_histogram_1981,
  title = {On the Histogram as a Density Estimator:{{L2}} Theory},
  volume = {57},
  issn = {1432-2064},
  shorttitle = {On the Histogram as a Density Estimator},
  language = {en},
  number = {4},
  journal = {Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und Verwandte Gebiete},
  doi = {10.1007/BF01025868},
  author = {Freedman, David and Diaconis, Persi},
  month = dec,
  year = {1981},
  keywords = {Density Estimator,Mathematical Biology,Probability Theory,Stochastic Process},
  pages = {453-476},
  file = {/home/victor/Zotero/storage/FVPC6M7L/Freedman et Diaconis - 1981 - On the histogram as a density estimatorL2 theory.pdf}
}

@misc{noauthor_[1703.04156]_nodate,
  title = {[1703.04156] {{A}} Trust-Region Method for Derivative-Free Nonlinear Constrained Stochastic Optimization},
  howpublished = {https://arxiv.org/abs/1703.04156},
  file = {/home/victor/Zotero/storage/2S3MXR35/1703.html}
}

@incollection{kim_guide_2015,
  title = {A {{Guide}} to {{Sample Average Approximation}}},
  volume = {216},
  abstract = {This chapter reviews the principles of sample average approximation (SAA) for solving simulation optimization problems. We provide an accessible overview of the area and survey interesting recent developments. We explain when one might want to use SAA and when one might expect it to provide good-quality solutions. We also review some of the key theoretical properties of the solutions obtained through SAA. We contrast SAA with stochastic approximation (SA) methods in terms of the computational effort required to obtain solutions of a given quality, explaining why SA ``wins'' asymptotically. However, an extension of SAA known as retrospective optimization can match the asymptotic convergence rate of SA, at least up to a multiplicative constant.},
  author = {Kim, Sujin and Pasupathy, Raghu and Henderson, Shane},
  month = jan,
  year = {2015},
  pages = {207-243},
  file = {/home/victor/Zotero/storage/CIX5IVAF/Kim et al. - 2015 - A Guide to Sample Average Approximation.pdf},
  doi = {10.1007/978-1-4939-1384-8\_8}
}

@article{uryasev_var_nodate,
  title = {{{VaR}} vs {{CVaR}} in {{Risk Management}} and {{Optimization}}},
  language = {en},
  journal = {Risk Management},
  author = {Uryasev, Stan},
  pages = {75},
  file = {/home/victor/Zotero/storage/3VFL725Y/Uryasev - VaR vs CVaR in Risk Management and Optimization.pdf}
}

@article{smith_optimizers_2006,
  title = {The {{Optimizer}}'s {{Curse}}: {{Skepticism}} and {{Postdecision Surprise}} in {{Decision Analysis}}},
  volume = {52},
  issn = {0025-1909, 1526-5501},
  shorttitle = {The {{Optimizer}}'s {{Curse}}},
  language = {en},
  number = {3},
  journal = {Management Science},
  doi = {10.1287/mnsc.1050.0451},
  author = {Smith, James E. and Winkler, Robert L.},
  month = mar,
  year = {2006},
  pages = {311-322},
  file = {/home/victor/Zotero/storage/VAK8B93Y/Smith et Winkler - 2006 - The Optimizer’s Curse Skepticism and Postdecision.pdf}
}

@misc{noauthor_structuring_nodate,
  title = {Structuring {{Your Project}} \textemdash{} {{The Hitchhiker}}'s {{Guide}} to {{Python}}},
  howpublished = {https://docs.python-guide.org/writing/structure},
  file = {/home/victor/Zotero/storage/QRS5EI7R/structure.html}
}

@article{capolei_waterflooding_2013,
  title = {Waterflooding Optimization in Uncertain Geological Scenarios},
  volume = {17},
  issn = {1420-0597, 1573-1499},
  language = {en},
  number = {6},
  journal = {Computational Geosciences},
  doi = {10.1007/s10596-013-9371-1},
  author = {Capolei, Andrea and Suwartadi, Eka and Foss, Bjarne and J{\o}rgensen, John Bagterp},
  month = dec,
  year = {2013},
  pages = {991-1013},
  file = {/home/victor/Zotero/storage/WH6UF2IS/Capolei et al. - 2013 - Waterflooding optimization in uncertain geological.pdf}
}

@article{huyse_probabilistic_2002,
  title = {Probabilistic {{Approach}} to {{Free}}-{{Form Airfoil Shape Optimization Under Uncertainty}}},
  volume = {40},
  issn = {0001-1452},
  number = {9},
  journal = {AIAA Journal},
  doi = {10.2514/2.1881},
  author = {Huyse, Luc and Padula, Sharon L. and Lewis, R. Michael and Li, Wu},
  year = {2002},
  pages = {1764-1772},
  file = {/home/victor/Zotero/storage/TSHAX373/2.html}
}

@article{huyse_free-form_2001,
  title = {Free-Form Airfoil Shape Optimization under Uncertainty Using Maximum Expected Value and Second-Order Second-Moment Strategies},
  author = {Huyse, Luc and Bushnell, Dennis M.},
  year = {2001},
  file = {/home/victor/Zotero/storage/N7A2YLMJ/Huyse et Bushnell - 2001 - Free-form airfoil shape optimization under uncerta.pdf;/home/victor/Zotero/storage/7GVM9C4N/search.html}
}

@article{pinar_mean_2014,
  series = {Recent {{Advances}} in {{Applied}} and {{Computational Mathematics}}: {{ICACM}}-{{IAM}}-{{METU}}},
  title = {Mean Semi-Deviation from a Target and Robust Portfolio Choice under Distribution and Mean Return Ambiguity},
  volume = {259},
  issn = {0377-0427},
  abstract = {We consider the problem of optimal portfolio choice using the lower partial moments risk measure for a market consisting of n risky assets and a riskless asset. For when the mean return vector and variance/covariance matrix of the risky assets are specified without specifying a return distribution, we derive distributionally robust portfolio rules. We then address potential uncertainty (ambiguity) in the mean return vector as well, in addition to distribution ambiguity, and derive a closed-form portfolio rule for when the uncertainty in the return vector is modelled via an ellipsoidal uncertainty set. Our result also indicates a choice criterion for the radius of ambiguity of the ellipsoid. Using the adjustable robustness paradigm we extend the single-period results to multiple periods, and derive closed-form dynamic portfolio policies which mimic closely the single-period policy.},
  journal = {Journal of Computational and Applied Mathematics},
  doi = {10.1016/j.cam.2013.06.028},
  author = {P\i{}nar, Mustafa {\c C}. and Burak Pa{\c c}, A.},
  month = mar,
  year = {2014},
  keywords = {Adjustable robustness,Distributional robustness,Dynamic portfolio rules,Ellipsoidal uncertainty,Lower partial moments,Portfolio choice},
  pages = {394-405},
  file = {/home/victor/Zotero/storage/3ELPEWAP/Pınar et Burak Paç - 2014 - Mean semi-deviation from a target and robust portf.pdf;/home/victor/Zotero/storage/9JNPBK3H/S0377042713003269.html}
}

@article{li_evaluation_2010,
  title = {Evaluation of Failure Probability via Surrogate Models},
  volume = {229},
  issn = {00219991},
  abstract = {Evaluation of failure probability of a given system requires sampling of the system response and can be computationally expensive. Therefore it is desirable to construct an accurate surrogate model for the system response and subsequently to sample the surrogate model. In this paper we discuss the properties of this approach. We demonstrate that the straightforward sampling of a surrogate model can lead to erroneous results, no matter how accurate the surrogate model is. We then propose a hybrid approach by sampling both the surrogate model in a ``large'' portion of the probability space and the original system in a ``small'' portion. The resulting algorithm is significantly more efficient than the traditional sampling method, and is more accurate and robust than the straightforward surrogate model approach. Rigorous convergence proof is established for the hybrid approach, and practical implementation is discussed. Numerical examples are provided to verify the theoretical findings and demonstrate the efficiency gain of the approach.},
  language = {en},
  number = {23},
  journal = {Journal of Computational Physics},
  doi = {10.1016/j.jcp.2010.08.022},
  author = {Li, Jing and Xiu, Dongbin},
  month = nov,
  year = {2010},
  pages = {8966-8980},
  file = {/home/victor/Zotero/storage/SBA5UB8Z/Li et Xiu - 2010 - Evaluation of failure probability via surrogate mo.pdf}
}

@incollection{pflug_remarks_2000,
  title = {Some Remarks on the Value-at-Risk and the Conditional Value-at-Risk},
  booktitle = {Probabilistic Constrained Optimization},
  publisher = {{Springer}},
  author = {Pflug, Georg Ch},
  year = {2000},
  pages = {272--281},
  file = {/home/victor/Zotero/storage/7K85S8BF/Pflug - 2000 - Some remarks on the value-at-risk and the conditio.pdf;/home/victor/Zotero/storage/PWB3L5GT/978-1-4757-3150-7_15.html}
}

@article{maier_uncertain_2016,
  title = {An Uncertain Future, Deep Uncertainty, Scenarios, Robustness and Adaptation: {{How}} Do They Fit Together?},
  volume = {81},
  issn = {13648152},
  shorttitle = {An Uncertain Future, Deep Uncertainty, Scenarios, Robustness and Adaptation},
  abstract = {A highly uncertain future due to changes in climate, technology and socio-economics has led to the realisation that identification of ``best-guess'' future conditions might no longer be appropriate. Instead, multiple plausible futures need to be considered, which requires (i) uncertainties to be described with the aid of scenarios that represent coherent future pathways based on different sets of assumptions, (ii) system performance to be represented by metrics that measure insensitivity (i.e. robustness) to changes in future conditions, and (iii) adaptive strategies to be considered alongside their more commonly used static counterparts. However, while these factors have been considered in isolation previously, there has been a lack of discussion of the way they are connected. In order to address this shortcoming, this paper presents a multidisciplinary perspective on how the above factors fit together to facilitate the development of strategies that are best suited to dealing with a deeply uncertain future.},
  language = {en},
  journal = {Environmental Modelling \& Software},
  doi = {10.1016/j.envsoft.2016.03.014},
  author = {Maier, H.R. and Guillaume, J.H.A. and {van Delden}, H. and Riddell, G.A. and Haasnoot, M. and Kwakkel, J.H.},
  month = jul,
  year = {2016},
  pages = {154-164},
  file = {/home/victor/Zotero/storage/UPRN7L7S/Maier et al. - 2016 - An uncertain future, deep uncertainty, scenarios, .pdf}
}

@article{lempert_general_2006,
  title = {A {{General}}, {{Analytic Method}} for {{Generating Robust Strategies}} and {{Narrative Scenarios}}},
  volume = {52},
  issn = {0025-1909, 1526-5501},
  language = {en},
  number = {4},
  journal = {Management Science},
  doi = {10.1287/mnsc.1050.0472},
  author = {Lempert, Robert J. and Groves, David G. and Popper, Steven W. and Bankes, Steve C.},
  month = apr,
  year = {2006},
  pages = {514-528},
  file = {/home/victor/Zotero/storage/SAHRJMET/Lempert et al. - 2006 - A General, Analytic Method for Generating Robust S.pdf}
}

@incollection{le_gratiet_metamodel-based_2016,
  title = {Metamodel-Based Sensitivity Analysis: Polynomial Chaos Expansions and {{Gaussian}} Processes},
  shorttitle = {Metamodel-Based Sensitivity Analysis},
  abstract = {Global sensitivity analysis is now established as a powerful approach for determining the key random input parameters that drive the uncertainty of model output predictions. Yet the classical computation of the so-called Sobol' indices is based on Monte Carlo simulation, which is not af- fordable when computationally expensive models are used, as it is the case in most applications in engineering and applied sciences. In this respect metamodels such as polynomial chaos expansions (PCE) and Gaussian processes (GP) have received tremendous attention in the last few years, as they allow one to replace the original, taxing model by a surrogate which is built from an experimental design of limited size. Then the surrogate can be used to compute the sensitivity indices in negligible time. In this chapter an introduction to each technique is given, with an emphasis on their strengths and limitations in the context of global sensitivity analysis. In particular, Sobol' (resp. total Sobol') indices can be computed analytically from the PCE coefficients. In contrast, confidence intervals on sensitivity indices can be derived straightforwardly from the properties of GPs. The performance of the two techniques is finally compared on three well-known analytical benchmarks (Ishigami, G-Sobol and Morris functions) as well as on a realistic engineering application (deflection of a truss structure).},
  booktitle = {Handbook of {{Uncertainty Quantification}} - {{Part III}}: {{Sensitivity}} Analysis},
  author = {Le Gratiet, Loic and Marelli, Stefano and Sudret, Bruno},
  year = {2016},
  keywords = {Kriging,Error estimation,Gaussian Processes,Polynomial Chaos Expansions,Sobol' indices},
  file = {/home/victor/Zotero/storage/BU5KFTVK/Le Gratiet et al. - 2016 - Metamodel-based sensitivity analysis polynomial c.pdf}
}

@misc{blanchet-scalliet_specific_2017,
  title = {A Specific Kriging Kernel for Dimensionality Reduction: {{Isotropic}} by Group Kernel},
  shorttitle = {A Specific Kriging Kernel for Dimensionality Reduction},
  abstract = {In the context of computer experiments, metamodels are largely used to represent the output of computer codes. Among these models, Gaussian process regression (kriging) is very efficient see e.g Snelson (2008). In high dimension that is with a large number of input variables , but with few observations the classical anisotropic kriging becomes inefficient and sometimes completely wrong. One way to overcome this drawback is to use the isotropic kernel which is more robust because it estimates not as many parameters. However this model is too restrictive. The aim of this paper is to construct a model between these two, that is at the same time a robust and a flexible model. These two skills are necessary for a model in high dimension. We propose a kernel which is an answer to these requests and that we call isotropic by group kernel. This kernel is a tensor product of few isotropic kernels built on well-chosen subgroup of variables. The number and the composition of the groups are found by an algorithm which explores different structures. The choice of the best model is based on the quality of prediction.},
  language = {en},
  author = {{Blanchet-Scalliet}, Christophette and Helbert, C{\'e}line and Ribaud, M{\'e}lina and Vial, C{\'e}line},
  month = mar,
  year = {2017},
  file = {/home/victor/Zotero/storage/EVJFQIXZ/Blanchet-Scalliet et al. - 2017 - A specific kriging kernel for dimensionality reduc.pdf;/home/victor/Zotero/storage/GSUA7RYB/hal-01496521v1.html}
}

@article{xiu_wiener-askey_nodate,
  title = {{{THE WIENER}}-{{ASKEY POLYNOMIAL CHAOS FOR STOCHASTIC DIFFERENTIAL EQUATIONS}}},
  abstract = {We present a new method for solving stochastic differential equations based on Galerkin projections and extensions of Wiener's polynomial chaos. Specifically, we represent the stochastic processes with an optimum trial basis from the Askey family of orthogonal polynomials that reduces the dimensionality of the system and leads to exponential convergence of the error. Several continuous and discrete processes are treated, and numerical examples show substantial speed-up compared to Monte-Carlo simulations for low dimensional stochastic inputs.},
  language = {en},
  author = {Xiu, Dongbin and Karniadakis, George Em},
  pages = {26},
  file = {/home/victor/Zotero/storage/B2ENDBXJ/Xiu et Karniadakis - THE WIENER-ASKEY POLYNOMIAL CHAOS FOR STOCHASTIC D.pdf}
}

@article{xiu_wiener--askey_2002,
  title = {The {{Wiener}}--{{Askey Polynomial Chaos}} for {{Stochastic Differential Equations}}},
  volume = {24},
  issn = {1064-8275},
  abstract = {We present a new method for solving stochastic differential equations based on Galerkin projections and extensions of Wiener's polynomial chaos. Specifically, we represent the stochastic processes with an optimum trial basis from the Askey family of orthogonal polynomials that reduces the dimensionality of the system and leads to exponential convergence of the error. Several continuous and discrete processes are treated, and numerical examples show substantial speed-up compared to Monte Carlo simulations for low dimensional stochastic inputs.},
  number = {2},
  journal = {SIAM Journal on Scientific Computing},
  doi = {10.1137/S1064827501387826},
  author = {Xiu, D. and Karniadakis, G.},
  month = jan,
  year = {2002},
  pages = {619-644},
  file = {/home/victor/Zotero/storage/HGFIPU5K/S1064827501387826.html}
}

@article{sudret_global_2008,
  title = {Global Sensitivity Analysis Using Polynomial Chaos Expansion},
  volume = {93},
  abstract = {Global sensitivity analysis (SA) aims at quantifying the respective effects of input random variables (or combinations thereof) onto the variance of the response of a physical or mathematical model. Among the abundant literature on sensitivity measures, the Sobol' indices have received much attention since they provide accurate information for most models. The paper introduces generalized polynomial chaos expansions (PCE) to build surrogate models that allow one to compute the Sobol' indices analytically as a post-processing of the PCE coefficients. Thus the computational cost of the sensitivity indices practically reduces to that of estimating the PCE coefficients. An original non intrusive regression-based approach is proposed, together with an experimental design of minimal size. Various application examples illustrate the approach, both from the field of global SA (i.e. well-known benchmark problems) and from the field of stochastic mechanics. The proposed method gives accurate results for various examples that involve up to eight input random variables, at a computational cost which is 2\textendash{}3 orders of magnitude smaller than the traditional Monte Carlo-based evaluation of the Sobol' indices.},
  journal = {Reliability Engineering \& System Safety},
  doi = {10.1016/j.ress.2007.04.002},
  author = {Sudret, Bruno},
  month = jul,
  year = {2008},
  pages = {964-979},
  file = {/home/victor/Zotero/storage/YJFEYP88/Sudret - 2008 - Global sensitivity analysis using polynomial chaos.pdf}
}

@article{beaumont_approximate_2010,
  title = {Approximate {{Bayesian Computation}} in {{Evolution}} and {{Ecology}}},
  volume = {41},
  issn = {1543-592X, 1545-2069},
  abstract = {In the past 10 years a statistical technique, approximate Bayesian computation (ABC), has been developed that can be used to infer parameters and choose between models in the complicated scenarios that are often considered in the environmental sciences. For example, based on gene sequence and microsatellite data, the method has been used to choose between competing models of human demographic history as well as to infer growth rates, times of divergence, and other parameters. The method fits naturally in the Bayesian inferential framework, and a brief overview is given of the key concepts. Three main approaches to ABC have been developed, and these are described and compared. Although the method arose in population genetics, ABC is increasingly used in other fields, including epidemiology, systems biology, ecology, and agent-based modeling, and many of these applications are briefly described.},
  language = {en},
  number = {1},
  journal = {Annual Review of Ecology, Evolution, and Systematics},
  doi = {10.1146/annurev-ecolsys-102209-144621},
  author = {Beaumont, Mark A.},
  month = dec,
  year = {2010},
  pages = {379-406},
  file = {/home/victor/Zotero/storage/3D67J548/Beaumont - 2010 - Approximate Bayesian Computation in Evolution and .pdf}
}

@article{buhmann_robust_2018,
  series = {Journal of {{Computer}} and {{System Science}}: 50 Years of Celebration. {{In}} Memory of {{Professor Edward Blum}}.},
  title = {Robust Optimization in the Presence of Uncertainty: {{A}} Generic Approach},
  volume = {94},
  issn = {0022-0000},
  shorttitle = {Robust Optimization in the Presence of Uncertainty},
  abstract = {We propose a novel approach for optimization under uncertainty. Our approach does not assume any particular noise model behind the measurements, and only requires two typical instances. We first propose a measure of similarity of instances (with respect to a given objective). Based on this measure, we then choose a solution randomly among all solutions that are near-optimum for both instances. The exact notion of near-optimum is intertwined with the proposed similarity measure. Our similarity measure also allows us to derive formal statements about the expected quality of the computed solution. Furthermore, we apply our approach to various optimization problems.},
  journal = {Journal of Computer and System Sciences},
  doi = {10.1016/j.jcss.2017.10.004},
  author = {Buhmann, J. M. and Gronskiy, A. Y. and Mihal{\'a}k, M. and Pr{\"o}ger, T. and {\v S}r{\'a}mek, R. and Widmayer, P.},
  month = jun,
  year = {2018},
  keywords = {Instance similarity,Noise,Optimization,Robustness,Uncertainty},
  pages = {135-166},
  file = {/home/victor/Zotero/storage/PBY9I6I9/Buhmann et al. - 2018 - Robust optimization in the presence of uncertainty.pdf;/home/victor/Zotero/storage/J2ZPYUX7/S002200001730212X.html}
}

@inproceedings{buhmann_robust_2013,
  address = {{Berkeley, California, USA}},
  title = {Robust Optimization in the Presence of Uncertainty},
  isbn = {978-1-4503-1859-4},
  abstract = {We study optimization in the presence of uncertainty such as noise in measurements, and advocate a novel approach of tackling it. The main difference to any existing approach is that we do not assume any knowledge about the nature of the uncertainty (such as for instance a probability distribution). Instead, we are given several instances of the same optimization problem as input, and, assuming they are typical w.r.t. the uncertainty, we make use of it in order to compute a solution that is good for the sample instances as well as for future (unknown) typical instances.},
  language = {en},
  booktitle = {Proceedings of the 4th Conference on {{Innovations}} in {{Theoretical Computer Science}} - {{ITCS}} '13},
  publisher = {{ACM Press}},
  doi = {10.1145/2422436.2422491},
  author = {Buhmann, Joachim M. and Mihalak, Matus and Sramek, Rastislav and Widmayer, Peter},
  year = {2013},
  pages = {505},
  file = {/home/victor/Zotero/storage/NI3RV4WM/Buhmann et al. - 2013 - Robust optimization in the presence of uncertainty.pdf}
}

@incollection{doumpos_performance_2016,
  address = {{Cham}},
  title = {Performance {{Analysis}} in {{Robust Optimization}}},
  volume = {241},
  isbn = {978-3-319-33119-5 978-3-319-33121-8},
  abstract = {We discuss the problem of evaluating a robust solution. To this end, we first give a short primer on how to apply robustification approaches to uncertain optimization problems using the assignment problem and the knapsack problem as illustrative examples. As it is not immediately clear in practice which such robustness approach is suitable for the problem at hand, we present current approaches for evaluating and comparing robustness from the literature, and introduce the new concept of a scenario curve. Using the methods presented in this paper, an easy guide is given to the decision maker to find, solve and compare the best robust optimization method for his purposes.},
  language = {en},
  booktitle = {Robustness {{Analysis}} in {{Decision Aiding}}, {{Optimization}}, and {{Analytics}}},
  publisher = {{Springer International Publishing}},
  author = {Chassein, Andr{\'e} and Goerigk, Marc},
  editor = {Doumpos, Michael and Zopounidis, Constantin and Grigoroudis, Evangelos},
  year = {2016},
  pages = {145-170},
  file = {/home/victor/Zotero/storage/EI2VPC8N/Chassein et Goerigk - 2016 - Performance Analysis in Robust Optimization.pdf},
  doi = {10.1007/978-3-319-33121-8_7}
}

@article{goerigk_ranking_2018,
  title = {Ranking Robustness and Its Application to Evacuation Planning},
  volume = {264},
  issn = {0377-2217},
  abstract = {We present a new approach to handle uncertain combinatorial optimization problems that uses solution ranking procedures to determine the degree of robustness of a solution. Unlike classic concepts for robust optimization, our approach is not purely based on absolute quantitative performance, but also includes qualitative aspects that are of major importance for the decision maker. We discuss the two variants, solution ranking and objective ranking robustness, in more detail, presenting problem complexities and solution approaches. Using an uncertain shortest path problem as a computational example, the potential of our approach is demonstrated in the context of evacuation planning due to river flooding.},
  number = {3},
  journal = {European Journal of Operational Research},
  doi = {10.1016/j.ejor.2016.05.037},
  author = {Goerigk, Marc and Hamacher, Horst W. and Kinscherff, Anika},
  month = feb,
  year = {2018},
  keywords = {Combinatorial optimization,Evacuation planning,Robust optimization,Solution ranking},
  pages = {837-846},
  file = {/home/victor/Zotero/storage/M3XSTTJ7/Goerigk et al. - 2018 - Ranking robustness and its application to evacuati.pdf;/home/victor/Zotero/storage/PYHEKM95/S0377221716303745.html}
}


