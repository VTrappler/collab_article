

@article{walker_defining_2003,
  author =        {Walker, Warren E. and Harremo{\"e}s, Poul and
                   Rotmans, Jan and {van der Sluijs}, Jeroen P. and
                   {van Asselt}, Marjolein BA and Janssen, Peter and
                   {Krayer von Krauss}, Martin P.},
  journal =       {Integrated assessment},
  number =        {1},
  pages =         {5--17},
  title =         {Defining Uncertainty: A Conceptual Basis for
                   Uncertainty Management in Model-Based Decision
                   Support},
  volume =        {4},
  year =          {2003},
}

@article{das_estimation_1991,
  author =        {Das, S. K. and Lardner, R. W.},
  journal =       {Journal of Geophysical Research},
  number =        {C8},
  pages =         {15187},
  title =         {On the Estimation of Parameters of Hydraulic Models
                   by Assimilation of Periodic Tidal Data},
  volume =        {96},
  year =          {1991},
  doi =           {10.1029/91JC01318},
  issn =          {0148-0227},
  language =      {en},
}

@article{das_variational_1992,
  author =        {Das, S. K. and Lardner, R. W.},
  journal =       {International Journal for Numerical Methods in
                   Fluids},
  month =         aug,
  number =        {3},
  pages =         {313--327},
  title =         {Variational Parameter Estimation for a
                   Two-Dimensional Numerical Tidal Model},
  volume =        {15},
  year =          {1992},
  abstract =      {It is shown that the parameters in a two-dimensional
                   (depth-averaged) numerical tidal model can be
                   estimated accurately by assimilation of data from
                   tide gauges. The tidal model considered is a
                   semi-linearized one in which kinematical
                   non-linearities are neglected but non-linear bottom
                   friction is included. The parameters to be estimated
                   (bottom friction coefficient and water depth) are
                   assumed to be position-dependent and are approximated
                   by piecewise linear interpolations between certain
                   nodal values. The numerical scheme consists of a
                   two-level leapfrog method. The adjoint scheme is
                   constructed on the assumption that a certain norm of
                   the difference between computed and observed
                   elevations at the tide gauges should be minimized. It
                   is shown that a satisfactory numerical minimization
                   can be completed using either the
                   Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton
                   algorithm or Nash's truncated Newton algorithm. On
                   the basis of a number of test problems, it is shown
                   that very effective estimation of the nodal values of
                   the parameters can be achieved provided the number of
                   data stations is sufficiently large in relation to
                   the number of nodes.},
  doi =           {10.1002/fld.1650150305},
  issn =          {1097-0363},
  language =      {en},
}

@phdthesis{boutet_estimation_2015,
  author =        {Boutet, Martial},
  school =        {Universit\'e d'Aix Marseille},
  title =         {Estimation Du Frottement Sur Le Fond Pour La
                   Mod\'elisation de La Mar\'ee Barotrope},
  year =          {2015},
}

@article{ten_brummelhuis_parameter_1990,
  author =        {{ten Brummelhuis}, P. G. J. and Heemink, A. W.},
  journal =       {Stochastic Hydrology and Hydraulics},
  month =         sep,
  number =        {3},
  pages =         {193--208},
  title =         {Parameter Identification in Tidal Models with
                   Uncertain Boundary Conditions},
  volume =        {4},
  year =          {1990},
  abstract =      {In this paper a parameter estimation algorithm is
                   developed to estimate uncertain parameters in two
                   dimensional shallow water flow models. Since in
                   practice the open boundary conditions of these models
                   are usually not known accurately, the uncertainty of
                   these boundary conditions has to be taken into
                   account to prevent that boundary errors are
                   interpreted by the estimation procedure as parameter
                   fluctuations. Therefore the open boundary conditions
                   are embedded into a stochastic environment and a
                   constant gain extended Kalman filter is employed to
                   identify the state of the system. Defining a error
                   functional that measures the differences between the
                   filtered state of the system and the measurements, a
                   quasi Newton method is employed to determine the
                   minimum of this functional. To reduce the
                   computational burden, the gradient of the criterium
                   that is required using the quasi Newton method is
                   determined by solving the adjoint system.},
  doi =           {10.1007/BF01543083},
  issn =          {1435-151X},
  language =      {en},
}

@article{huyse_free-form_2001,
  author =        {Huyse, Luc and Bushnell, Dennis M.},
  title =         {Free-Form Airfoil Shape Optimization under
                   Uncertainty Using Maximum Expected Value and
                   Second-Order Second-Moment Strategies},
  year =          {2001},
}

@article{kuczera_there_2010,
  author =        {Kuczera, George and Renard, Benjamin and Thyer, Mark and
                   Kavetski, Dmitri},
  journal =       {Hydrological Sciences Journal},
  month =         aug,
  number =        {6},
  pages =         {980--991},
  title =         {There Are No Hydrological Monsters, Just Models and
                   Observations with Large Uncertainties!},
  volume =        {55},
  year =          {2010},
  abstract =      {Catchments that do not behave in the way the
                   hydrologist expects, expose the frailties of
                   hydrological science, particularly its unduly
                   simplistic treatment of input and model uncertainty.
                   A conceptual rainfall\textendash runoff model
                   represents a highly simplified hypothesis of the
                   transformation of rainfall into runoff. Sub-grid
                   variability and mis-specification of processes
                   introduce an irreducible model error, about which
                   little is currently known. In addition, hydrological
                   observation systems are far from perfect, with the
                   principal catchment forcing (rainfall) often subject
                   to large sampling errors. When ignored or treated
                   simplistically, these errors develop into monsters
                   that destroy our ability to model certain catchments.
                   In this paper, these monsters are tackled using
                   Bayesian Total Error Analysis, a framework that
                   accounts for user-specified sources of error and
                   yields quantitative insights into how prior knowledge
                   of these uncertainties affects our ability to infer
                   models and use them for predictive purposes. A case
                   study involving a catchment with an apparent water
                   balance anomaly (a hydrological monstrosity!)
                   illustrates these concepts. It is found that, in the
                   absence of additional information, the
                   rainfall\textendash runoff record is insufficient to
                   explain this anomaly \textendash{} it could be due to
                   a large export of groundwater, systematic
                   overestimation of catchment rainfall of the order of
                   40\%, or a conspiracy of these factors. There is ``no
                   free lunch'' in hydrology. The rainfall\textendash
                   runoff record on its own is insufficient to decompose
                   the different sources of uncertainty affecting
                   calibration, testing and prediction, and hydrological
                   monstrosities will persist until additional
                   independent knowledge of uncertainties is obtained.
                   Citation Kuczera, G., Renard, B., Thyer, M. \&
                   Kavetski, D. (2010) There are no hydrological
                   monsters, just models and observations with large
                   uncertainties! Hydrol. Sci. J. 55(6), 980\textendash
                   991.},
  doi =           {10.1080/02626667.2010.504677},
  issn =          {0262-6667},
}

@article{wong_neglecting_2018,
  author =        {Wong, Tony E. and Klufas, Alexandra and
                   Srikrishnan, Vivek and Keller, Klaus},
  journal =       {Environmental Research Letters},
  month =         jul,
  number =        {7},
  pages =         {074019},
  publisher =     {{IOP Publishing}},
  title =         {Neglecting Model Structural Uncertainty
                   Underestimates Upper Tails of Flood Hazard},
  volume =        {13},
  year =          {2018},
  abstract =      {Coastal flooding drives considerable risks to many
                   communities, but projections of future flood risks
                   are deeply uncertain. The paucity of observations of
                   extreme events often motivates the use of statistical
                   approaches to model the distribution of extreme storm
                   surge events. One key deep uncertainty that is often
                   overlooked is model structural uncertainty. There is
                   currently no strong consensus among experts regarding
                   which class of statistical model to use as a `best
                   practice'. Robust management of coastal flooding
                   risks requires coastal managers to consider the
                   distinct possibility of non-stationarity in storm
                   surges. This increases the complexity of the
                   potential models to use, which tends to increase the
                   data required to constrain the model. Here, we use a
                   Bayesian model averaging approach to analyze the
                   balance between (i) model complexity sufficient to
                   capture decision-relevant risks and (ii) data
                   availability to constrain complex model structures.
                   We characterize deep model structural uncertainty
                   through a set of calibration experiments.
                   Specifically, we calibrate a set of models ranging in
                   complexity using long-term tide gauge observations
                   from the Netherlands and the United States. We find
                   that in both considered cases, roughly half of the
                   model weight is associated with the non-stationary
                   models. Our approach provides a formal framework to
                   integrate information across model structures, in
                   light of the potentially sizable modeling
                   uncertainties. By combining information from multiple
                   models, our inference sharpens for the projected
                   storm surge 100 year return levels, and estimated
                   return levels increase by several centimeters. We
                   assess the impacts of data availability through a set
                   of experiments with temporal subsets and model
                   comparison metrics. Our analysis suggests that about
                   70 years of data are required to stabilize estimates
                   of the 100 year return level, for the locations and
                   methods considered here.},
  doi =           {10.1088/1748-9326/aacb3d},
  issn =          {1748-9326},
  language =      {en},
}

@article{hu_robust_2011,
  author =        {Hu, Yi and Rao, Singiresu},
  journal =       {Journal of Mechanical Design},
  month =         nov,
  pages =         {111009},
  title =         {Robust {{Design}} of {{Horizontal Axis Wind Turbines
                   Using Taguchi Method}}},
  volume =        {133},
  year =          {2011},
  abstract =      {The robust design of horizontal axis wind turbines,
                   including both parameter design and tolerance design,
                   is presented. A simple way of designing robust
                   horizontal axis wind turbine systems under realistic
                   conditions is outlined with multiple design
                   parameters (variables), multiple objectives, and
                   multiple constraints simultaneously by using the
                   traditional Taguchi method and its extensions. The
                   performance of the turbines is predicted using the
                   axial momentum theory and the blade element momentum
                   theory. In the parameter design stage, the energy
                   output of the turbine is maximized using the Taguchi
                   method and an extended penalty-based Taguchi method
                   is proposed to solve constrained parameter design
                   problems. The results of the unconstrained and
                   constrained parameter design problems, in terms of
                   the objective function and constraints are compared.
                   Using an appropriate set of tolerance settings of the
                   parameters, the tolerance design problem is
                   formulated so as to yield an economical design, while
                   ensuring a minimal variability in the performance of
                   the wind turbine. The resulting multi-objective
                   tolerance design problem is solved using the
                   traditional Taguchi method. The present work provides
                   a simple and economical approach for the robust
                   optimal design of horizontal axis wind turbines.},
  doi =           {10.1115/1.4004989},
}

@phdthesis{brevault_contributions_2015,
  author =        {Brevault, L.},
  month =         oct,
  school =        {Ecole Nationale Sup\'erieure des Mines de
                   Saint-\'Etienne},
  type =          {Theses},
  title =         {Contributions to Multidisciplinary Design
                   Optimization under Uncertainty, Application to Launch
                   Vehicle Design.},
  year =          {2015},
}

@article{lelievre_consideration_2016,
  author =        {Leli{\`e}vre, Nicolas and Beaurepaire, Pierre and
                   Mattrand, C{\'e}cile and Gayton, Nicolas and
                   Otsmane, Abdelkader},
  journal =       {Structural and Multidisciplinary Optimization},
  number =        {6},
  pages =         {1423--1437},
  title =         {On the Consideration of Uncertainty in Design:
                   Optimization-Reliability-Robustness},
  volume =        {54},
  year =          {2016},
}

@article{petrone_robustness_2011,
  author =        {Petrone, Giovanni and Iaccarino, Gianluca and
                   Quagliarella, D.},
  journal =       {Evolutionary and deterministic methods for design,
                   optimization and control (EUROGEN 2011). CIRA, Capua},
  pages =         {244--252},
  title =         {Robustness Criteria in Optimization under
                   Uncertainty},
  year =          {2011},
}

@article{seshadri_density-matching_2014,
  author =        {Seshadri, Pranay and Constantine, Paul and
                   Iaccarino, Gianluca and Parks, Geoffrey},
  journal =       {arXiv:1409.7089 [math, stat]},
  month =         sep,
  title =         {A Density-Matching Approach for Optimization under
                   Uncertainty},
  year =          {2014},
  abstract =      {Modern computers enable methods for design
                   optimization that account for uncertainty in the
                   system---so-called optimization under uncertainty. We
                   propose a metric for OUU that measures the distance
                   between a designer-specified probability density
                   function of the system response the target and system
                   response's density function at a given design. We
                   study an OUU formulation that minimizes this distance
                   metric over all designs. We discretize the objective
                   function with numerical quadrature and approximate
                   the response density function with a Gaussian kernel
                   density estimate. We offer heuristics for addressing
                   issues that arise in this formulation, and we apply
                   the approach to a CFD-based airfoil shape
                   optimization problem. We qualitatively compare the
                   density-matching approach to a multi-objective robust
                   design optimization to gain insight into the method.},
}

@article{cook_horsetail_2018,
  author =        {Cook, L. W. and Jarrett, J. P.},
  journal =       {Engineering Optimization},
  month =         apr,
  number =        {4},
  pages =         {549--567},
  title =         {Horsetail Matching: A Flexible Approach to
                   Optimization under Uncertainty},
  volume =        {50},
  year =          {2018},
  abstract =      {It is important to design engineering systems to be
                   robust with respect to uncertainties in the design
                   process. Often, this is done by considering
                   statistical moments, but over-reliance on statistical
                   moments when formulating a robust optimization can
                   produce designs that are stochastically dominated by
                   other feasible designs. This article instead proposes
                   a formulation for optimization under uncertainty that
                   minimizes the difference between a design's
                   cumulative distribution function and a target. A
                   standard target is proposed that produces
                   stochastically non-dominated designs, but the
                   formulation also offers enough flexibility to recover
                   existing approaches for robust optimization. A
                   numerical implementation is developed that employs
                   kernels to give a differentiable objective function.
                   The method is applied to algebraic test problems and
                   a robust transonic airfoil design problem where it is
                   compared to multi-objective, weighted-sum and density
                   matching approaches to robust optimization; several
                   advantages over these existing methods are
                   demonstrated.},
  doi =           {10.1080/0305215X.2017.1327581},
  issn =          {0305-215X, 1029-0273},
  language =      {en},
}

@article{ning_optimization_2019,
  author =        {Ning, Chao and You, Fengqi},
  journal =       {Computers \& Chemical Engineering},
  month =         jun,
  pages =         {434--448},
  title =         {Optimization under Uncertainty in the Era of Big Data
                   and Deep Learning: {{When}} Machine Learning Meets
                   Mathematical Programming},
  volume =        {125},
  year =          {2019},
  abstract =      {This paper reviews recent advances in the field of
                   optimization under uncertainty via a modern data
                   lens, highlights key research challenges and promise
                   of data-driven optimization that organically
                   integrates machine learning and mathematical
                   programming for decision-making under uncertainty,
                   and identifies potential research opportunities. A
                   brief review of classical mathematical programming
                   techniques for hedging against uncertainty is first
                   presented, along with their wide spectrum of
                   applications in Process Systems Engineering. A
                   comprehensive review and classification of the
                   relevant publications on data-driven distributionally
                   robust optimization, data-driven chance constrained
                   program, data-driven robust optimization, and
                   data-driven scenario-based optimization is then
                   presented. This paper also identifies fertile avenues
                   for future research that focuses on a closed-loop
                   data-driven optimization framework, which allows the
                   feedback from mathematical programming to machine
                   learning, as well as scenario-based optimization
                   leveraging the power of deep learning techniques.
                   Perspectives on online learning-based data-driven
                   multistage optimization with a
                   learning-while-optimizing scheme are presented.},
  doi =           {10.1016/j.compchemeng.2019.03.034},
  issn =          {0098-1354},
  language =      {en},
}

@incollection{huber_robust_2011,
  author =        {Huber, Peter J.},
  booktitle =     {International {{Encyclopedia}} of {{Statistical
                   Science}}},
  pages =         {1248--1251},
  publisher =     {{Springer}},
  title =         {Robust Statistics},
  year =          {2011},
}

@article{rao_robust_2015,
  author =        {Rao, Vishwas and Sandu, Adrian and Ng, Michael and
                   {Nino-Ruiz}, Elias},
  journal =       {SciRate},
  month =         nov,
  title =         {Robust Data Assimilation Using \${{L}}\_1\$ and
                   {{Huber}} Norms},
  year =          {2015},
  abstract =      {Data assimilation is the process to fuse information
                   from priors, observations of nature, and numerical
                   models, in order to obtain best estimates of the
                   parameters or state of a physical system of interest.
                   Presence of large errors in some observational data,
                   e.g., data collected from a faulty instrument,
                   negatively affect the quality of the overall
                   assimilation results. This work develops a systematic
                   framework for robust data assimilation. The new
                   algorithms continue to produce good analyses in the
                   presence of observation outliers. The approach is
                   based on replacing the traditional \$\textbackslash
                   L\_2\$ norm formulation of data assimilation problems
                   with formulations based on \$\textbackslash L\_1\$
                   and Huber norms. Numerical experiments using the
                   Lorenz-96 and the shallow water on the sphere models
                   illustrate how the new algorithms outperform
                   traditional data assimilation approaches in the
                   presence of data outliers.},
}

@article{berger_overview_1994,
  author =        {Berger, James O. and Moreno, El{\'i}as and
                   Pericchi, Luis Raul and Bayarri, M. Jes{\'u}s and
                   Bernardo, Jos{\'e} M. and Cano, Juan A. and
                   {De la Horra}, Juli{\'a}n and Mart{\'i}n, Jacinto and
                   {R{\'i}os-Ins{\'u}a}, David and Betr{\`o}, Bruno},
  journal =       {Test},
  number =        {1},
  pages =         {5--124},
  title =         {An Overview of Robust {{Bayesian}} Analysis},
  volume =        {3},
  year =          {1994},
}

@article{rahimian_distributionally_2019,
  author =        {Rahimian, Hamed and Mehrotra, Sanjay},
  journal =       {arXiv:1908.05659 [cs, math, stat]},
  month =         aug,
  title =         {Distributionally {{Robust Optimization}}: {{A
                   Review}}},
  year =          {2019},
  abstract =      {The concepts of risk-aversion, chance-constrained
                   optimization, and robust optimization have developed
                   significantly over the last decade. Statistical
                   learning community has also witnessed a rapid
                   theoretical and applied growth by relying on these
                   concepts. A modeling framework, called
                   distributionally robust optimization (DRO), has
                   recently received significant attention in both the
                   operations research and statistical learning
                   communities. This paper surveys main concepts and
                   contributions to DRO, and its relationships with
                   robust optimization, risk-aversion,
                   chance-constrained optimization, and function
                   regularization.},
  language =      {en},
}

@article{marzat_worst-case_2013,
  author =        {Marzat, Julien and Walter, Eric and
                   {Piet-Lahanier}, H{\'e}l{\`e}ne},
  journal =       {Journal of Global Optimization},
  month =         apr,
  number =        {4},
  pages =         {707--727},
  title =         {Worst-Case Global Optimization of Black-Box Functions
                   through {{Kriging}} and Relaxation},
  volume =        {55},
  year =          {2013},
  abstract =      {A new algorithm is proposed to deal with the
                   worst-case optimization of black-box functions
                   evaluated through costly computer simulations. The
                   input variables of these computer experiments are
                   assumed to be of two types. Control variables must be
                   tuned while environmental variables have an
                   undesirable effect, to which the design of the
                   control variables should be robust. The algorithm to
                   be proposed searches for a minimax solution, i.e.,
                   values of the control variables that minimize the
                   maximum of the objective function with respect to the
                   environmental variables. The problem is particularly
                   difficult when the control and environmental
                   variables live in continuous spaces. Combining a
                   relaxation procedure with Krigingbased optimization
                   makes it possible to deal with the continuity of the
                   variables and the fact that no analytical expression
                   of the objective function is available in most
                   real-case problems. Numerical experiments are
                   conducted to assess the accuracy and efficiency of
                   the algorithm, both on analytical test functions with
                   known results and on an engineering application.},
  doi =           {10.1007/s10898-012-9899-y},
  issn =          {0925-5001, 1573-2916},
  language =      {en},
}

@article{villemonteix_informational_2006,
  author =        {Villemonteix, Julien and Vazquez, Emmanuel and
                   Walter, Eric},
  journal =       {arXiv:cs/0611143},
  month =         nov,
  title =         {An Informational Approach to the Global Optimization
                   of Expensive-to-Evaluate Functions},
  year =          {2006},
  abstract =      {In many global optimization problems motivated by
                   engineering applications, the number of function
                   evaluations is severely limited by time or cost. To
                   ensure that each evaluation contributes to the
                   localization of good candidates for the role of
                   global minimizer, a sequential choice of evaluation
                   points is usually carried out. In particular, when
                   Kriging is used to interpolate past evaluations, the
                   uncertainty associated with the lack of information
                   on the function can be expressed and used to compute
                   a number of criteria accounting for the interest of
                   an additional evaluation at any given point. This
                   paper introduces minimizer entropy as a new
                   Kriging-based criterion for the sequential choice of
                   points at which the function should be evaluated.
                   Based on stepwise uncertainty reduction, it accounts
                   for the informational gain on the minimizer expected
                   from a new evaluation. The criterion is approximated
                   using conditional simulations of the Gaussian process
                   model behind Kriging, and then inserted into an
                   algorithm similar in spirit to the Efficient Global
                   Optimization (EGO) algorithm. An empirical comparison
                   is carried out between our criterion and expected
                   improvement, one of the reference criteria in the
                   literature. Experimental results indicate major
                   evaluation savings over EGO. Finally, the method,
                   which we call IAGO (for Informational Approach to
                   Global Optimization) is extended to robust
                   optimization problems, where both the factors to be
                   tuned and the function evaluations are corrupted by
                   noise.},
  language =      {en},
}

@article{hennig_entropy_2011,
  author =        {Hennig, Philipp and Schuler, Christian J.},
  month =         dec,
  title =         {Entropy {{Search}} for {{Information}}-{{Efficient
                   Global Optimization}}},
  year =          {2011},
  language =      {en},
}

@inproceedings{buhmann_robust_2013,
  address =       {{Berkeley, California, USA}},
  author =        {Buhmann, Joachim M. and Mihalak, Matus and
                   Sramek, Rastislav and Widmayer, Peter},
  booktitle =     {Proceedings of the 4th Conference on {{Innovations}}
                   in {{Theoretical Computer Science}} - {{ITCS}} '13},
  pages =         {505},
  publisher =     {{ACM Press}},
  title =         {Robust Optimization in the Presence of Uncertainty},
  year =          {2013},
  abstract =      {We study optimization in the presence of uncertainty
                   such as noise in measurements, and advocate a novel
                   approach of tackling it. The main difference to any
                   existing approach is that we do not assume any
                   knowledge about the nature of the uncertainty (such
                   as for instance a probability distribution). Instead,
                   we are given several instances of the same
                   optimization problem as input, and, assuming they are
                   typical w.r.t. the uncertainty, we make use of it in
                   order to compute a solution that is good for the
                   sample instances as well as for future (unknown)
                   typical instances.},
  doi =           {10.1145/2422436.2422491},
  isbn =          {978-1-4503-1859-4},
  language =      {en},
}

@article{kouvelis_algorithms_1992,
  author =        {Kouvelis, Panagiotis and Kurawarwala, Abbas A. and
                   Guti{\'e}rrez, Genaro J.},
  journal =       {European Journal of Operational Research},
  month =         dec,
  number =        {2},
  pages =         {287--303},
  series =        {Strategic {{Planning}} of {{Facilities}}},
  title =         {Algorithms for Robust Single and Multiple Period
                   Layout Planning for Manufacturing Systems},
  volume =        {63},
  year =          {1992},
  abstract =      {In many layout design situations, the use of
                   `optimality' with respect to a design objective, such
                   as the minimization of the material handling cost, is
                   insufficiently discriminating. Robustness of the
                   layout, in cases of demand uncertainty, is more
                   important for the manufacturing manager. A robust
                   layout is one that is close to the optimal solution
                   for a wide variety of demand scenarios even though it
                   may not be optimal under any specific demand
                   scenario. In this paper, we develop algorithms to
                   generate robust layout designs for manufacturing
                   systems. Our robustness approach to the layout
                   decision making can be applied to single and multiple
                   period problems in the presence of considerable
                   uncertainty, both in terms of products to be produced
                   as well as their production volumes. Our algorithms,
                   executed in a heuristic fashion, can be effectively
                   used for layout design of large size manufacturing
                   systems.},
  doi =           {10.1016/0377-2217(92)90032-5},
  issn =          {0377-2217},
  language =      {en},
}

@article{snyder_stochastic_2004,
  author =        {Snyder, Lawrence and Daskin, Mark},
  journal =       {Iie Transactions},
  month =         aug,
  title =         {Stochastic P-Robust Location Problems},
  volume =        {38},
  year =          {2004},
  abstract =      {Many objectives have been proposed for optimization
                   under uncertainty. The typical stochastic programming
                   ob-jective of minimizing expected cost may yield
                   solutions that are inexpensive in the long run but
                   perform poorly under certain realizations of the
                   random data. On the other hand, the typical robust
                   optimization objective of minimizing maximum cost or
                   regret tends to be overly conservative, planning
                   against a disastrous but unlikely scenario. In this
                   paper, we present facility location models that
                   combine the two objectives by minimizing the expected
                   cost while bounding the relative regret in each
                   scenario. In particular, the models seek the
                   minimum-expected-cost solution that is p-robust;
                   i.e., whose relative regret is no more than 100p\% in
                   each scenario. We present p-robust models based on
                   two classical facility location problems, the P
                   -median problem and the uncapacitated fixed-charge
                   location problem. We solve both problems using
                   variable splitting (Lagrangian decomposition), in
                   which the subproblem reduces to the multiple-choice
                   knapsack problem. Feasible solutions are found using
                   an upper-bounding heuristic. For many instances of
                   the problems, finding a feasible solution, and even
                   determining whether the instance is feasible, is
                   difficult; we discuss a mechanism for determining
                   infeasibility. We also show how the algorithm can be
                   used as a heuristic to solve minimax-regret versions
                   of the location problems.},
  doi =           {10.1080/07408170500469113},
}

@inproceedings{juditsky_stochastic_2009,
  author =        {Juditsky, Anatoli and Nemirovski, Arkadii S. and
                   Lan, Guanghui and Shapiro, Alexander},
  booktitle =     {{{ISMP}} 2009 - 20th {{International Symposium}} of
                   {{Mathematical Programming}}},
  month =         aug,
  title =         {Stochastic {{Approximation Approach}} to {{Stochastic
                   Programming}}},
  year =          {2009},
  abstract =      {A basic difficulty with solving stochastic
                   programming problems is that it requires computation
                   of expectations given by multidimensional integrals.
                   One approach, based on Monte Carlo sampling
                   techniques, is to generate a reasonably large random
                   sample and consequently to solve the constructed
                   so-called Sample Average Approximation (SAA) problem.
                   The other classical approach is based on Stochastic
                   Approximation (SA) techniques. In this talk we
                   discuss some recent advances in development of SA
                   type numerical algorithms for solving convex
                   stochastic programming problems. Numerical
                   experiments show that for some classes of problems
                   the so-called Mirror Descent SA Method can
                   significantly outperform the SAA approach.},
  language =      {en},
}

@incollection{kim_guide_2015,
  author =        {Kim, Sujin and Pasupathy, Raghu and Henderson, Shane},
  month =         jan,
  pages =         {207--243},
  title =         {A {{Guide}} to {{Sample Average Approximation}}},
  volume =        {216},
  year =          {2015},
  abstract =      {This chapter reviews the principles of sample average
                   approximation (SAA) for solving simulation
                   optimization problems. We provide an accessible
                   overview of the area and survey interesting recent
                   developments. We explain when one might want to use
                   SAA and when one might expect it to provide
                   good-quality solutions. We also review some of the
                   key theoretical properties of the solutions obtained
                   through SAA. We contrast SAA with stochastic
                   approximation (SA) methods in terms of the
                   computational effort required to obtain solutions of
                   a given quality, explaining why SA ``wins''
                   asymptotically. However, an extension of SAA known as
                   retrospective optimization can match the asymptotic
                   convergence rate of SA, at least up to a
                   multiplicative constant.},
  doi =           {10.1007/978-1-4939-1384-8-8},
}

@techreport{janusevskis_simultaneous_2010,
  author =        {Janusevskis, Janis and Le Riche, Rodolphe},
  month =         jul,
  title =         {Simultaneous Kriging-Based Sampling for Optimization
                   and Uncertainty Propagation},
  year =          {2010},
  abstract =      {Robust analysis and optimization is typically based
                   on repeated calls to a deterministic simulator that
                   aim at propagating uncertainties and finding optimal
                   design variables. Without loss of generality a double
                   set of simulation parameters can be assumed: x are
                   deterministic optimization variables, u are random
                   parameters of known probability density function and
                   f (x, u) is the objective function attached to the
                   simulator. Most robust optimization methods involve
                   two imbricated tasks, the u's uncertainty propagation
                   (e.g., Monte Carlo simulations, reliability index
                   calculation) which is recurcively performed inside
                   optimization iterations on the x's. In practice, f is
                   often calculated through a computationally expensive
                   software. This makes the computational cost one of
                   the principal obstacle to optimization in the
                   presence of uncertainties. This report proposes a new
                   efficient method for minimizing the mean objective
                   function, min E[f(x, U)]. The efficiency stems from
                   the simultaneous sampling of f for uncertainty
                   propagation and optimization, i.e., the hierarchical
                   imbrication is avoided. Y(x,u) ({$\omega$}), a
                   kriging (Gaussian process conditioned on t past
                   calculations of f) model of f (x, u) is built and the
                   mean process, Z(x) ({$\omega$}) =
                   E[Y(x,U)({$\omega$})], is analytically derived from
                   it. The sampling criterion that yields both x and u
                   is the one-step ahead minimum variance of the mean
                   process Z at the maximizer of the expected
                   improvement. The method is compared with Monte Carlo
                   and kriging-based approaches on analytical test
                   functions in two, four and six dimensions.},
}

@phdthesis{baudoui_optimisation_2012,
  author =        {Baudoui, Vincent},
  school =        {Toulouse, ISAE},
  title =         {Optimisation Robuste Multiobjectifs Par Mod\`eles de
                   Substitution},
  year =          {2012},
}

@article{grodzevich_normalization_2006,
  author =        {Grodzevich, Oleg and Romanko, Oleksandr},
  title =         {Normalization and Other Topics in Multi-Objective
                   Optimization},
  year =          {2006},
}

@article{marler_weighted_2010,
  author =        {Marler, R. Timothy and Arora, Jasbir S.},
  journal =       {Structural and Multidisciplinary Optimization},
  month =         jun,
  number =        {6},
  pages =         {853--862},
  title =         {The Weighted Sum Method for Multi-Objective
                   Optimization: New Insights},
  volume =        {41},
  year =          {2010},
  abstract =      {As a common concept in multi-objective optimization,
                   minimizing a weighted sum constitutes an independent
                   method as well as a component of other methods.
                   Consequently, insight into characteristics of the
                   weighted sum method has far reaching implications.
                   However, despite the many published applications for
                   this method and the literature addressing its
                   pitfalls with respect to depicting the Pareto optimal
                   set, there is little comprehensive discussion
                   concerning the conceptual significance of the weights
                   and techniques for maximizing the effectiveness of
                   the method with respect to a priori articulation of
                   preferences. Thus, in this paper, we investigate the
                   fundamental significance of the weights in terms of
                   preferences, the Pareto optimal set, and
                   objective-function values. We determine the factors
                   that dictate which solution point results from a
                   particular set of weights. Fundamental deficiencies
                   are identified in terms of a priori articulation of
                   preferences, and guidelines are provided to help
                   avoid blind use of the method.},
  doi =           {10.1007/s00158-009-0460-7},
  issn =          {1615-147X, 1615-1488},
  language =      {en},
}

@article{lehman_designing_2004,
  author =        {Lehman, Jeffrey S. and Santner, Thomas J. and
                   Notz, William I.},
  journal =       {Statistica Sinica},
  pages =         {571--590},
  title =         {Designing Computer Experiments to Determine Robust
                   Control Variables},
  year =          {2004},
}

@book{silverman_density_2018,
  author =        {Silverman, B.W.},
  month =         jan,
  title =         {Density Estimation: {{For}} Statistics and Data
                   Analysis},
  year =          {2018},
  abstract =      {Although there has been a surge of interest in
                   density estimation in recent years, much of the
                   published research has been concerned with purely
                   technical matters with insufficient emphasis given to
                   the technique's practical value. Furthermore, the
                   subject has been rather inaccessible to the general
                   statistician. The account presented in this book
                   places emphasis on topics of methodological
                   importance, in the hope that this will facilitate
                   broader practical application of density estimation
                   and also encourage research into relevant theoretical
                   work. The book also provides an introduction to the
                   subject for those with general interests in
                   statistics. The important role of density estimation
                   as a graphical technique is reflected by the
                   inclusion of more than 50 graphs and figures
                   throughout the text. Several contexts in which
                   density estimation can be used are discussed,
                   including the exploration and presentation of data,
                   nonparametric discriminant analysis, cluster
                   analysis, simulation and the bootstrap, bump hunting,
                   projection pursuit, and the estimation of hazard
                   rates and other quantities that depend on the
                   density. This book includes general survey of methods
                   available for density estimation. The Kernel method,
                   both for univariate and multivariate data, is
                   discussed in detail, with particular emphasis on ways
                   of deciding how much to smooth and on computation
                   aspects. Attention is also given to adaptive methods,
                   which smooth to a greater degree in the tails of the
                   distribution, and to methods based on the idea of
                   penalized likelihood.},
  doi =           {10.1201/9781315140919},
}

@article{dempster_maximum_1977,
  author =        {Dempster, Arthur P. and Laird, Nan M. and
                   Rubin, Donald B.},
  journal =       {Journal of the royal statistical society. Series B
                   (methodological)},
  pages =         {1--38},
  title =         {Maximum Likelihood from Incomplete Data via the
                   {{EM}} Algorithm},
  year =          {1977},
}

@article{freedman_histogram_1981,
  author =        {Freedman, David and Diaconis, Persi},
  journal =       {Zeitschrift f\"ur Wahrscheinlichkeitstheorie und
                   Verwandte Gebiete},
  month =         dec,
  number =        {4},
  pages =         {453--476},
  title =         {On the Histogram as a Density Estimator:{{L2}}
                   Theory},
  volume =        {57},
  year =          {1981},
  doi =           {10.1007/BF01025868},
  issn =          {1432-2064},
  language =      {en},
}

@article{scott_optimal_1979,
  author =        {Scott, David W.},
  journal =       {Biometrika},
  month =         dec,
  number =        {3},
  pages =         {605},
  title =         {On {{Optimal}} and {{Data}}-{{Based Histograms}}},
  volume =        {66},
  year =          {1979},
  abstract =      {In thispaper the formulaforthe optimalhistogrambin
                   widthis derivedwhichasymptotically minimizesthe
                   integratedmean squared error.Monte Carlo methodsare
                   used to verify the usefulnessofthisformulaforsmall
                   samples. A data-based procedureforchoosingthe bin
                   widthparameteris proposed,whichassumes a Gaussian
                   referencestandard and requiresonly the sample size
                   and an estimateofthe standard deviation. The
                   sensitivityofthe procedureis investigatedusingseveral
                   probabilitymodelswhichviolate the Gaussian
                   assumption.},
  doi =           {10.2307/2335182},
  issn =          {00063444},
  language =      {en},
}

@techreport{rockafellar_deviation_2002,
  address =       {{Rochester, NY}},
  author =        {Rockafellar, R. Tyrrell and Uryasev, Stanislav P. and
                   Zabarankin, Michael},
  institution =   {{Social Science Research Network}},
  month =         dec,
  number =        {ID 365640},
  type =          {{{SSRN Scholarly Paper}}},
  title =         {Deviation {{Measures}} in {{Risk Analysis}} and
                   {{Optimization}}},
  year =          {2002},
  abstract =      {General deviation measures, which include standard
                   deviation as a special case but need not be symmetric
                   with respect to ups and downs, are defined and shown
                   to correspond to risk measures in the sense of
                   Artzner, Delbaen, Eber and Heath when those are
                   applied to the difference between a random variable
                   and its expectation, instead of to the random
                   variable itself. A property called
                   expectation-boundedness of the risk measure is
                   uncovered as essential for this correspondence. It is
                   shown to be satisfied by conditional value-at-risk
                   and by worst-case risk, as well as various mixtures,
                   although not by ordinary value-at-risk.},
  language =      {en},
}

@article{savage_theory_1951,
  author =        {Savage, L. J.},
  journal =       {Journal of the American Statistical Association},
  month =         mar,
  number =        {253},
  pages =         {55--67},
  title =         {The {{Theory}} of {{Statistical Decision}}},
  volume =        {46},
  year =          {1951},
  doi =           {10.1080/01621459.1951.10500768},
  issn =          {0162-1459},
}

@article{luedtke_sample_2008,
  author =        {Luedtke, James and Ahmed, Shabbir},
  journal =       {SIAM Journal on Optimization},
  number =        {2},
  pages =         {674--699},
  publisher =     {{SIAM}},
  title =         {A Sample Approximation Approach for Optimization with
                   Probabilistic Constraints},
  volume =        {19},
  year =          {2008},
}

@article{thulin_cost_2014,
  author =        {Thulin, M{\textbackslash}aans},
  journal =       {Electronic Journal of Statistics},
  number =        {1},
  pages =         {817--840},
  publisher =     {{The Institute of Mathematical Statistics and the
                   Bernoulli Society}},
  title =         {The Cost of Using Exact Confidence Intervals for a
                   Binomial Proportion},
  volume =        {8},
  year =          {2014},
}

@article{kennedy_bayesian_2001,
  author =        {Kennedy, Marc C. and O'Hagan, Anthony},
  journal =       {Journal of the Royal Statistical Society: Series B
                   (Statistical Methodology)},
  month =         jan,
  number =        {3},
  pages =         {425--464},
  title =         {Bayesian Calibration of Computer Models},
  volume =        {63},
  year =          {2001},
  abstract =      {We consider prediction and uncertainty analysis for
                   systems which are approximated using complex
                   mathematical models. Such models, implemented as
                   computer codes, are often generic in the sense that
                   by a suitable choice of some of the model's input
                   parameters the code can be used to predict the
                   behaviour of the system in a variety of specific
                   applications. However, in any specific application
                   the values of necessary parameters may be unknown. In
                   this case, physical observations of the system in the
                   specific context are used to learn about the unknown
                   parameters. The process of fitting the model to the
                   observed data by adjusting the parameters is known as
                   calibration. Calibration is typically effected by ad
                   hoc fitting, and after calibration the model is used,
                   with the fitted input values, to predict the future
                   behaviour of the system. We present a Bayesian
                   calibration technique which improves on this
                   traditional approach in two respects. First, the
                   predictions allow for all sources of uncertainty,
                   including the remaining uncertainty over the fitted
                   parameters. Second, they attempt to correct for any
                   inadequacy of the model which is revealed by a
                   discrepancy between the observed data and the model
                   predictions from even the best-fitting parameter
                   values. The method is illustrated by using data from
                   a nuclear radiation release at Tomsk, and from a more
                   complex simulated nuclear accident exercise.},
  doi =           {10.1111/1467-9868.00294},
  issn =          {1467-9868},
  language =      {en},
}

@article{ginsbourger_bayesian_2014,
  author =        {Ginsbourger, David and Baccou, Jean and
                   Chevalier, Cl{\'e}ment and Perales, Fr{\'e}d{\'e}ric and
                   Garland, Nicolas and Monerie, Yann},
  journal =       {SIAM/ASA Journal on Uncertainty Quantification},
  month =         jan,
  number =        {1},
  pages =         {490--510},
  title =         {Bayesian {{Adaptive Reconstruction}} of {{Profile
                   Optima}} and {{Optimizers}}},
  volume =        {2},
  year =          {2014},
  abstract =      {Given a function depending both on decision
                   parameters and nuisance variables, we consider the
                   issue of estimating and quantifying uncertainty on
                   profile optima and/or optimal points as functions of
                   the nuisance variables. The proposed methods base on
                   interpolations of the objective function constructed
                   from a finite set of evaluations. Here the functions
                   of interest are reconstructed relying on a kriging
                   model, but also using Gaussian field conditional
                   simulations, that allow a quantification of
                   uncertainties in the Bayesian framework. Besides, we
                   elaborate a variant of the Expected Improvement
                   criterion, that proves efficient for adaptively
                   learning the set of profile optima and optimizers.
                   The results are illustrated on a toy example and
                   through a physics case study on the optimal packing
                   of polydisperse frictionless spheres.},
  doi =           {10.1137/130949555},
  issn =          {2166-2525},
  language =      {en},
}

@inproceedings{bossek_learning_2015,
  address =       {{New York, NY, USA}},
  author =        {Bossek, Jakob and Bischl, Bernd and Wagner, Tobias and
                   Rudolph, G{\"u}nter},
  booktitle =     {Proceedings of the 2015 {{Annual Conference}} on
                   {{Genetic}} and {{Evolutionary Computation}}},
  pages =         {1319--1326},
  publisher =     {{ACM}},
  series =        {{{GECCO}} '15},
  title =         {Learning {{Feature}}-{{Parameter Mappings}} for
                   {{Parameter Tuning}} via the {{Profile Expected
                   Improvement}}},
  year =          {2015},
  abstract =      {The majority of algorithms can be controlled or
                   adjusted by parameters. Their values can
                   substantially affect the algorithms' performance.
                   Since the manual exploration of the parameter space
                   is tedious -- even for few parameters -- several
                   automatic procedures for parameter tuning have been
                   proposed. Recent approaches also take into account
                   some characteristic properties of the problem
                   instances, frequently termed instance features. Our
                   contribution is the proposal of a novel concept for
                   feature-based algorithm parameter tuning, which
                   applies an approximating surrogate model for learning
                   the continuous feature-parameter mapping. To
                   accomplish this, we learn a joint model of the
                   algorithm performance based on both the algorithm
                   parameters and the instance features. The required
                   data is gathered using a recently proposed
                   acquisition function for model refinement in
                   surrogate-based optimization: the profile expected
                   improvement. This function provides an avenue for
                   maximizing the information required for the
                   feature-parameter mapping, i.e., the mapping from
                   instance features to the corresponding optimal
                   algorithm parameters. The approach is validated by
                   applying the tuner to exemplary evolutionary
                   algorithms and problems, for which theoretically
                   grounded or heuristically determined
                   feature-parameter mappings are available.},
  doi =           {10.1145/2739480.2754673},
  isbn =          {978-1-4503-3472-3},
}

@article{gneiting_probabilistic_2014,
  author =        {Gneiting, Tilmann and Katzfuss, Matthias},
  journal =       {Annual Review of Statistics and Its Application},
  month =         jan,
  number =        {1},
  pages =         {125--151},
  title =         {Probabilistic {{Forecasting}}},
  volume =        {1},
  year =          {2014},
  abstract =      {A probabilistic forecast takes the form of a
                   predictive probability distribution over future
                   quantities or events of interest. Probabilistic
                   forecasting aims to maximize the sharpness of the
                   predictive distributions, subject to calibration, on
                   the basis of the available information set. We
                   formalize and study notions of calibration in a
                   prediction space setting. In practice, probabilistic
                   calibration can be checked by examining probability
                   integral transform (PIT) histograms. Proper scoring
                   rules such as the logarithmic score and the
                   continuous ranked probability score serve to assess
                   calibration and sharpness simultaneously. As a
                   special case, consistent scoring functions provide
                   decision-theoretically coherent tools for evaluating
                   point forecasts. We emphasize methodological links to
                   parametric and nonparametric distributional
                   regression techniques, which attempt to model and to
                   estimate conditional distribution functions; we use
                   the context of statistically postprocessed ensemble
                   forecasts in numerical weather prediction as an
                   example. Throughout, we illustrate concepts and
                   methodologies in data examples.},
  doi =           {10.1146/annurev-statistics-062713-085831},
  issn =          {2326-8298},
}

@inproceedings{ginsbourger_towards_2010,
  address =       {{Heidelberg}},
  author =        {Ginsbourger, David and Le Riche, Rodolphe},
  booktitle =     {{{mODa}} 9 \textendash{} {{Advances}} in
                   {{Model}}-{{Oriented Design}} and {{Analysis}}},
  editor =        {Giovagnoli, Alessandra and Atkinson, Anthony C. and
                   Torsney, Bernard and May, Caterina},
  pages =         {89--96},
  publisher =     {{Physica-Verlag HD}},
  series =        {Contributions to {{Statistics}}},
  title =         {Towards {{Gaussian Process}}-Based {{Optimization}}
                   with {{Finite Time Horizon}}},
  year =          {2010},
  abstract =      {During the last decade, Kriging-based sequential
                   optimization algorithms have become standard methods
                   in computer experiments. These algorithms rely on the
                   iterative maximization of sampling criteria such as
                   the Expected Improvement (EI), which takes advantage
                   of Kriging conditional distributions to make an
                   explicit trade-off between promising and uncertain
                   points in the search space. We have recently worked
                   on a multipoint EI criterion meant to choose
                   simultaneously several points for synchronous
                   parallel computation. The results presented in this
                   article concern sequential procedures with a fixed
                   number of iterations. We show that maximizing the
                   usual EI at each iteration is suboptimal. In essence,
                   the latter amounts to considering the current
                   iteration as the last one. This work formulates the
                   problem of optimal strategy for finite horizon
                   sequential optimization, provides the solution to
                   this problem in terms of a new multipoint EI, and
                   illustrates the suboptimality of maximizing the
                   1-point EI at each iteration on the basis of a first
                   counter-example.},
  doi =           {10.1007/978-3-7908-2410-0_12},
  isbn =          {978-3-7908-2410-0},
  language =      {en},
}

